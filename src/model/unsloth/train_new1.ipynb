{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install unsloth\n",
    "# # Also get the latest nightly Unsloth!\n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall unsloth unsloth_zoo -y\n",
    "# %pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# %pip install --upgrade --no-cache-dir \"git+https://github.com/unslothai/unsloth-zoo.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 03:04:57.801083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-12 03:04:57.814852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739297097.831023  465020 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739297097.836008  465020 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-12 03:04:57.853114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu118)\n",
      "    Python  3.12.8 (you have 3.12.0)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.4: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 8.0. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.2.4 patched 32 layers with 32 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/diabetes_v18\", split = \"train\")\n",
    "dataset = dataset.filter(lambda example: example['task'] in ['qa1', 'qa2', 'qa3', 'nli', 'ie', 're'])\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 50,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_new/classification\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,716 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 255\n",
      " \"-____-\"     Number of trainable parameters = 9,437,184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 28:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/461] Sample processed in 0.20s, ETA: 1.57 min\n",
      "[2/461] Sample processed in 0.12s, ETA: 1.25 min\n",
      "[3/461] Sample processed in 0.12s, ETA: 1.15 min\n",
      "[4/461] Sample processed in 0.12s, ETA: 1.09 min\n",
      "[5/461] Sample processed in 0.12s, ETA: 1.06 min\n",
      "[6/461] Sample processed in 0.12s, ETA: 1.03 min\n",
      "[7/461] Sample processed in 0.12s, ETA: 1.02 min\n",
      "[8/461] Sample processed in 0.12s, ETA: 1.00 min\n",
      "[9/461] Sample processed in 0.12s, ETA: 0.99 min\n",
      "[10/461] Sample processed in 0.12s, ETA: 0.98 min\n",
      "[11/461] Sample processed in 0.12s, ETA: 0.97 min\n",
      "[12/461] Sample processed in 0.12s, ETA: 0.97 min\n",
      "[13/461] Sample processed in 0.12s, ETA: 0.96 min\n",
      "[14/461] Sample processed in 0.14s, ETA: 0.97 min\n",
      "[15/461] Sample processed in 0.12s, ETA: 0.96 min\n",
      "[16/461] Sample processed in 0.12s, ETA: 0.95 min\n",
      "[17/461] Sample processed in 0.12s, ETA: 0.95 min\n",
      "[18/461] Sample processed in 0.12s, ETA: 0.94 min\n",
      "[19/461] Sample processed in 0.12s, ETA: 0.94 min\n",
      "[20/461] Sample processed in 0.12s, ETA: 0.94 min\n",
      "[21/461] Sample processed in 0.12s, ETA: 0.93 min\n",
      "[22/461] Sample processed in 0.12s, ETA: 0.93 min\n",
      "[23/461] Sample processed in 0.12s, ETA: 0.92 min\n",
      "[24/461] Sample processed in 0.13s, ETA: 0.92 min\n",
      "[25/461] Sample processed in 0.12s, ETA: 0.92 min\n",
      "[26/461] Sample processed in 0.12s, ETA: 0.92 min\n",
      "[27/461] Sample processed in 0.12s, ETA: 0.91 min\n",
      "[28/461] Sample processed in 0.12s, ETA: 0.91 min\n",
      "[29/461] Sample processed in 0.13s, ETA: 0.91 min\n",
      "[30/461] Sample processed in 0.12s, ETA: 0.91 min\n",
      "[31/461] Sample processed in 0.12s, ETA: 0.90 min\n",
      "[32/461] Sample processed in 0.12s, ETA: 0.90 min\n",
      "[33/461] Sample processed in 0.12s, ETA: 0.90 min\n",
      "[34/461] Sample processed in 0.12s, ETA: 0.89 min\n",
      "[35/461] Sample processed in 0.12s, ETA: 0.89 min\n",
      "[36/461] Sample processed in 0.12s, ETA: 0.89 min\n",
      "[37/461] Sample processed in 0.12s, ETA: 0.88 min\n",
      "[38/461] Sample processed in 0.12s, ETA: 0.88 min\n",
      "[39/461] Sample processed in 0.12s, ETA: 0.88 min\n",
      "[40/461] Sample processed in 0.12s, ETA: 0.88 min\n",
      "[41/461] Sample processed in 0.14s, ETA: 0.88 min\n",
      "[42/461] Sample processed in 0.12s, ETA: 0.88 min\n",
      "[43/461] Sample processed in 0.12s, ETA: 0.87 min\n",
      "[44/461] Sample processed in 0.12s, ETA: 0.87 min\n",
      "[45/461] Sample processed in 0.13s, ETA: 0.87 min\n",
      "[46/461] Sample processed in 0.13s, ETA: 0.87 min\n",
      "[47/461] Sample processed in 0.12s, ETA: 0.87 min\n",
      "[48/461] Sample processed in 0.12s, ETA: 0.86 min\n",
      "[49/461] Sample processed in 0.12s, ETA: 0.86 min\n",
      "[50/461] Sample processed in 0.12s, ETA: 0.86 min\n",
      "[51/461] Sample processed in 0.12s, ETA: 0.86 min\n",
      "[52/461] Sample processed in 0.13s, ETA: 0.85 min\n",
      "[53/461] Sample processed in 0.12s, ETA: 0.85 min\n",
      "[54/461] Sample processed in 0.12s, ETA: 0.85 min\n",
      "[55/461] Sample processed in 0.12s, ETA: 0.85 min\n",
      "[56/461] Sample processed in 0.12s, ETA: 0.85 min\n",
      "[57/461] Sample processed in 0.12s, ETA: 0.84 min\n",
      "[58/461] Sample processed in 0.12s, ETA: 0.84 min\n",
      "[59/461] Sample processed in 0.12s, ETA: 0.84 min\n",
      "[60/461] Sample processed in 0.13s, ETA: 0.84 min\n",
      "[61/461] Sample processed in 0.12s, ETA: 0.83 min\n",
      "[62/461] Sample processed in 0.13s, ETA: 0.83 min\n",
      "[63/461] Sample processed in 0.12s, ETA: 0.83 min\n",
      "[64/461] Sample processed in 0.14s, ETA: 0.83 min\n",
      "[65/461] Sample processed in 0.12s, ETA: 0.83 min\n",
      "[66/461] Sample processed in 0.12s, ETA: 0.83 min\n",
      "[67/461] Sample processed in 0.12s, ETA: 0.82 min\n",
      "[68/461] Sample processed in 0.12s, ETA: 0.82 min\n",
      "[69/461] Sample processed in 0.12s, ETA: 0.82 min\n",
      "[70/461] Sample processed in 0.12s, ETA: 0.82 min\n",
      "[71/461] Sample processed in 0.12s, ETA: 0.81 min\n",
      "[72/461] Sample processed in 0.12s, ETA: 0.81 min\n",
      "[73/461] Sample processed in 0.13s, ETA: 0.81 min\n",
      "[74/461] Sample processed in 0.12s, ETA: 0.81 min\n",
      "[75/461] Sample processed in 0.12s, ETA: 0.80 min\n",
      "[76/461] Sample processed in 0.12s, ETA: 0.80 min\n",
      "[77/461] Sample processed in 0.13s, ETA: 0.80 min\n",
      "[78/461] Sample processed in 0.12s, ETA: 0.80 min\n",
      "[79/461] Sample processed in 0.12s, ETA: 0.80 min\n",
      "[80/461] Sample processed in 0.12s, ETA: 0.79 min\n",
      "[81/461] Sample processed in 0.14s, ETA: 0.79 min\n",
      "[82/461] Sample processed in 0.12s, ETA: 0.79 min\n",
      "[83/461] Sample processed in 0.13s, ETA: 0.79 min\n",
      "[84/461] Sample processed in 0.12s, ETA: 0.79 min\n",
      "[85/461] Sample processed in 0.13s, ETA: 0.79 min\n",
      "[86/461] Sample processed in 0.13s, ETA: 0.78 min\n",
      "[87/461] Sample processed in 0.12s, ETA: 0.78 min\n",
      "[88/461] Sample processed in 0.13s, ETA: 0.78 min\n",
      "[89/461] Sample processed in 0.12s, ETA: 0.78 min\n",
      "[90/461] Sample processed in 0.12s, ETA: 0.78 min\n",
      "[91/461] Sample processed in 0.13s, ETA: 0.77 min\n",
      "[92/461] Sample processed in 0.12s, ETA: 0.77 min\n",
      "[93/461] Sample processed in 0.13s, ETA: 0.77 min\n",
      "[94/461] Sample processed in 0.12s, ETA: 0.77 min\n",
      "[95/461] Sample processed in 0.13s, ETA: 0.77 min\n",
      "[96/461] Sample processed in 0.12s, ETA: 0.76 min\n",
      "[97/461] Sample processed in 0.13s, ETA: 0.76 min\n",
      "[98/461] Sample processed in 0.12s, ETA: 0.76 min\n",
      "[99/461] Sample processed in 0.12s, ETA: 0.76 min\n",
      "[100/461] Sample processed in 0.13s, ETA: 0.76 min\n",
      "[101/461] Sample processed in 0.13s, ETA: 0.75 min\n",
      "[102/461] Sample processed in 0.12s, ETA: 0.75 min\n",
      "[103/461] Sample processed in 0.12s, ETA: 0.75 min\n",
      "[104/461] Sample processed in 0.12s, ETA: 0.75 min\n",
      "[105/461] Sample processed in 0.12s, ETA: 0.74 min\n",
      "[106/461] Sample processed in 0.12s, ETA: 0.74 min\n",
      "[107/461] Sample processed in 0.12s, ETA: 0.74 min\n",
      "[108/461] Sample processed in 0.12s, ETA: 0.74 min\n",
      "[109/461] Sample processed in 0.12s, ETA: 0.73 min\n",
      "[110/461] Sample processed in 0.12s, ETA: 0.73 min\n",
      "[111/461] Sample processed in 0.12s, ETA: 0.73 min\n",
      "[112/461] Sample processed in 0.12s, ETA: 0.73 min\n",
      "[113/461] Sample processed in 0.12s, ETA: 0.73 min\n",
      "[114/461] Sample processed in 0.12s, ETA: 0.72 min\n",
      "[115/461] Sample processed in 0.12s, ETA: 0.72 min\n",
      "[116/461] Sample processed in 0.12s, ETA: 0.72 min\n",
      "[117/461] Sample processed in 0.12s, ETA: 0.72 min\n",
      "[118/461] Sample processed in 0.12s, ETA: 0.71 min\n",
      "[119/461] Sample processed in 0.12s, ETA: 0.71 min\n",
      "[120/461] Sample processed in 0.12s, ETA: 0.71 min\n",
      "[121/461] Sample processed in 0.12s, ETA: 0.71 min\n",
      "[122/461] Sample processed in 0.12s, ETA: 0.71 min\n",
      "[123/461] Sample processed in 0.12s, ETA: 0.70 min\n",
      "[124/461] Sample processed in 0.12s, ETA: 0.70 min\n",
      "[125/461] Sample processed in 0.12s, ETA: 0.70 min\n",
      "[126/461] Sample processed in 0.12s, ETA: 0.70 min\n",
      "[127/461] Sample processed in 0.12s, ETA: 0.69 min\n",
      "[128/461] Sample processed in 0.12s, ETA: 0.69 min\n",
      "[129/461] Sample processed in 0.12s, ETA: 0.69 min\n",
      "[130/461] Sample processed in 0.12s, ETA: 0.69 min\n",
      "[131/461] Sample processed in 0.12s, ETA: 0.69 min\n",
      "[132/461] Sample processed in 0.12s, ETA: 0.68 min\n",
      "[133/461] Sample processed in 0.12s, ETA: 0.68 min\n",
      "[134/461] Sample processed in 0.12s, ETA: 0.68 min\n",
      "[135/461] Sample processed in 0.12s, ETA: 0.68 min\n",
      "[136/461] Sample processed in 0.12s, ETA: 0.67 min\n",
      "[137/461] Sample processed in 0.12s, ETA: 0.67 min\n",
      "[138/461] Sample processed in 0.12s, ETA: 0.67 min\n",
      "[139/461] Sample processed in 0.12s, ETA: 0.67 min\n",
      "[140/461] Sample processed in 0.12s, ETA: 0.67 min\n",
      "[141/461] Sample processed in 0.12s, ETA: 0.66 min\n",
      "[142/461] Sample processed in 0.12s, ETA: 0.66 min\n",
      "[143/461] Sample processed in 0.12s, ETA: 0.66 min\n",
      "[144/461] Sample processed in 0.12s, ETA: 0.66 min\n",
      "[145/461] Sample processed in 0.12s, ETA: 0.65 min\n",
      "[146/461] Sample processed in 0.12s, ETA: 0.65 min\n",
      "[147/461] Sample processed in 0.12s, ETA: 0.65 min\n",
      "[148/461] Sample processed in 0.12s, ETA: 0.65 min\n",
      "[149/461] Sample processed in 0.12s, ETA: 0.65 min\n",
      "[150/461] Sample processed in 0.12s, ETA: 0.64 min\n",
      "[151/461] Sample processed in 0.12s, ETA: 0.64 min\n",
      "[152/461] Sample processed in 0.12s, ETA: 0.64 min\n",
      "[153/461] Sample processed in 0.12s, ETA: 0.64 min\n",
      "[154/461] Sample processed in 0.12s, ETA: 0.64 min\n",
      "[155/461] Sample processed in 0.12s, ETA: 0.63 min\n",
      "[156/461] Sample processed in 0.12s, ETA: 0.63 min\n",
      "[157/461] Sample processed in 0.12s, ETA: 0.63 min\n",
      "[158/461] Sample processed in 0.12s, ETA: 0.63 min\n",
      "[159/461] Sample processed in 0.12s, ETA: 0.62 min\n",
      "[160/461] Sample processed in 0.12s, ETA: 0.62 min\n",
      "[161/461] Sample processed in 0.13s, ETA: 0.62 min\n",
      "[162/461] Sample processed in 0.12s, ETA: 0.62 min\n",
      "[163/461] Sample processed in 0.12s, ETA: 0.62 min\n",
      "[164/461] Sample processed in 0.12s, ETA: 0.61 min\n",
      "[165/461] Sample processed in 0.12s, ETA: 0.61 min\n",
      "[166/461] Sample processed in 0.12s, ETA: 0.61 min\n",
      "[167/461] Sample processed in 0.12s, ETA: 0.61 min\n",
      "[168/461] Sample processed in 0.12s, ETA: 0.61 min\n",
      "[169/461] Sample processed in 0.12s, ETA: 0.60 min\n",
      "[170/461] Sample processed in 0.12s, ETA: 0.60 min\n",
      "[171/461] Sample processed in 0.12s, ETA: 0.60 min\n",
      "[172/461] Sample processed in 0.12s, ETA: 0.60 min\n",
      "[173/461] Sample processed in 0.12s, ETA: 0.59 min\n",
      "[174/461] Sample processed in 0.12s, ETA: 0.59 min\n",
      "[175/461] Sample processed in 0.12s, ETA: 0.59 min\n",
      "[176/461] Sample processed in 0.12s, ETA: 0.59 min\n",
      "[177/461] Sample processed in 0.12s, ETA: 0.59 min\n",
      "[178/461] Sample processed in 0.12s, ETA: 0.58 min\n",
      "[179/461] Sample processed in 0.12s, ETA: 0.58 min\n",
      "[180/461] Sample processed in 0.12s, ETA: 0.58 min\n",
      "[181/461] Sample processed in 0.12s, ETA: 0.58 min\n",
      "[182/461] Sample processed in 0.12s, ETA: 0.58 min\n",
      "[183/461] Sample processed in 0.12s, ETA: 0.57 min\n",
      "[184/461] Sample processed in 0.12s, ETA: 0.57 min\n",
      "[185/461] Sample processed in 0.12s, ETA: 0.57 min\n",
      "[186/461] Sample processed in 0.12s, ETA: 0.57 min\n",
      "[187/461] Sample processed in 0.12s, ETA: 0.56 min\n",
      "[188/461] Sample processed in 0.12s, ETA: 0.56 min\n",
      "[189/461] Sample processed in 0.12s, ETA: 0.56 min\n",
      "[190/461] Sample processed in 0.12s, ETA: 0.56 min\n",
      "[191/461] Sample processed in 0.12s, ETA: 0.56 min\n",
      "[192/461] Sample processed in 0.12s, ETA: 0.55 min\n",
      "[193/461] Sample processed in 0.12s, ETA: 0.55 min\n",
      "[194/461] Sample processed in 0.12s, ETA: 0.55 min\n",
      "[195/461] Sample processed in 0.12s, ETA: 0.55 min\n",
      "[196/461] Sample processed in 0.12s, ETA: 0.55 min\n",
      "[197/461] Sample processed in 0.12s, ETA: 0.54 min\n",
      "[198/461] Sample processed in 0.12s, ETA: 0.54 min\n",
      "[199/461] Sample processed in 0.12s, ETA: 0.54 min\n",
      "[200/461] Sample processed in 0.12s, ETA: 0.54 min\n",
      "[201/461] Sample processed in 0.13s, ETA: 0.54 min\n",
      "[202/461] Sample processed in 0.14s, ETA: 0.53 min\n",
      "[203/461] Sample processed in 0.13s, ETA: 0.53 min\n",
      "[204/461] Sample processed in 0.14s, ETA: 0.53 min\n",
      "[205/461] Sample processed in 0.12s, ETA: 0.53 min\n",
      "[206/461] Sample processed in 0.12s, ETA: 0.53 min\n",
      "[207/461] Sample processed in 0.14s, ETA: 0.52 min\n",
      "[208/461] Sample processed in 0.13s, ETA: 0.52 min\n",
      "[209/461] Sample processed in 0.12s, ETA: 0.52 min\n",
      "[210/461] Sample processed in 0.12s, ETA: 0.52 min\n",
      "[211/461] Sample processed in 0.12s, ETA: 0.52 min\n",
      "[212/461] Sample processed in 0.14s, ETA: 0.51 min\n",
      "[213/461] Sample processed in 0.12s, ETA: 0.51 min\n",
      "[214/461] Sample processed in 0.12s, ETA: 0.51 min\n",
      "[215/461] Sample processed in 0.12s, ETA: 0.51 min\n",
      "[216/461] Sample processed in 0.14s, ETA: 0.51 min\n",
      "[217/461] Sample processed in 0.13s, ETA: 0.50 min\n",
      "[218/461] Sample processed in 0.13s, ETA: 0.50 min\n",
      "[219/461] Sample processed in 0.14s, ETA: 0.50 min\n",
      "[220/461] Sample processed in 0.13s, ETA: 0.50 min\n",
      "[221/461] Sample processed in 0.12s, ETA: 0.50 min\n",
      "[222/461] Sample processed in 0.13s, ETA: 0.49 min\n",
      "[223/461] Sample processed in 0.12s, ETA: 0.49 min\n",
      "[224/461] Sample processed in 0.12s, ETA: 0.49 min\n",
      "[225/461] Sample processed in 0.13s, ETA: 0.49 min\n",
      "[226/461] Sample processed in 0.13s, ETA: 0.49 min\n",
      "[227/461] Sample processed in 0.13s, ETA: 0.48 min\n",
      "[228/461] Sample processed in 0.14s, ETA: 0.48 min\n",
      "[229/461] Sample processed in 0.12s, ETA: 0.48 min\n",
      "[230/461] Sample processed in 0.13s, ETA: 0.48 min\n",
      "[231/461] Sample processed in 0.12s, ETA: 0.48 min\n",
      "[232/461] Sample processed in 0.12s, ETA: 0.47 min\n",
      "[233/461] Sample processed in 0.13s, ETA: 0.47 min\n",
      "[234/461] Sample processed in 0.13s, ETA: 0.47 min\n",
      "[235/461] Sample processed in 0.12s, ETA: 0.47 min\n",
      "[236/461] Sample processed in 0.14s, ETA: 0.47 min\n",
      "[237/461] Sample processed in 0.14s, ETA: 0.47 min\n",
      "[238/461] Sample processed in 0.13s, ETA: 0.46 min\n",
      "[239/461] Sample processed in 0.13s, ETA: 0.46 min\n",
      "[240/461] Sample processed in 0.13s, ETA: 0.46 min\n",
      "[241/461] Sample processed in 0.12s, ETA: 0.46 min\n",
      "[242/461] Sample processed in 0.14s, ETA: 0.46 min\n",
      "[243/461] Sample processed in 0.12s, ETA: 0.45 min\n",
      "[244/461] Sample processed in 0.12s, ETA: 0.45 min\n",
      "[245/461] Sample processed in 0.14s, ETA: 0.45 min\n",
      "[246/461] Sample processed in 0.12s, ETA: 0.45 min\n",
      "[247/461] Sample processed in 0.12s, ETA: 0.45 min\n",
      "[248/461] Sample processed in 0.12s, ETA: 0.44 min\n",
      "[249/461] Sample processed in 0.12s, ETA: 0.44 min\n",
      "[250/461] Sample processed in 0.13s, ETA: 0.44 min\n",
      "[251/461] Sample processed in 0.14s, ETA: 0.44 min\n",
      "[252/461] Sample processed in 0.12s, ETA: 0.44 min\n",
      "[253/461] Sample processed in 0.13s, ETA: 0.43 min\n",
      "[254/461] Sample processed in 0.13s, ETA: 0.43 min\n",
      "[255/461] Sample processed in 0.14s, ETA: 0.43 min\n",
      "[256/461] Sample processed in 0.13s, ETA: 0.43 min\n",
      "[257/461] Sample processed in 0.12s, ETA: 0.43 min\n",
      "[258/461] Sample processed in 0.13s, ETA: 0.42 min\n",
      "[259/461] Sample processed in 0.13s, ETA: 0.42 min\n",
      "[260/461] Sample processed in 0.14s, ETA: 0.42 min\n",
      "[261/461] Sample processed in 0.13s, ETA: 0.42 min\n",
      "[262/461] Sample processed in 0.12s, ETA: 0.42 min\n",
      "[263/461] Sample processed in 0.12s, ETA: 0.41 min\n",
      "[264/461] Sample processed in 0.13s, ETA: 0.41 min\n",
      "[265/461] Sample processed in 0.12s, ETA: 0.41 min\n",
      "[266/461] Sample processed in 0.13s, ETA: 0.41 min\n",
      "[267/461] Sample processed in 0.12s, ETA: 0.40 min\n",
      "[268/461] Sample processed in 0.13s, ETA: 0.40 min\n",
      "[269/461] Sample processed in 0.14s, ETA: 0.40 min\n",
      "[270/461] Sample processed in 0.14s, ETA: 0.40 min\n",
      "[271/461] Sample processed in 0.13s, ETA: 0.40 min\n",
      "[272/461] Sample processed in 0.12s, ETA: 0.39 min\n",
      "[273/461] Sample processed in 0.12s, ETA: 0.39 min\n",
      "[274/461] Sample processed in 0.14s, ETA: 0.39 min\n",
      "[275/461] Sample processed in 0.13s, ETA: 0.39 min\n",
      "[276/461] Sample processed in 0.13s, ETA: 0.39 min\n",
      "[277/461] Sample processed in 0.12s, ETA: 0.38 min\n",
      "[278/461] Sample processed in 0.14s, ETA: 0.38 min\n",
      "[279/461] Sample processed in 0.13s, ETA: 0.38 min\n",
      "[280/461] Sample processed in 0.13s, ETA: 0.38 min\n",
      "[281/461] Sample processed in 0.12s, ETA: 0.38 min\n",
      "[282/461] Sample processed in 0.13s, ETA: 0.37 min\n",
      "[283/461] Sample processed in 0.12s, ETA: 0.37 min\n",
      "[284/461] Sample processed in 0.12s, ETA: 0.37 min\n",
      "[285/461] Sample processed in 0.13s, ETA: 0.37 min\n",
      "[286/461] Sample processed in 0.12s, ETA: 0.37 min\n",
      "[287/461] Sample processed in 0.13s, ETA: 0.36 min\n",
      "[288/461] Sample processed in 0.12s, ETA: 0.36 min\n",
      "[289/461] Sample processed in 0.13s, ETA: 0.36 min\n",
      "[290/461] Sample processed in 0.12s, ETA: 0.36 min\n",
      "[291/461] Sample processed in 0.13s, ETA: 0.36 min\n",
      "[292/461] Sample processed in 0.12s, ETA: 0.35 min\n",
      "[293/461] Sample processed in 0.12s, ETA: 0.35 min\n",
      "[294/461] Sample processed in 0.13s, ETA: 0.35 min\n",
      "[295/461] Sample processed in 0.12s, ETA: 0.35 min\n",
      "[296/461] Sample processed in 0.14s, ETA: 0.35 min\n",
      "[297/461] Sample processed in 0.13s, ETA: 0.34 min\n",
      "[298/461] Sample processed in 0.13s, ETA: 0.34 min\n",
      "[299/461] Sample processed in 0.12s, ETA: 0.34 min\n",
      "[300/461] Sample processed in 0.12s, ETA: 0.34 min\n",
      "[301/461] Sample processed in 0.22s, ETA: 0.34 min\n",
      "[302/461] Sample processed in 0.21s, ETA: 0.33 min\n",
      "[303/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[304/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[305/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[306/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[307/461] Sample processed in 0.23s, ETA: 0.33 min\n",
      "[308/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[309/461] Sample processed in 0.22s, ETA: 0.33 min\n",
      "[310/461] Sample processed in 0.22s, ETA: 0.32 min\n",
      "[311/461] Sample processed in 0.22s, ETA: 0.32 min\n",
      "[312/461] Sample processed in 0.22s, ETA: 0.32 min\n",
      "[313/461] Sample processed in 0.22s, ETA: 0.32 min\n",
      "[314/461] Sample processed in 0.23s, ETA: 0.32 min\n",
      "[315/461] Sample processed in 0.23s, ETA: 0.32 min\n",
      "[316/461] Sample processed in 0.22s, ETA: 0.32 min\n",
      "[317/461] Sample processed in 0.23s, ETA: 0.31 min\n",
      "[318/461] Sample processed in 0.22s, ETA: 0.31 min\n",
      "[319/461] Sample processed in 0.22s, ETA: 0.31 min\n",
      "[320/461] Sample processed in 0.21s, ETA: 0.31 min\n",
      "[321/461] Sample processed in 0.23s, ETA: 0.31 min\n",
      "[322/461] Sample processed in 0.22s, ETA: 0.31 min\n",
      "[323/461] Sample processed in 0.22s, ETA: 0.30 min\n",
      "[324/461] Sample processed in 0.21s, ETA: 0.30 min\n",
      "[325/461] Sample processed in 0.22s, ETA: 0.30 min\n",
      "[326/461] Sample processed in 0.22s, ETA: 0.30 min\n",
      "[327/461] Sample processed in 0.22s, ETA: 0.30 min\n",
      "[328/461] Sample processed in 0.22s, ETA: 0.30 min\n",
      "[329/461] Sample processed in 0.22s, ETA: 0.29 min\n",
      "[330/461] Sample processed in 0.22s, ETA: 0.29 min\n",
      "[331/461] Sample processed in 0.22s, ETA: 0.29 min\n",
      "[332/461] Sample processed in 0.24s, ETA: 0.29 min\n",
      "[333/461] Sample processed in 0.22s, ETA: 0.29 min\n",
      "[334/461] Sample processed in 0.22s, ETA: 0.29 min\n",
      "[335/461] Sample processed in 0.22s, ETA: 0.28 min\n",
      "[336/461] Sample processed in 0.22s, ETA: 0.28 min\n",
      "[337/461] Sample processed in 0.21s, ETA: 0.28 min\n",
      "[338/461] Sample processed in 0.21s, ETA: 0.28 min\n",
      "[339/461] Sample processed in 0.22s, ETA: 0.28 min\n",
      "[340/461] Sample processed in 0.22s, ETA: 0.28 min\n",
      "[341/461] Sample processed in 0.23s, ETA: 0.27 min\n",
      "[342/461] Sample processed in 0.23s, ETA: 0.27 min\n",
      "[343/461] Sample processed in 0.23s, ETA: 0.27 min\n",
      "[344/461] Sample processed in 0.22s, ETA: 0.27 min\n",
      "[345/461] Sample processed in 0.23s, ETA: 0.27 min\n",
      "[346/461] Sample processed in 0.22s, ETA: 0.26 min\n",
      "[347/461] Sample processed in 0.21s, ETA: 0.26 min\n",
      "[348/461] Sample processed in 0.22s, ETA: 0.26 min\n",
      "[349/461] Sample processed in 0.22s, ETA: 0.26 min\n",
      "[350/461] Sample processed in 0.22s, ETA: 0.26 min\n",
      "[351/461] Sample processed in 0.22s, ETA: 0.26 min\n",
      "[352/461] Sample processed in 0.22s, ETA: 0.25 min\n",
      "[353/461] Sample processed in 0.22s, ETA: 0.25 min\n",
      "[354/461] Sample processed in 0.23s, ETA: 0.25 min\n",
      "[355/461] Sample processed in 0.22s, ETA: 0.25 min\n",
      "[356/461] Sample processed in 0.22s, ETA: 0.25 min\n",
      "[357/461] Sample processed in 0.22s, ETA: 0.24 min\n",
      "[358/461] Sample processed in 0.25s, ETA: 0.24 min\n",
      "[359/461] Sample processed in 0.22s, ETA: 0.24 min\n",
      "[360/461] Sample processed in 0.22s, ETA: 0.24 min\n",
      "[361/461] Sample processed in 0.22s, ETA: 0.24 min\n",
      "[362/461] Sample processed in 0.22s, ETA: 0.23 min\n",
      "[363/461] Sample processed in 0.22s, ETA: 0.23 min\n",
      "[364/461] Sample processed in 0.24s, ETA: 0.23 min\n",
      "[365/461] Sample processed in 0.22s, ETA: 0.23 min\n",
      "[366/461] Sample processed in 0.23s, ETA: 0.23 min\n",
      "[367/461] Sample processed in 0.22s, ETA: 0.22 min\n",
      "[368/461] Sample processed in 0.22s, ETA: 0.22 min\n",
      "[369/461] Sample processed in 0.22s, ETA: 0.22 min\n",
      "[370/461] Sample processed in 0.22s, ETA: 0.22 min\n",
      "[371/461] Sample processed in 0.22s, ETA: 0.22 min\n",
      "[372/461] Sample processed in 0.22s, ETA: 0.21 min\n",
      "[373/461] Sample processed in 0.22s, ETA: 0.21 min\n",
      "[374/461] Sample processed in 0.22s, ETA: 0.21 min\n",
      "[375/461] Sample processed in 0.22s, ETA: 0.21 min\n",
      "[376/461] Sample processed in 0.23s, ETA: 0.21 min\n",
      "[377/461] Sample processed in 0.18s, ETA: 0.20 min\n",
      "[378/461] Sample processed in 1.73s, ETA: 0.21 min\n",
      "[379/461] Sample processed in 1.10s, ETA: 0.21 min\n",
      "[380/461] Sample processed in 2.32s, ETA: 0.21 min\n",
      "[381/461] Sample processed in 0.79s, ETA: 0.21 min\n",
      "[382/461] Sample processed in 0.16s, ETA: 0.21 min\n",
      "[383/461] Sample processed in 0.17s, ETA: 0.21 min\n",
      "[384/461] Sample processed in 0.31s, ETA: 0.20 min\n",
      "[385/461] Sample processed in 5.41s, ETA: 0.22 min\n",
      "[386/461] Sample processed in 1.23s, ETA: 0.22 min\n",
      "[387/461] Sample processed in 0.27s, ETA: 0.22 min\n",
      "[388/461] Sample processed in 1.27s, ETA: 0.22 min\n",
      "[389/461] Sample processed in 0.43s, ETA: 0.22 min\n",
      "[390/461] Sample processed in 0.08s, ETA: 0.21 min\n",
      "[391/461] Sample processed in 0.23s, ETA: 0.21 min\n",
      "[392/461] Sample processed in 0.77s, ETA: 0.21 min\n",
      "[393/461] Sample processed in 0.59s, ETA: 0.21 min\n",
      "[394/461] Sample processed in 1.34s, ETA: 0.21 min\n",
      "[395/461] Sample processed in 0.70s, ETA: 0.20 min\n",
      "[396/461] Sample processed in 0.19s, ETA: 0.20 min\n",
      "[397/461] Sample processed in 0.26s, ETA: 0.20 min\n",
      "[398/461] Sample processed in 0.43s, ETA: 0.20 min\n",
      "[399/461] Sample processed in 5.41s, ETA: 0.21 min\n",
      "[400/461] Sample processed in 0.46s, ETA: 0.20 min\n",
      "[401/461] Sample processed in 0.29s, ETA: 0.20 min\n",
      "[402/461] Sample processed in 0.30s, ETA: 0.20 min\n",
      "[403/461] Sample processed in 1.27s, ETA: 0.20 min\n",
      "[404/461] Sample processed in 0.75s, ETA: 0.19 min\n",
      "[405/461] Sample processed in 0.43s, ETA: 0.19 min\n",
      "[406/461] Sample processed in 0.74s, ETA: 0.19 min\n",
      "[407/461] Sample processed in 0.56s, ETA: 0.19 min\n",
      "[408/461] Sample processed in 0.65s, ETA: 0.18 min\n",
      "[409/461] Sample processed in 0.69s, ETA: 0.18 min\n",
      "[410/461] Sample processed in 1.89s, ETA: 0.18 min\n",
      "[411/461] Sample processed in 1.27s, ETA: 0.18 min\n",
      "[412/461] Sample processed in 0.39s, ETA: 0.18 min\n",
      "[413/461] Sample processed in 0.43s, ETA: 0.17 min\n",
      "[414/461] Sample processed in 0.56s, ETA: 0.17 min\n",
      "[415/461] Sample processed in 0.12s, ETA: 0.17 min\n",
      "[416/461] Sample processed in 1.39s, ETA: 0.17 min\n",
      "[417/461] Sample processed in 0.55s, ETA: 0.16 min\n",
      "[418/461] Sample processed in 1.81s, ETA: 0.16 min\n",
      "[419/461] Sample processed in 0.56s, ETA: 0.16 min\n",
      "[420/461] Sample processed in 0.25s, ETA: 0.15 min\n",
      "[421/461] Sample processed in 0.08s, ETA: 0.15 min\n",
      "[422/461] Sample processed in 0.25s, ETA: 0.15 min\n",
      "[423/461] Sample processed in 0.25s, ETA: 0.14 min\n",
      "[424/461] Sample processed in 0.25s, ETA: 0.14 min\n",
      "[425/461] Sample processed in 0.25s, ETA: 0.14 min\n",
      "[426/461] Sample processed in 0.25s, ETA: 0.13 min\n",
      "[427/461] Sample processed in 0.21s, ETA: 0.13 min\n",
      "[428/461] Sample processed in 0.25s, ETA: 0.12 min\n",
      "[429/461] Sample processed in 0.25s, ETA: 0.12 min\n",
      "[430/461] Sample processed in 0.25s, ETA: 0.12 min\n",
      "[431/461] Sample processed in 0.25s, ETA: 0.11 min\n",
      "[432/461] Sample processed in 0.21s, ETA: 0.11 min\n",
      "[433/461] Sample processed in 0.25s, ETA: 0.11 min\n",
      "[434/461] Sample processed in 0.25s, ETA: 0.10 min\n",
      "[435/461] Sample processed in 0.25s, ETA: 0.10 min\n",
      "[436/461] Sample processed in 0.25s, ETA: 0.09 min\n",
      "[437/461] Sample processed in 0.25s, ETA: 0.09 min\n",
      "[438/461] Sample processed in 0.25s, ETA: 0.09 min\n",
      "[439/461] Sample processed in 0.25s, ETA: 0.08 min\n",
      "[440/461] Sample processed in 0.25s, ETA: 0.08 min\n",
      "[441/461] Sample processed in 0.21s, ETA: 0.08 min\n",
      "[442/461] Sample processed in 0.25s, ETA: 0.07 min\n",
      "[443/461] Sample processed in 0.25s, ETA: 0.07 min\n",
      "[444/461] Sample processed in 0.25s, ETA: 0.06 min\n",
      "[445/461] Sample processed in 0.25s, ETA: 0.06 min\n",
      "[446/461] Sample processed in 0.25s, ETA: 0.06 min\n",
      "[447/461] Sample processed in 0.25s, ETA: 0.05 min\n",
      "[448/461] Sample processed in 0.25s, ETA: 0.05 min\n",
      "[449/461] Sample processed in 0.25s, ETA: 0.05 min\n",
      "[450/461] Sample processed in 0.25s, ETA: 0.04 min\n",
      "[451/461] Sample processed in 0.25s, ETA: 0.04 min\n",
      "[452/461] Sample processed in 0.25s, ETA: 0.03 min\n",
      "[453/461] Sample processed in 0.25s, ETA: 0.03 min\n",
      "[454/461] Sample processed in 0.25s, ETA: 0.03 min\n",
      "[455/461] Sample processed in 0.25s, ETA: 0.02 min\n",
      "[456/461] Sample processed in 0.25s, ETA: 0.02 min\n",
      "[457/461] Sample processed in 0.25s, ETA: 0.02 min\n",
      "[458/461] Sample processed in 0.25s, ETA: 0.01 min\n",
      "[459/461] Sample processed in 0.25s, ETA: 0.01 min\n",
      "[460/461] Sample processed in 0.25s, ETA: 0.00 min\n",
      "[461/461] Sample processed in 0.25s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 1.75 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def generate_response(instruction_text, input_text, max_tokens):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            instruction_text, # instruction\n",
    "            input_text, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=max_tokens, \n",
    "                             use_cache=True,)\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "\n",
    "    return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "# ì‹¤í–‰ \n",
    "input_json_path = \"/data/jaesung/llm_for_diabetes/src/model/mLoRA/demo/data_classification_test.json\"\n",
    "output_json_path = \"/data/jaesung/llm_for_diabetes/src/model/unsloth/model_output/classification_model_output.jsonl\"\n",
    "\n",
    "data = load_json(input_json_path)\n",
    "\n",
    "start_time = time.time()\n",
    "total_samples = len(data)\n",
    "for idx, item in enumerate(data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "\n",
    "    model_output_128 = generate_response(instruction, input_text, 128)\n",
    "    # model_output_2048 = generate_response(instruction, input_text, 1024)\n",
    "\n",
    "    output_data = item.copy()\n",
    "    output_data.update({\n",
    "        \"model_output_128\": model_output_128,\n",
    "        # \"model_output_2048\": model_output_2048,\n",
    "    })\n",
    "    save_to_jsonl(output_json_path, output_data)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDI-false<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Analyze the sentence with two drugs labeled as @DRUG_A$ and @DRUG_B$. Extract the interaction between @DRUG_A$ and @DRUG_B$ from the input sentence by selecting only one of the following options: 'DDI-effect', 'DDI-mechanism', 'DDI-advise', 'DDI-false', and 'DDI-int'. 'DDI-effect': Choose this when the interaction describes an effect or a pharmacodynamic mechanism. 'DDI-mechanism': Choose this for interactions explained by pharmacokinetic mechanisms. 'DDI-advise': Choose this when the sentence provides a recommendation or advice about the drug interaction. 'DDI-false': Choose this if there is no actual drug-drug interaction in the sentence. 'DDI-int': Choose this when a drug-drug interaction is mentioned without additional detail.\", # instruction\n",
    "        \"@DRUG_A$ did not interfere with the absorption or disposition of the @DRUG_B$ glyburide in diabetic patients.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "\n",
    "print(response_texts[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meal_kernel",
   "language": "python",
   "name": "meal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
