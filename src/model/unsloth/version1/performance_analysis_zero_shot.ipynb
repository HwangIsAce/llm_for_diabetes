{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 10:48:06.548656: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-06 10:48:06.565156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738806486.582608 2833459 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738806486.587950 2833459 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-06 10:48:06.610204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a daily diet that includes a specific ingredient.\n",
      "\n",
      "### Input:\n",
      "Create a diet that includes baby bok choy(roots trimmed and roughly chopped).\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Recommend a daily diet that includes a specific ingredient.\",\n",
    "\n",
    "        \"Create a diet that includes baby bok choy(roots trimmed and roughly chopped).\",\n",
    "\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/data/jaesung/llm_for_diabetes/src/model/inference_results_test.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    daily_diets: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    generation: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    alternative_diet: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    summarization: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    re: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    qa1: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    qa2: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    qa3: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    nli: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    ie: Dataset({\n",
       "        features: ['dataset', 'split_data', 'task', 'instruction', 'input', 'output', 'text_length'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "columns_to_keep = [\"dataset\", \"split_data\", \"task\", \"instruction\", \"input\", \"output\", \"text_length\"]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "task_grouped_data = {task: df[df[\"task\"] == task] for task in df[\"task\"].unique()}\n",
    "\n",
    "sampled_dataset = DatasetDict({task: Dataset.from_pandas(task_df, preserve_index=False) for task, task_df in task_grouped_data.items()})\n",
    "\n",
    "sampled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [29:21<00:00,  7.05s/sample] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "output_file = \"inference_results_zero_shot.jsonl\"\n",
    "\n",
    "\n",
    "task_max_new_tokens = {\n",
    "    \"alternative_diet\": 512,\n",
    "    \"daily_diets\": 1024,\n",
    "    \"dfh_info\": 256,\n",
    "    \"generation\": 2048,\n",
    "    \"ie\": 128,\n",
    "    \"nli\": 32,\n",
    "    \"qa_objective_1\": 64,\n",
    "    \"qa_objective_2\": 64,\n",
    "    \"qa_objective_3\": 64,\n",
    "    \"re\": 64,\n",
    "    \"summarization\": 1024, \n",
    "}\n",
    "\n",
    "\n",
    "# 1. `tqdm`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏßÑÌñâ ÏÉÅÌÉú ÌëúÏãú\n",
    "total_samples = sum(len(samples) for samples in sampled_dataset.values())\n",
    "with open(output_file, 'w') as f_out:\n",
    "    with tqdm(total=total_samples, desc=\"Processing samples\", unit=\"sample\") as pbar:\n",
    "        for task, sample in sampled_dataset.items():\n",
    "            for samp in sample:\n",
    "                # Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶ÑÏóê Îî∞Î•∏ context length ÏÑ§Ï†ï\n",
    "                task_name = samp.get(\"task\", \"\")\n",
    "                # max_new_tokens = 8192 if dataset_name in [\"meddialog\", \"pubmed\"] else 8\n",
    "                max_new_tokens = task_max_new_tokens.get(task_name, 8)  # Í∏∞Î≥∏Í∞í 8\n",
    "\n",
    "\n",
    "                # ÏûÖÎ†• ÌÜ†ÌÅ∞ ÏÉùÏÑ±\n",
    "                inputs = tokenizer(\n",
    "                    [\n",
    "                        alpaca_prompt.format(\n",
    "                            samp['instruction'],  # instruction\n",
    "                            samp['input'],  # input\n",
    "                            \"\",  # output\n",
    "                        )\n",
    "                    ], return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "                # TextStreamer ÏÑ§Ï†ï\n",
    "                text_streamer = TextStreamer(tokenizer)\n",
    "                \n",
    "                # Î™®Îç∏ ÏÉùÏÑ± Î∞è Ï∂úÎ†•\n",
    "                output_tensor = model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=max_new_tokens\n",
    "                )\n",
    "                model_output = tokenizer.decode(output_tensor[0], skip_special_tokens=True)\n",
    "\n",
    "                # `### Response:` Îí§Ïùò ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú\n",
    "                response_text = None\n",
    "                response_match = re.search(r\"### Response:\\s*(.+)\", model_output, re.DOTALL)\n",
    "                if response_match:\n",
    "                    response_text = response_match.group(1).strip()\n",
    "                else:\n",
    "                    response_text = \"No valid response found\"\n",
    "\n",
    "                # Î™®Îç∏ Ï∂úÎ†• Í≤∞Í≥ºÎ•º sampÏóê Ï∂îÍ∞Ä\n",
    "                samp['model_output'] = response_text\n",
    "\n",
    "                # JSONL ÌòïÏãùÏúºÎ°ú Ï†ÄÏû•\n",
    "                f_out.write(json.dumps(samp, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                # tqdm ÏßÑÌñâ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/data/jaesung/llm_for_diabetes/src/model/inference_results_zero_shot.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2833459/1017709785.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_objective_df['output_label'] = qa_objective_df['output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
      "/tmp/ipykernel_2833459/1017709785.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_objective_df['model_output_label'] = qa_objective_df['model_output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
      "/tmp/ipykernel_2833459/1017709785.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_objective_df['correct'] = qa_objective_df['output_label'] == qa_objective_df['model_output_label']\n"
     ]
    }
   ],
   "source": [
    "# medqa\n",
    "\n",
    "import pandas as pd\n",
    "qa_objective_df = df[df['task']=='qa1']\n",
    "\n",
    "# 'output'ÏôÄ 'model_output'ÏóêÏÑú A), B), C), D)Îßå Ï∂îÏ∂ú\n",
    "qa_objective_df['output_label'] = qa_objective_df['output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
    "qa_objective_df['model_output_label'] = qa_objective_df['model_output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
    "\n",
    "# Îëê Ïª¨Îüº ÎπÑÍµêÌïòÏó¨ ÎßûÏùÄ Í≤ΩÏö∞Î•º Í≥ÑÏÇ∞\n",
    "qa_objective_df['correct'] = qa_objective_df['output_label'] == qa_objective_df['model_output_label']\n",
    "\n",
    "# Accuracy Í≥ÑÏÇ∞\n",
    "accuracy = qa_objective_df['correct'].mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2833459/2145043051.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_subjective_df['output_label'] = qa_subjective_df['output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
      "/tmp/ipykernel_2833459/2145043051.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_subjective_df['model_output_label'] = qa_subjective_df['model_output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
      "/tmp/ipykernel_2833459/2145043051.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_subjective_df['correct'] = qa_subjective_df['output_label'] == qa_subjective_df['model_output_label']\n"
     ]
    }
   ],
   "source": [
    "# medmcqa\n",
    "\n",
    "import pandas as pd\n",
    "qa_subjective_df = df[df['task']=='qa2']\n",
    "\n",
    "# 'output'ÏôÄ 'model_output'ÏóêÏÑú A), B), C), D)Îßå Ï∂îÏ∂ú\n",
    "qa_subjective_df['output_label'] = qa_subjective_df['output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
    "qa_subjective_df['model_output_label'] = qa_subjective_df['model_output'].str.extract(r'(A\\)|B\\)|C\\)|D\\))')\n",
    "\n",
    "# Îëê Ïª¨Îüº ÎπÑÍµêÌïòÏó¨ ÎßûÏùÄ Í≤ΩÏö∞Î•º Í≥ÑÏÇ∞\n",
    "qa_subjective_df['correct'] = qa_subjective_df['output_label'] == qa_subjective_df['model_output_label']\n",
    "\n",
    "# Accuracy Í≥ÑÏÇ∞\n",
    "accuracy = qa_subjective_df['correct'].mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Match Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2833459/841677399.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_descriptive_df['token_match_score'] = qa_descriptive_df.apply(lambda row: token_overlap(row['output'], row['model_output']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# pubmedqa\n",
    "\n",
    "def token_overlap(output, model_output):\n",
    "    # ÌÜ†ÌÅ∞Ìôî\n",
    "    output_tokens = set(output.split())\n",
    "    model_output_tokens = set(model_output.split())\n",
    "    \n",
    "    # Í≥µÌÜµ ÌÜ†ÌÅ∞ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "    common_tokens = output_tokens.intersection(model_output_tokens)\n",
    "    \n",
    "    # ÏùºÏπò ÎπÑÏú® Í≥ÑÏÇ∞\n",
    "    return len(common_tokens) / len(output_tokens) if len(output_tokens) > 0 else 0\n",
    "\n",
    "qa_descriptive_df = df[df['task'] == 'qa3']\n",
    "\n",
    "# ÏùºÏπò ÎπÑÏú® Í≥ÑÏÇ∞ Î∞è Ï†ÄÏû•\n",
    "qa_descriptive_df['token_match_score'] = qa_descriptive_df.apply(lambda row: token_overlap(row['output'], row['model_output']), axis=1)\n",
    "\n",
    "# ÌèâÍ∑† Ï†êÏàòÎ•º Ï†ïÌôïÎèÑÎ°ú Í∞ÑÏ£º\n",
    "accuracy = qa_descriptive_df['token_match_score'].mean()\n",
    "\n",
    "print(f\"Token Match Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24\n"
     ]
    }
   ],
   "source": [
    "# nli\n",
    "\n",
    "nli_df = df[df['task'] == 'nli']\n",
    "\n",
    "correct_predictions = (nli_df['output'] == nli_df['model_output']).sum()\n",
    "total_predictions = len(nli_df) \n",
    "\n",
    "nli_acc = correct_predictions / total_predictions\n",
    "\n",
    "print(nli_acc)  # bioinstruct - 0.33 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Precision: 0.0000\n",
      "Total Recall: 0.0000\n",
      "Total F1-Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_re = df[df['task'] == 're']\n",
    "\n",
    "# Precision, Recall, F1 Í≥ÑÏÇ∞ Ìï®Ïàò (Ï†ÑÏ≤¥)\n",
    "def calculate_total_metrics(output_col, model_output_col):\n",
    "    total_true_positive = 0\n",
    "    total_false_positive = 0\n",
    "    total_false_negative = 0\n",
    "\n",
    "    for output, model_output in zip(output_col, model_output_col):\n",
    "        # ','Î°ú Íµ¨Î∂ÑÎêú Î¨∏ÏûêÏó¥ÏùÑ ÏßëÌï©ÏúºÎ°ú Î≥ÄÌôò\n",
    "        output_set = set(output.split(', '))\n",
    "        model_output_set = set(model_output.split(', '))\n",
    "\n",
    "        # ÍµêÏßëÌï©, Ï†ïÎãµÏùò ÌÅ¨Í∏∞, Î™®Îç∏ ÏòàÏ∏°Ïùò ÌÅ¨Í∏∞ Í≥ÑÏÇ∞\n",
    "        true_positive = len(output_set & model_output_set)\n",
    "        false_positive = len(model_output_set - output_set)\n",
    "        false_negative = len(output_set - model_output_set)\n",
    "\n",
    "        # ÎàÑÏ†Å Ìï©ÏÇ∞\n",
    "        total_true_positive += true_positive\n",
    "        total_false_positive += false_positive\n",
    "        total_false_negative += false_negative\n",
    "\n",
    "    # Ï¥ù Precision, Recall, F1 Í≥ÑÏÇ∞\n",
    "    precision = total_true_positive / (total_true_positive + total_false_positive) if (total_true_positive + total_false_positive) > 0 else 0\n",
    "    recall = total_true_positive / (total_true_positive + total_false_negative) if (total_true_positive + total_false_negative) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞\n",
    "total_precision, total_recall, total_f1 = calculate_total_metrics(df_re['output'], df_re['model_output'])\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(f\"Total Precision: {total_precision:.4f}\")\n",
    "print(f\"Total Recall: {total_recall:.4f}\")\n",
    "print(f\"Total F1-Score: {total_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1102\n",
      "Recall: 0.1816\n",
      "F1-score: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2833459/1779761820.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ie[\"output\"] = df_ie[\"output\"].str.lower().str.split(\", \")\n",
      "/tmp/ipykernel_2833459/1779761820.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ie[\"model_output\"] = df_ie[\"model_output\"].str.lower().str.split(\", \")\n"
     ]
    }
   ],
   "source": [
    "# ie\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏòàÏãú (df_ie Í∞Ä Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ)\n",
    "df_ie = df[df['task'] == 'ie']\n",
    "\n",
    "df_ie[\"output\"] = df_ie[\"output\"].str.lower().str.split(\", \")\n",
    "df_ie[\"model_output\"] = df_ie[\"model_output\"].str.lower().str.split(\", \")\n",
    "\n",
    "# Precision, Recall, F1-score Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def calculate_scores(y_true, y_pred):\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "    \n",
    "    for true_vals, pred_vals in zip(y_true, y_pred):\n",
    "        true_set = set(true_vals) if isinstance(true_vals, list) else set()\n",
    "        pred_set = set(pred_vals) if isinstance(pred_vals, list) else set()\n",
    "\n",
    "        TP = len(true_set & pred_set)  # True Positives (Ï†ïÎãµÍ≥º ÏòàÏ∏°Ïù¥ ÏùºÏπòÌïòÎäî Í≤É)\n",
    "        FP = len(pred_set - true_set)  # False Positives (ÏòàÏ∏°ÌñàÏßÄÎßå Ï†ïÎãµÏù¥ ÏïÑÎãå Í≤É)\n",
    "        FN = len(true_set - pred_set)  # False Negatives (Ï†ïÎãµÏù¥ÏßÄÎßå ÏòàÏ∏°ÌïòÏßÄ Î™ªÌïú Í≤É)\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    return sum(all_precisions) / len(all_precisions), sum(all_recalls) / len(all_recalls), sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "# Precision, Recall, F1-score Í≥ÑÏÇ∞\n",
    "precision, recall, f1 = calculate_scores(df_ie[\"output\"], df_ie[\"model_output\"])\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0\n",
      "- Completeness: 1.5\n",
      "- Naturalness: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided model's response and the true answer, here are the evaluations for each metric:\n",
      "\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 2.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 5.0\n",
      "- Naturalness: 4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided information, here is the evaluation of the QA model's response:\n",
      "\n",
      "- Coherence: 4.0\n",
      "The model's response is generally coherent with the input text, as it covers the same topic and includes relevant details about the study methods and results. However, it repeats the information in the input without providing additional insights.\n",
      "\n",
      "- Completeness: 3.0\n",
      "The model's response partially addresses the question in the input by discussing the association between road traffic noise and type 2 diabetes and mentioning the methods and results of the study. However, it lacks a succinct and concise summary of the key findings.\n",
      "\n",
      "- Naturalness: 3.5\n",
      "The model's response sounds somewhat fluent and structured, mirroring the style of a research study report. However, certain parts seem overly repetitive and could be improved for better readability and flow.\n",
      "\n",
      "Overall, the model's response shows potential but could benefit from improvements in providing a more concise and informative answer with better flow and structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 5.0\n",
      "- Naturalness: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided input and model's response, here is my evaluation:\n",
      "\n",
      "- Coherence: 4.0\n",
      "The model's response is mostly coherent with the context provided in the input. It correctly identifies diabetes mellitus as a condition associated with critical limb ischemia (CLI) and mentions the role of inadequate blood supply due to blocked or narrowed vessels as a cause of CLI. However, the response could have directly connected the excessive ROS production and oxidative stress mentioned in the input to the development of CLI for better coherence.\n",
      "\n",
      "- Completeness: 4.5\n",
      "The model's response is quite complete in addressing the topic of critical limb ischemia (CLI) associated with diabetes mellitus. It covers the definition and severity of CLI, the importance of immediate medical attention, and even touches upon the possible treatment strategies involving ROS reduction and lifestyle modifications. However, a more direct link between ROS/oxidative stress and CLI could have improved completeness.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "The response sounds fluent and human-like, presenting the information in a clear and understandable manner. The language used is appropriate for the topic and maintains a professional tone throughout. However, there are some areas where the response could be more concise and focused on directly answering the query without unnecessary elaboration.\n",
      "\n",
      "Overall, the model's response is quite good in terms of coherence, completeness, and naturalness, with some areas for improvement in explicitly connecting the role of ROS and oxidative stress to CLI and refining the response for better conciseness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 5.0\n",
      "- Naturalness: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌèâÍ∑† Ï†êÏàò:\n",
      "Coherence       4.440000\n",
      "Completeness    4.020000\n",
      "Naturalness     4.000000\n",
      "BLEURT          0.337864\n",
      "BERTScore_F1    0.759047\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summarization\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT Î™®Îç∏ Î°úÎìú\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 ÌèâÍ∞Ä Ìï®Ïàò\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = input_text[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = model_output[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = true_output[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 Ï†êÏàò Ï∂îÏ∂ú\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT Ï†êÏàò Í≥ÑÏÇ∞\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT Î∞è BERTScore Í≥ÑÏÇ∞\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# Ï†êÏàò Ï†ïÍ∑úÌôî Ìï®Ïàò\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "qa_df = df[df['task'] == 'summarization']\n",
    "results = []\n",
    "\n",
    "for _, row in qa_df.iterrows():\n",
    "    input_text, model_output, true_output = row['input'], row['model_output'], row['output']\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' Ïª¨ÎüºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ ÌõÑ Ï†ïÍ∑úÌôî ÏàòÌñâ\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"ÌèâÍ∑† Ï†êÏàò:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 5.0\n",
      "- Naturalness: 4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the metrics provided, here is the evaluation of the QA model's response:\n",
      "\n",
      "- Coherence: 1.0\n",
      "- Completeness: 1.0\n",
      "- Naturalness: 1.0\n",
      "\n",
      "Overall, the response lacks coherence and completeness as it veers off the main question and provides an extensive list of irrelevant queries. Additionally, the response is unnatural and does not sound human-like. The response does not address the initial query about the high white blood cell count and instead asks for a vast amount of unnecessary details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- **Coherence**: 4.5\n",
      "- **Completeness**: 4.5\n",
      "- **Naturalness**: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided input and the model's response, here is the evaluation of the model's performance:\n",
      "\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.0\n",
      "\n",
      "Overall, the model's response is coherent, providing relevant information and advice based on the symptoms described by the patient. The response is medically accurate and detailed, addressing possible causes and providing practical recommendations. However, there is room for improvement in completeness as the response could elaborate more on certain aspects such as the potential causes of the condition. Additionally, while the response is professional and informative, it could be refined to sound more natural and empathetic in addressing the patient's concerns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 1.0\n",
      "- Completeness: 1.0\n",
      "- Naturalness: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n",
      "\n",
      "Overall, the model's response shows good coherence with the input context, providing appropriate recommendations for further evaluation and treatment. It addresses the complications faced by the patient and emphasizes the importance of consulting with a specialist. The response is medically accurate and aligned with guidelines. The language used is professional, but it could be slightly more natural in tone to better mimic a human doctor's response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the given information, here is the evaluation of the model's response:\n",
      "\n",
      "- **Coherence**: 4.0\n",
      "The response demonstrates good coherence by addressing the patient's medical history and symptoms, suggesting potential causes, and recommending tests and treatments. However, it could benefit from more detailed explanations and connections between the symptoms and proposed solutions.\n",
      "\n",
      "- **Completeness**: 3.5\n",
      "The response partially answers the patient's concerns by suggesting potential conditions like IBS, Crohn's disease, and Amebiasis, along with lifestyle recommendations. However, it lacks specific details on how these conditions relate to the symptoms described by the patient and could provide a more thorough explanation of the proposed treatments.\n",
      "\n",
      "- **Naturalness**: 4.0\n",
      "The response sounds professional and informative, providing medical advice in a clear and concise manner. It maintains a formal tone suitable for a medical consultation. However, it could be enhanced by offering more empathy and understanding towards the patient's health struggles.\n",
      "\n",
      "Overall, the model's response shows promise in coherence and naturalness but could improve in completeness by providing more detailed and personalized recommendations for the patient's specific health issues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.5\n",
      "- Completeness: 3.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided information, here is the evaluation of the QA model's response:\n",
      "\n",
      "- Coherence: 4.5\n",
      "The response logically aligns with the medical context provided in the input by addressing the presence of proteins in urine and pus cells as indicators of kidney and urinary tract issues, respectively. The advice to consult a doctor for further evaluation makes sense in this context.\n",
      "\n",
      "- Completeness: 4.0\n",
      "The response sufficiently answers the question by addressing the medical implications of the test results and providing general advice for the patient to seek further evaluation. The mention of the kidneys and urinary tract infection covers the key aspects, but additional details on potential treatments could enhance completeness.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "The response sounds fluent and professional, maintaining a formal tone suitable for addressing medical concerns. The empathy towards the patient and the clarity in communication contribute to the naturalness of the response.\n",
      "\n",
      "Overall, the QA model's response demonstrates good coherence, completeness, and naturalness in addressing the medical query provided in the input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.0\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "Based on the provided context, here is the evaluation of the QA model's response:\n",
      "\n",
      "- Coherence: 1.5\n",
      "- Completeness: 1.0\n",
      "- Naturalness: 1.0\n",
      "\n",
      "Overall, the model's response lacks coherence, completeness, and naturalness. The response does not provide any information or answer related to the input given by the user, and it sounds very unnatural and incomplete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 3.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0\n",
      "- Completeness: 2.5\n",
      "- Naturalness: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌèâÍ∑† Ï†êÏàò:\n",
      "Coherence       3.440000\n",
      "Completeness    3.180000\n",
      "Naturalness     3.140000\n",
      "BLEURT          0.545510\n",
      "BERTScore_F1    0.884018\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# generation\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT Î™®Îç∏ Î°úÎìú\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 ÌèâÍ∞Ä Ìï®Ïàò\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = input_text[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = model_output[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = true_output[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 Ï†êÏàò Ï∂îÏ∂ú\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT Ï†êÏàò Í≥ÑÏÇ∞\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT Î∞è BERTScore Í≥ÑÏÇ∞\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# Ï†êÏàò Ï†ïÍ∑úÌôî Ìï®Ïàò\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "qa_df = df[df['task'] == 'generation']\n",
    "results = []\n",
    "\n",
    "for _, row in qa_df.iterrows():\n",
    "    input_text, model_output, true_output = row['input'], row['model_output'], row['output']\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' Ïª¨ÎüºÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ ÌõÑ Ï†ïÍ∑úÌôî ÏàòÌñâ\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"ÌèâÍ∑† Ï†êÏàò:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating daily diets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:20<00:00,  1.03it/s]\n",
      "Evaluating alternative diets: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Diets Average Scores:\n",
      "Coherence               4.166667\n",
      "Completeness            3.285714\n",
      "Naturalness             3.752381\n",
      "Nutritional Adequacy    3.628571\n",
      "Caloric Balance         3.838095\n",
      "dtype: float64\n",
      "\n",
      "Alternative Diets scores not available:\n",
      "Missing columns: ['Improvement', 'Suitability']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# daily diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output, task_type):\n",
    "    input_text = input_text[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = model_output[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = true_output[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    if task_type == \"daily_diets\":\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with evaluating the quality of a meal recommendation model's responses based on the following metrics:\n",
    "        1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "        2. **Completeness**: Does the model's response sufficiently answer the input request?\n",
    "        3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "        4. **Nutritional Adequacy**: Does the meal align with the nutritional goals in the input, considering reasonable flexibility and practical applicability in real-life scenarios?\n",
    "        5. **Caloric Balance**: Are the recommended meals well-balanced in terms of calories?\n",
    "\n",
    "        **Input**:\n",
    "        {input_text}\n",
    "\n",
    "        **Model's Response**:\n",
    "        {model_output}\n",
    "\n",
    "        **True Answer**:\n",
    "        {true_output}\n",
    "\n",
    "        Please rate each metric on a scale from 1 to 5. \n",
    "        Example response format:\n",
    "        - Coherence: X.X\n",
    "        - Completeness: X.X\n",
    "        - Naturalness: X.X\n",
    "        - Nutritional Adequacy: X.X\n",
    "        - Caloric Balance: X.X\n",
    "        \"\"\"\n",
    "    elif task_type == \"alternative_diets\":\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with evaluating the quality of a meal recommendation model's responses based on the following metrics:\n",
    "        1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "        2. **Completeness**: Does the model's response sufficiently answer the input request?\n",
    "        3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "        4. **Improvement**: Does the recommended meal address the shortcomings of the previous meal?\n",
    "        5. **Suitability**: Is the recommended meal suitable for a diabetes patient?\n",
    "\n",
    "        **Input**:\n",
    "        {input_text}\n",
    "\n",
    "        **Model's Response**:\n",
    "        {model_output}\n",
    "\n",
    "        **True Answer**:\n",
    "        {true_output}\n",
    "\n",
    "        Please rate each metric on a scale from 1 to 5. \n",
    "        Example response format:\n",
    "        - Coherence: X.X\n",
    "        - Completeness: X.X\n",
    "        - Naturalness: X.X\n",
    "        - Improvement: X.X\n",
    "        - Suitability: X.X\n",
    "        \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for meal recommendation models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# Ï†êÏàò Ï∂îÏ∂ú Ìï®Ïàò\n",
    "def extract_scores(evaluation, task_type):\n",
    "    if evaluation is None:\n",
    "        if task_type == \"daily_diets\":\n",
    "            return {\n",
    "                \"Coherence\": 0.0,\n",
    "                \"Completeness\": 0.0,\n",
    "                \"Naturalness\": 0.0,\n",
    "                \"Nutritional Adequacy\": 0.0,\n",
    "                \"Caloric Balance\": 0.0\n",
    "            }\n",
    "        elif task_type == \"alternative_diets\":\n",
    "            return {\n",
    "                \"Coherence\": 0.0,\n",
    "                \"Completeness\": 0.0,\n",
    "                \"Naturalness\": 0.0,\n",
    "                \"Improvement\": 0.0,\n",
    "                \"Suitability\": 0.0\n",
    "            }\n",
    "\n",
    "    scores = {}\n",
    "    if task_type == \"daily_diets\":\n",
    "        metrics = [\"Coherence\", \"Completeness\", \"Naturalness\", \"Nutritional Adequacy\", \"Caloric Balance\"]\n",
    "    elif task_type == \"alternative_diets\":\n",
    "        metrics = [\"Coherence\", \"Completeness\", \"Naturalness\", \"Improvement\", \"Suitability\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        match = re.search(fr\"{metric}: (\\d\\.\\d)\", evaluation)\n",
    "        scores[metric] = float(match.group(1)) if match else 0.0\n",
    "\n",
    "    return scores\n",
    "\n",
    "results = []\n",
    "\n",
    "daily_df = df[(df['task'] == 'daily_diets') & (df['output'].str.contains('\"Breakfast\"'))]\n",
    "\n",
    "for _, row in tqdm(daily_df.iterrows(), total=len(daily_df), desc=\"Evaluating daily diets\"):\n",
    "    evaluation = evaluate_with_gpt4(row['input'], row['model_output'], row['output'], \"daily_diets\")\n",
    "    scores = extract_scores(evaluation, \"daily_diets\")\n",
    "    results.append({**row.to_dict(), **scores})\n",
    "\n",
    "alternative_df = df[df['task'] == 'alternative_diets']\n",
    "\n",
    "for _, row in tqdm(alternative_df.iterrows(), total=len(alternative_df), desc=\"Evaluating alternative diets\"):\n",
    "    evaluation = evaluate_with_gpt4(row['input'], row['model_output'], row['output'], \"alternative_diets\")\n",
    "    scores = extract_scores(evaluation, \"alternative_diets\")\n",
    "    results.append({**row.to_dict(), **scores})\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "if \"Nutritional Adequacy\" in evaluation_df.columns and \"Caloric Balance\" in evaluation_df.columns:\n",
    "    daily_avg = evaluation_df[evaluation_df['task'] == 'daily_diets'][[\n",
    "        \"Coherence\", \"Completeness\", \"Naturalness\", \n",
    "        \"Nutritional Adequacy\", \"Caloric Balance\"\n",
    "    ]].mean()\n",
    "    print(\"Daily Diets Average Scores:\")\n",
    "    print(daily_avg)\n",
    "\n",
    "if \"Improvement\" in evaluation_df.columns and \"Suitability\" in evaluation_df.columns:\n",
    "    alternative_avg = evaluation_df[evaluation_df['task'] == 'alternative_diets'][[\n",
    "        \"Coherence\", \"Completeness\", \"Naturalness\", \n",
    "        \"Improvement\", \"Suitability\"\n",
    "    ]].mean()\n",
    "    print(\"\\nAlternative Diets Average Scores:\")\n",
    "    print(alternative_avg)\n",
    "else:\n",
    "    missing_columns = [col for col in [\"Improvement\", \"Suitability\"] if col not in evaluation_df.columns]\n",
    "    print(\"\\nAlternative Diets scores not available:\")\n",
    "    print(f\"Missing columns: {missing_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating alternative diets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:22<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alternative Diets Average Scores:\n",
      "Coherence       4.14\n",
      "Completeness    3.90\n",
      "Naturalness     3.88\n",
      "Improvement     3.72\n",
      "Suitability     4.14\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# alternative diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output, task_type):\n",
    "\n",
    "    input_text = input_text[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = model_output[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = true_output[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    if task_type == \"daily_diets\":\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with evaluating the quality of a meal recommendation model's responses based on the following metrics:\n",
    "        1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "        2. **Completeness**: Does the model's response sufficiently answer the input request?\n",
    "        3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "        4. **Nutritional Adequacy**: Does the meal response meet the nutritional goals mentioned in the input?\n",
    "        5. **Caloric Balance**: Are the recommended meals well-balanced in terms of calories?\n",
    "\n",
    "        **Input**:\n",
    "        {input_text}\n",
    "\n",
    "        **Model's Response**:\n",
    "        {model_output}\n",
    "\n",
    "        **True Answer**:\n",
    "        {true_output}\n",
    "\n",
    "        Please rate each metric on a scale from 1 to 5. \n",
    "        Example response format:\n",
    "        - Coherence: X.X\n",
    "        - Completeness: X.X\n",
    "        - Naturalness: X.X\n",
    "        - Nutritional Adequacy: X.X\n",
    "        - Caloric Balance: X.X\n",
    "        \"\"\"\n",
    "    elif task_type == \"alternative_diet\":\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with evaluating the quality of a meal recommendation model's responses based on the following metrics:\n",
    "\n",
    "        1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "        2. **Completeness**: Does the model's response sufficiently answer the input request?\n",
    "        3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "        4. **Improvement**: Assume that the recommended alternative meal is an improvement over the previous meal. Evaluate how effectively it builds upon and enhances the previous meal, even if the changes are small or subtle.\n",
    "        5. **Suitability**: Is the recommended meal suitable for a diabetes patient?\n",
    "\n",
    "        **Input**:\n",
    "        {input_text}\n",
    "\n",
    "        **Model's Response**:\n",
    "        {model_output}\n",
    "\n",
    "        **True Answer**:\n",
    "        {true_output}\n",
    "\n",
    "        Please rate each metric on a scale from 1 to 5. \n",
    "        Example response format:\n",
    "        - Coherence: X.X\n",
    "        - Completeness: X.X\n",
    "        - Naturalness: X.X\n",
    "        - Improvement: X.X\n",
    "        - Suitability: X.X\n",
    "        \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for meal recommendation models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "def extract_scores(evaluation, task_type):\n",
    "    if evaluation is None:\n",
    "        if task_type == \"daily_diets\":\n",
    "            return {\n",
    "                \"Coherence\": 0.0,\n",
    "                \"Completeness\": 0.0,\n",
    "                \"Naturalness\": 0.0,\n",
    "                \"Nutritional Adequacy\": 0.0,\n",
    "                \"Caloric Balance\": 0.0\n",
    "            }\n",
    "        elif task_type == \"alternative_diet\":\n",
    "            return {\n",
    "                \"Coherence\": 0.0,\n",
    "                \"Completeness\": 0.0,\n",
    "                \"Naturalness\": 0.0,\n",
    "                \"Improvement\": 0.0,\n",
    "                \"Suitability\": 0.0\n",
    "            }\n",
    "\n",
    "    scores = {}\n",
    "    if task_type == \"daily_diets\":\n",
    "        metrics = [\"Coherence\", \"Completeness\", \"Naturalness\", \"Nutritional Adequacy\", \"Caloric Balance\"]\n",
    "    elif task_type == \"alternative_diet\":\n",
    "        metrics = [\"Coherence\", \"Completeness\", \"Naturalness\", \"Improvement\", \"Suitability\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        match = re.search(fr\"{metric}: (\\d\\.\\d)\", evaluation)\n",
    "        scores[metric] = float(match.group(1)) if match else 0.0\n",
    "\n",
    "    return scores\n",
    "\n",
    "results = []\n",
    "\n",
    "alternative_df = df[df['task'] == 'alternative_diet']\n",
    "\n",
    "for _, row in tqdm(alternative_df.iterrows(), total=len(alternative_df), desc=\"Evaluating alternative diets\"):\n",
    "    evaluation = evaluate_with_gpt4(row['input'], row['model_output'], row['output'], \"alternative_diet\")\n",
    "    scores = extract_scores(evaluation, \"alternative_diet\")\n",
    "    results.append({**row.to_dict(), **scores})\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "if all(col in evaluation_df.columns for col in [\"Improvement\", \"Suitability\"]):\n",
    "    alternative_avg = evaluation_df[[\n",
    "        \"Coherence\", \"Completeness\", \"Naturalness\", \n",
    "        \"Improvement\", \"Suitability\"\n",
    "    ]].mean()\n",
    "    print(\"\\nAlternative Diets Average Scores:\")\n",
    "    print(alternative_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>cook_time</th>\n",
       "      <th>servings</th>\n",
       "      <th>steps</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition_facts</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark</td>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark: Dive into ...</td>\n",
       "      <td>10 min</td>\n",
       "      <td>4 hr</td>\n",
       "      <td>6 Servings</td>\n",
       "      <td>['Cover a freezer-safe tray with parchment pap...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...</td>\n",
       "      <td>{'Servings': '6 Servings', 'Serving Size': '1 ...</td>\n",
       "      <td>[{'label': 'Plain Nonfat Greek yogurt', 'us_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple-Pumpkin Spice Oatmeal Cookies</td>\n",
       "      <td>Description not found</td>\n",
       "      <td>10 min</td>\n",
       "      <td>25 min</td>\n",
       "      <td>14 Servings</td>\n",
       "      <td>['Preheat the oven to 350 degrees F. Line two ...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...</td>\n",
       "      <td>{'Servings': '14 Servings', 'Serving Size': '1...</td>\n",
       "      <td>[{'label': 'old-fashioned rolled oats', 'us_me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0   Raspberry Swirl Frozen Yogurt Bark   \n",
       "1  Maple-Pumpkin Spice Oatmeal Cookies   \n",
       "\n",
       "                                         description prep_time cook_time  \\\n",
       "0  Raspberry Swirl Frozen Yogurt Bark: Dive into ...    10 min      4 hr   \n",
       "1                              Description not found    10 min    25 min   \n",
       "\n",
       "      servings                                              steps  \\\n",
       "0   6 Servings  ['Cover a freezer-safe tray with parchment pap...   \n",
       "1  14 Servings  ['Preheat the oven to 350 degrees F. Line two ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...   \n",
       "1  ['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...   \n",
       "\n",
       "                                     nutrition_facts  \\\n",
       "0  {'Servings': '6 Servings', 'Serving Size': '1 ...   \n",
       "1  {'Servings': '14 Servings', 'Serving Size': '1...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [{'label': 'Plain Nonfat Greek yogurt', 'us_me...  \n",
       "1  [{'label': 'old-fashioned rolled oats', 'us_me...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfh = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data2_daily_diets/diabetes_food_hub_new_nutri_facts.csv\")\n",
    "dfh.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:37<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for Each Row ===\n",
      "Row Index: 0\n",
      "Output Scores: {'Breakfast': {'score': 1.2832821300563242, 'grade': 'B'}, 'Lunch': {'score': 0.12827070932539675, 'grade': 'B'}, 'Dinner': {'score': 0.1844070961718014, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 1\n",
      "Output Scores: {'Breakfast': {'score': 0.27903677232536306, 'grade': 'B'}, 'Lunch': {'score': 1.2434792311769467, 'grade': 'B'}, 'Dinner': {'score': 0.12827070932539675, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 2\n",
      "Output Scores: {'Breakfast': {'score': -0.06054961667206582, 'grade': 'B'}, 'Lunch': {'score': 1.4849885674520331, 'grade': 'B'}, 'Dinner': {'score': 0.12827070932539675, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 3\n",
      "Output Scores: {'Breakfast': {'score': -5.211711711711713, 'grade': 'A'}, 'Lunch': {'score': -0.6524003743136113, 'grade': 'B'}, 'Dinner': {'score': 1.210600194254846, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 4\n",
      "Output Scores: {'Breakfast': {'score': 0.08622999129328224, 'grade': 'B'}, 'Lunch': {'score': 5.048513302034429, 'grade': 'C'}, 'Dinner': {'score': 0.021039548778313044, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 5\n",
      "Output Scores: {'Breakfast': {'score': 1.934614824523082, 'grade': 'B'}, 'Lunch': {'score': -0.13559683963271407, 'grade': 'B'}, 'Dinner': {'score': 0.7137387880113759, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 6\n",
      "Output Scores: {'Breakfast': {'score': 0.912467700258398, 'grade': 'B'}, 'Lunch': {'score': -0.8293254408681712, 'grade': 'B'}, 'Dinner': {'score': 0.12827070932539675, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 7\n",
      "Output Scores: {'Breakfast': {'score': 0.42270531400966216, 'grade': 'B'}, 'Lunch': {'score': 0.8727017632030825, 'grade': 'B'}, 'Dinner': {'score': -0.27594371081766056, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 8\n",
      "Output Scores: {'Breakfast': {'score': 0.8727017632030825, 'grade': 'B'}, 'Lunch': {'score': 0.5100293542074366, 'grade': 'B'}, 'Dinner': {'score': -0.36950770646422804, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 9\n",
      "Output Scores: {'Breakfast': {'score': -2.0825661036928644, 'grade': 'A'}, 'Lunch': {'score': 3.7777777777777803, 'grade': 'C'}, 'Dinner': {'score': 0.12827070932539675, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 10\n",
      "Output Scores: {'Breakfast': {'score': -0.41280659310399087, 'grade': 'B'}, 'Lunch': {'score': 0.2462391862198252, 'grade': 'B'}, 'Dinner': {'score': -0.3906122681790034, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 11\n",
      "Output Scores: {'Breakfast': {'score': 0.12827070932539675, 'grade': 'B'}, 'Lunch': {'score': 3.991531435665514, 'grade': 'C'}, 'Dinner': {'score': 10.100156494522693, 'grade': 'D'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 12\n",
      "Output Scores: {'Breakfast': {'score': 0.8367122426528368, 'grade': 'B'}, 'Lunch': {'score': -0.4020340890818459, 'grade': 'B'}, 'Dinner': {'score': -0.13559683963271407, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 13\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 14\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 15\n",
      "Output Scores: {'Breakfast': {'score': 0.8727017632030825, 'grade': 'B'}, 'Lunch': {'score': -0.4266791593180477, 'grade': 'B'}, 'Dinner': {'score': -0.36950770646422804, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 16\n",
      "Output Scores: {'Breakfast': {'score': -0.3739171323675692, 'grade': 'B'}, 'Lunch': {'score': 0.25068770677669977, 'grade': 'B'}, 'Dinner': {'score': -1.5227480566986737, 'grade': 'A'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 17\n",
      "Output Scores: {'Breakfast': {'score': 2.357681755829904, 'grade': 'C'}, 'Lunch': {'score': 0.12827070932539675, 'grade': 'B'}, 'Dinner': {'score': 0.1844070961718014, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 18\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 19\n",
      "Output Scores: {'Breakfast': {'score': 0.7137387880113759, 'grade': 'B'}, 'Lunch': {'score': -0.11996228751547866, 'grade': 'B'}, 'Dinner': {'score': 1.4849885674520331, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 20\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 21\n",
      "Output Scores: {'Breakfast': {'score': -0.31896724934699616, 'grade': 'B'}, 'Lunch': {'score': 0.037248942130043594, 'grade': 'B'}, 'Dinner': {'score': -2.6285387892530747, 'grade': 'A'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 22\n",
      "Output Scores: {'Breakfast': {'score': 0.4852935434116248, 'grade': 'B'}, 'Lunch': {'score': 0.7137387880113759, 'grade': 'B'}, 'Dinner': {'score': 0.12827070932539675, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 23\n",
      "Output Scores: {'Breakfast': {'score': 0.8727017632030825, 'grade': 'B'}, 'Lunch': {'score': 0.5100293542074366, 'grade': 'B'}, 'Dinner': {'score': -0.36950770646422804, 'grade': 'B'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 24\n",
      "Output Scores: {'Breakfast': {'score': -0.5334085274869877, 'grade': 'B'}, 'Lunch': {'score': -0.34868400536376987, 'grade': 'B'}, 'Dinner': {'score': -1.6295609152752002, 'grade': 'A'}}\n",
      "Model Output Scores: {}\n",
      "\n",
      "=== Overall Averages ===\n",
      "Output Average Nutri-Score: 0.4117810220807128\n",
      "Model Output Average Nutri-Score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# daily diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return all(key in data for key in ['Breakfast', 'Lunch', 'Dinner'])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # Í≥ºÏùº/Ï±ÑÏÜå ÎπÑÏú®ÏùÑ 100g Í∏∞Ï§ÄÏúºÎ°ú Î≥ÄÌôò\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data) if isinstance(data, (int, float, str)) else default\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # Ï†ÑÏ≤¥ Î¨¥Í≤å Í≥ÑÏÇ∞\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g Í∏∞Ï§ÄÏúºÎ°ú ÏÑ±Î∂Ñ Ï†ïÍ∑úÌôî\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_meal_nutri_score(meal_data, dfh):\n",
    "    meal_scores = {}\n",
    "\n",
    "    for meal, title in meal_data.items():\n",
    "        matched_row = find_most_similar_row(title, dfh)\n",
    "        if matched_row is None:\n",
    "            continue\n",
    "\n",
    "        nutrition_facts = matched_row['nutrition_facts']\n",
    "        ingredients = matched_row['ingredients']\n",
    "        score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "\n",
    "        if score is None:\n",
    "            print(f\"Warning: Nutri-Score calculation failed for meal '{meal}' with title '{title}'.\")\n",
    "            grade = \"N/A\"\n",
    "        else:\n",
    "            grade = get_nutri_score_grade(score)\n",
    "\n",
    "        meal_scores[meal] = {'score': score, 'grade': grade}\n",
    "\n",
    "    return meal_scores\n",
    "\n",
    "def calculate_scores_with_comparison(df, dfh):\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        output_scores = {}\n",
    "        model_scores = {}\n",
    "        if is_valid_meal_structure(row.get('output', '')):\n",
    "            output_data = json.loads(row['output'])\n",
    "            output_scores = calculate_meal_nutri_score(output_data, dfh)\n",
    "        if is_valid_meal_structure(row.get('model_output', '')):\n",
    "            model_data = json.loads(row['model_output'])\n",
    "            model_scores = calculate_meal_nutri_score(model_data, dfh)\n",
    "        results.append({'row_index': idx, 'output_scores': output_scores, 'model_scores': model_scores})\n",
    "    return results\n",
    "\n",
    "def calculate_average_scores(results):\n",
    "    \"\"\"\n",
    "    Calculate the average Nutri-Scores for outputs and model outputs.\n",
    "    \"\"\"\n",
    "    output_total_score = 0\n",
    "    model_total_score = 0\n",
    "    output_count = 0\n",
    "    model_count = 0\n",
    "\n",
    "    for result in results:\n",
    "        # Extract output scores\n",
    "        for meal, score_data in result['output_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                output_total_score += score_data['score']\n",
    "                output_count += 1\n",
    "\n",
    "        # Extract model scores\n",
    "        for meal, score_data in result['model_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                model_total_score += score_data['score']\n",
    "                model_count += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    output_avg = output_total_score / output_count if output_count > 0 else None\n",
    "    model_avg = model_total_score / model_count if model_count > 0 else None\n",
    "\n",
    "    return output_avg, model_avg\n",
    "\n",
    "\n",
    "# 'daily_diets' task Nutri-Score calculation\n",
    "filtered_df = df[df['task'] == 'daily_diets']\n",
    "results = calculate_scores_with_comparison(filtered_df, dfh)\n",
    "\n",
    "# Calculate overall averages\n",
    "output_avg, model_avg = calculate_average_scores(results)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Results for Each Row ===\")\n",
    "for result in results:\n",
    "    print(f\"Row Index: {result['row_index']}\")\n",
    "    print(f\"Output Scores: {result['output_scores']}\")\n",
    "    print(f\"Model Output Scores: {result['model_scores']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Overall Averages ===\")\n",
    "print(f\"Output Average Nutri-Score: {output_avg}\")\n",
    "print(f\"Model Output Average Nutri-Score: {model_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:31<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Average Nutri-Score: 0.4176467924666062\n",
      "Model Output Average Nutri-Score: 0.592826207957158\n",
      "{'row_index': 50, 'output_score': -0.5456349206349205, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 51, 'output_score': 0.32153783832762994, 'model_output_score': 1.1896494708994707}\n",
      "{'row_index': 52, 'output_score': 1.2495974235104654, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 53, 'output_score': -0.2137724271482232, 'model_output_score': -0.9043691424237499}\n",
      "{'row_index': 54, 'output_score': 0.09538432905267075, 'model_output_score': 2.305605786618444}\n",
      "{'row_index': 55, 'output_score': 0.09538432905267075, 'model_output_score': 8.479166666666668}\n",
      "{'row_index': 56, 'output_score': -4.595731219412345, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 57, 'output_score': 0.32153783832762994, 'model_output_score': 10.671052631578949}\n",
      "{'row_index': 58, 'output_score': -0.04938140184041795, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 59, 'output_score': -1.3771852609061912, 'model_output_score': 0.6299603174603177}\n",
      "{'row_index': 60, 'output_score': 0.6509013861955051, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 61, 'output_score': -0.3154319831908299, 'model_output_score': -2.254920499369688}\n",
      "{'row_index': 62, 'output_score': -1.8280321920168663, 'model_output_score': -0.07704481092786941}\n",
      "{'row_index': 63, 'output_score': 0.08814244409039951, 'model_output_score': 0.5184079415151972}\n",
      "{'row_index': 64, 'output_score': -1.8280321920168663, 'model_output_score': -0.5846493616506323}\n",
      "{'row_index': 65, 'output_score': -0.08572926025756211, 'model_output_score': -5.796296296296297}\n",
      "{'row_index': 66, 'output_score': 1.0389211034967496, 'model_output_score': 0.19219935712628833}\n",
      "{'row_index': 67, 'output_score': 3.005339996080737, 'model_output_score': -2.2812382456932223}\n",
      "{'row_index': 68, 'output_score': -0.04938140184041795, 'model_output_score': -2.292310346658173}\n",
      "{'row_index': 69, 'output_score': -0.08572926025756211, 'model_output_score': -2.2812382456932223}\n",
      "{'row_index': 70, 'output_score': 3.005339996080737, 'model_output_score': -0.06543201696262924}\n",
      "{'row_index': 71, 'output_score': 1.8030071355759434, 'model_output_score': -1.93626407732553}\n",
      "{'row_index': 72, 'output_score': 10.288530465949819, 'model_output_score': 8.479166666666668}\n",
      "{'row_index': 73, 'output_score': 0.5535285495992766, 'model_output_score': -2.2812382456932223}\n",
      "{'row_index': 74, 'output_score': -1.1019415041528768, 'model_output_score': 0.5184079415151972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# alternative diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return isinstance(data, dict)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # Í≥ºÏùº/Ï±ÑÏÜå ÎπÑÏú®ÏùÑ 100g Í∏∞Ï§ÄÏúºÎ°ú Î≥ÄÌôò\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # Ï†ÑÏ≤¥ Î¨¥Í≤å Í≥ÑÏÇ∞\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g Í∏∞Ï§ÄÏúºÎ°ú ÏÑ±Î∂Ñ Ï†ïÍ∑úÌôî\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_scores_with_comparison_no_meals(df, dfh):\n",
    "    results = []\n",
    "    output_scores_list = []\n",
    "    model_output_scores_list = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            output_text = row.get('output', '')\n",
    "            if output_text:\n",
    "                matched_row = find_most_similar_row(output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    output_scores_list.append(output_score)\n",
    "                else:\n",
    "                    output_score = None\n",
    "\n",
    "            model_output_text = row.get('model_output', '')\n",
    "            if model_output_text:\n",
    "                matched_row = find_most_similar_row(model_output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    model_output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    model_output_scores_list.append(model_output_score)\n",
    "                else:\n",
    "                    model_output_score = None\n",
    "\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': output_score,\n",
    "                'model_output_score': model_output_score\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': None,\n",
    "                'model_output_score': None\n",
    "            })\n",
    "\n",
    "    final_output_avg = sum(output_scores_list) / len(output_scores_list) if output_scores_list else None\n",
    "    final_model_output_avg = sum(model_output_scores_list) / len(model_output_scores_list) if model_output_scores_list else None\n",
    "\n",
    "    print(f\"Output Average Nutri-Score: {final_output_avg}\")\n",
    "    print(f\"Model Output Average Nutri-Score: {final_model_output_avg}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Execution\n",
    "filtered_df = df[df['task'] == 'alternative_diet']\n",
    "results = calculate_scores_with_comparison_no_meals(filtered_df, dfh)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meal_kernel",
   "language": "python",
   "name": "meal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
