{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b4191fde97421eabd26e6febb87484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'ifd_loss'],\n",
       "        num_rows: 15156\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "seed = load_dataset(\"passionMan/real_seed_IFD_rIFD14\")\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-31 18:45:28 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: chaoyi-wu/PMC_LLAMA_7B can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0294dabbf9d943559fce99c2b4fbd066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4d034bab94ec0a7f20c49f57dac74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a77669073b14a94990e49c1cc52aaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef486b4289b491e8e41a0dfe3ff44ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"medalpaca/medalpaca-7b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.12 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3d7c3f325349dfb6793a5816468d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/real_seed_IFD_rIFD14\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2365d82664d74d63aa79fc1e2f7af6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4d297ae93e4515881a228fdb62a1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeabaca3777549dda735da7f351ad32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 150, # 50\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/real_seed_IFD_rIFD14\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 15,156 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 474\n",
      " \"-____-\"     Number of trainable parameters = 9,175,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 44:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.440700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/900] Sample processed in 12.32s, ETA: 184.64 min\n",
      "[2/900] Sample processed in 6.99s, ETA: 144.51 min\n",
      "[3/900] Sample processed in 11.39s, ETA: 153.00 min\n",
      "[4/900] Sample processed in 9.26s, ETA: 149.19 min\n",
      "[5/900] Sample processed in 12.05s, ETA: 155.18 min\n",
      "[6/900] Sample processed in 11.58s, ETA: 157.94 min\n",
      "[7/900] Sample processed in 13.43s, ETA: 163.78 min\n",
      "[8/900] Sample processed in 11.71s, ETA: 164.91 min\n",
      "[9/900] Sample processed in 6.55s, ETA: 157.23 min\n",
      "[10/900] Sample processed in 12.93s, ETA: 160.53 min\n",
      "[11/900] Sample processed in 12.68s, ETA: 162.85 min\n",
      "[12/900] Sample processed in 8.10s, ETA: 159.09 min\n",
      "[13/900] Sample processed in 11.53s, ETA: 159.80 min\n",
      "[14/900] Sample processed in 6.98s, ETA: 155.58 min\n",
      "[15/900] Sample processed in 9.24s, ETA: 154.13 min\n",
      "[16/900] Sample processed in 12.82s, ETA: 156.14 min\n",
      "[17/900] Sample processed in 11.97s, ETA: 157.15 min\n",
      "[18/900] Sample processed in 11.74s, ETA: 157.84 min\n",
      "[19/900] Sample processed in 16.41s, ETA: 162.04 min\n",
      "[20/900] Sample processed in 8.87s, ETA: 160.27 min\n",
      "[21/900] Sample processed in 12.55s, ETA: 161.21 min\n",
      "[22/900] Sample processed in 11.85s, ETA: 161.60 min\n",
      "[23/900] Sample processed in 12.54s, ETA: 162.36 min\n",
      "[24/900] Sample processed in 12.25s, ETA: 162.87 min\n",
      "[25/900] Sample processed in 13.05s, ETA: 163.79 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### ‚úÖ Hugging FaceÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "dataset_name = \"passionMan/test_dataset10\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split Î°úÎìú\n",
    "\n",
    "### ‚úÖ JSONL Ï†ÄÏû• Ìï®Ïàò (ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû•Ïö©)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### ‚úÖ Î™®Îç∏ ÏùëÎãµ ÏÉùÏÑ± Ìï®Ïàò\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # ‚úÖ Î™®Îç∏Ïùò ÏµúÎåÄ ÏûÖÎ†• Í∏∏Ïù¥ Í∞ÄÏ†∏Ïò§Í∏∞ (Î≥¥ÌÜµ 4096 ÎòêÎäî 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # ‚úÖ ÏûÖÎ†• ÌÜ†ÌÅ∞ Í∏∏Ïù¥ ÌôïÏù∏\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # üî• ÏûÖÎ†•Ïù¥ ÎÑàÎ¨¥ Í∏∏Î©¥ ÏµúÎåÄ ÏûÖÎ†• Í∏∏Ïù¥Ïóê ÎßûÍ≤å ÏûêÎ¶Ñ\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # ‚úÖ ÏÉùÏÑ± ÏàòÌñâ (max_new_tokensÏùÑ Ï†ÅÏö©)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # ‚úÖ ÏÉùÏÑ± Í∏∏Ïù¥ Ï†ÅÏö©\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï (Í≤∞Í≥º Ï†ÄÏû•Ïö©)\n",
    "output_json_path = \"response/pmc_llama_7b.jsonl\"\n",
    "\n",
    "# ‚úÖ TaskÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Í∑∏Î£πÌôî (Í∞Å ÌÉúÏä§ÌÅ¨Î≥Ñ 0~29Î≤à ÏÉòÌîå ÏÑ†ÌÉù)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# ‚úÖ ÏÑ±Îä• ÌèâÍ∞ÄÌï† Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (Í∞Å ÌÉúÏä§ÌÅ¨Î≥Ñ 30Í∞úÎßå Ï∂îÏ∂ú)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:100])  # ÏµúÎåÄ 30Í∞ú ÏÑ†ÌÉù\n",
    "\n",
    "# ‚úÖ ÏÑ±Îä• ÌèâÍ∞Ä ÏãúÏûë\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # ‚úÖ ÏÉùÏÑ±Ìï† ÌÜ†ÌÅ∞ Í∏∏Ïù¥ ÏÑ§Ï†ï (ÏÉùÏÑ± ÌÜ†ÌÅ∞ Ïàò Ï°∞Ï†à)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"re\", \"re2\"}  # ÏÉùÏÑ± 32\n",
    "    medium_context_tasks = {\"ie\"} # ÏÉùÏÑ± 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # ÏÉùÏÑ± 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 32  # ‚úÖ ÏÉùÏÑ± Í∏∏Ïù¥ 128\n",
    "    elif task in medium_context_tasks:\n",
    "        max_new_tokens = 32\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 512  # ‚úÖ ÏÉùÏÑ± Í∏∏Ïù¥ 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # Í∏∞Î≥∏Í∞í\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-26 17:53:33 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial7/main_table/llama_8b/outputs/last_seed/checkpoint-458\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Prepare a healthy daily meal plan for diebeic patien, including portion sizes in grams.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Breakfast: 1 cup of steel-cut oats with 1/2 cup of nonfat milk and 1 tablespoon of ground flaxseed. Lunch: 3 ounces of grilled chicken breast with 1 cup of steamed broccoli and 1/2 cup of brown rice. Snack: 1 small apple with 1 tablespoon of almond butter. Dinner: 4 ounces of grilled salmon with 1 cup of steamed asparagus and 1/2 cup of quinoa.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1000) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Can you recommend a daily diet recipe? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Prepare a healthy daily meal plan for a female aged 35 with 1400 kcal, including portion sizes in grams.\",\n",
    "        \"Prepare a healthy daily meal plan for diebeic patien, including portion sizes in grams.\",\n",
    "        \"\",\n",
    "        \"\", \n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes is a metabolic disorder that affects how the body uses food for energy. It occurs when the body cannot produce enough insulin or when the body cannot properly use the insulin it produces. Insulin is a hormone that helps the body use glucose, a type of sugar, for energy. People with diabetes must carefully manage their blood sugar levels to avoid complications such as heart disease, stroke, and nerve damage.\n",
      "\n",
      "A healthy diet for a diabetic patient should include plenty of vegetables, fruits, whole grains, lean proteins, and healthy fats. It should also limit processed foods, sugary drinks, and refined carbohydrates. A typical daily diet for a diabetic patient might look like this:\n",
      "\n",
      "Breakfast: oatmeal with fresh berries, a glass of milk, and a cup of green tea\n",
      "Lunch: grilled chicken breast with steamed vegetables, brown rice, and a glass of water\n",
      "Dinner: baked fish with roasted vegetables, quinoa, and a small glass of red wine\n",
      "Snacks: a handful of nuts, a piece of fruit, or a small serving of yogurt\n",
      "It's important to consult with a registered dietitian or healthcare provider to develop a personalized diet plan for a diabetic patient. They can provide guidance on portion sizes, macronutrient ratios, and specific foods to include or avoid based on individual needs and health goals.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(400) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        \"\",\n",
    "        \"\", \n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
