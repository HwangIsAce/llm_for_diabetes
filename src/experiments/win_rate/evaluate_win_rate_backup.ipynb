{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd681e6491a4e568e6ea0baaf0302b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7897aef07a44b19207567d239ab60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae04feba872348948ac00c2a31aecf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00004-1949301f12e51c42.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4843005bdfa14b23aca2fc13fdb84323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00001-of-00004-a4cd9ef5eaa4f14a.parquet:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622580c545614b01b53865e123485ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00002-of-00004-9d12bf5267a07ac2.parquet:   0%|          | 0.00/209M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3d6e8f728c4959866ce9ff9cbd219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00003-of-00004-e49d8d0bebbbf654.parquet:   0%|          | 0.00/87.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fe714a9d344a4c84cc348b1ea9c107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1990915 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response'],\n",
       "        num_rows: 1990915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c7ba58fc44b4a9b049a384194cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1990915 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 1383\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sni = load_dataset(\"andersonbcdefg/supernatural-instructions-2m\")\n",
    "sni\n",
    "\n",
    "diabetes_keyword = ['diabetes']\n",
    "\n",
    "def extract_indices(dataset, keywords, columns=['prompt', 'response']):\n",
    "    indices = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        for col in columns:\n",
    "            if any(keyword.lower() in str(row[col]).lower() for keyword in keywords):\n",
    "                indices.append(i)\n",
    "                break\n",
    "    return indices\n",
    "\n",
    "train_indices = extract_indices(sni['train'], diabetes_keyword)\n",
    "\n",
    "filtered_train = sni[\"train\"].filter(lambda row: any(keyword.lower() in str(row[\"prompt\"]).lower() for keyword in diabetes_keyword))\n",
    "\n",
    "print(filtered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(filtered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"In this task, you're given passages that contain mentions of names of people, places, or things. Some of these mentions refer to the same person, place, or thing. Your job is to write questions that evaluate one's understanding of such references. Good questions are expected to link pronouns (she, her, him, his, their, etc.) or other mentions to people, places, or things to which they may refer. Do not ask questions that can be answered correctly without understanding the paragraph or having multiple answers. Avoid questions that do not link phrases referring to the same entity. For each of your questions, the answer should be one or more phrases in the paragraph, and it should be unambiguous.\\nPassage: Oklahoma was the 21st-largest recipient of medical funding from the federal government in 2005, with health-related federal expenditures in the state totaling $75,801,364; immunizations, bioterrorism preparedness, and health education were the top three most funded medical items. Instances of major diseases are near the national average in Oklahoma, and the state ranks at or slightly above the rest of the country in percentage of people with asthma, diabetes, cancer, and hypertension.In 2000, Oklahoma ranked 45th in physicians per capita and slightly below the national average in nurses per capita, but was slightly over the national average in hospital beds per 100,000 people and above the national average in net growth of health services over a 12-year period. One of the worst states for percentage of insured people, nearly 25 percent of Oklahomans between the age of 18 and 64 did not have health insurance in 2005, the fifth-highest rate in the nation.Oklahomans are in the upper half of Americans in terms of obesity prevalence, and the state is the 5th most obese in the nation, with 30.3 percent of its population at or near obesity. Oklahoma ranked last among the 50 states in a 2007 study by the Commonwealth Fund on health care performance.The OU Medical Center, Oklahoma's largest collection of hospitals, is the only hospital in the state designated a Level I trauma center by the American College of Surgeons. OU Medical Center is on the grounds of the Oklahoma Health Center in Oklahoma City, the state's largest concentration of medical research facilities.The Cancer Treatment Centers of America at Southwestern Regional Medical Center in Tulsa is one of four such regional facilities nationwide, offering cancer treatment to the entire southwestern United States, and is one of the largest cancer treatment hospitals in the country. The largest osteopathic teaching facility in the nation, Oklahoma State University Medical Center at Tulsa, also rates as one of the largest facilities in the field of neuroscience.\\nOn June 26, 2018, Oklahoma made marijuana legal for medical purposes. This was a milestone for a state in the Bible Belt.\",\n",
       " 'response': 'In what city is the only Level 1 trauma center in Oklahoma located?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Task Classification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1383/1383 [35:58<00:00,  1.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task classification complete. Dataset updated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API Key is missing! Please set OPENAI_API in .env file.\")\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def classify_task(prompt, response):\n",
    "    system_prompt = \"\"\"You are an AI assistant that classifies NLP tasks based on the given prompt and response. \n",
    "    Given a prompt and response, return only the most relevant NLP task category from the following list without any extra text:\n",
    "    \n",
    "    Named Entity Recognition (NER)\n",
    "    Coreference Resolution\n",
    "    Question Answering (QA)\n",
    "    Summarization\n",
    "    Text Classification\n",
    "    Text Generation\n",
    "    Relation Extraction (RE)\n",
    "    Natural Language Inference (NLI)\n",
    "    Information Extraction (IE)\n",
    "    Commonsense Reasoning\n",
    "    Other\n",
    "\n",
    "    Ensure that identical or similar tasks receive exactly the same category name from the list.\n",
    "    If unsure, return 'Other' without explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_input = f\"Prompt: {prompt}\\nResponse: {response}\\nWhat is the most appropriate NLP task category?\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "df = pd.DataFrame(filtered_train)\n",
    "df[\"task_category\"] = \"Pending\"  \n",
    "\n",
    "with open(\"task_classification_log_file.txt\", \"a\") as log_file:\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Task Classification\"):\n",
    "        try:\n",
    "            result = classify_task(row[\"prompt\"], row[\"response\"])\n",
    "            df.at[index, \"task_category\"] = result\n",
    "\n",
    "            log_file.write(f\"Index {index}: {result}\\n\")\n",
    "            log_file.flush()  \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {index}: {e}\")\n",
    "            df.at[index, \"task_category\"] = \"Error\"\n",
    "\n",
    "filtered_train = Dataset.from_pandas(df)\n",
    "print(\"Task classification complete. Dataset updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_train\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_train' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf5459c5f4d46ed99741ddc328dd441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a08953ed092495ea217b913f447444a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset successfully uploaded to: https://huggingface.co/datasets/passionMan/diabetes_sni\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN_WRITE\") \n",
    "if not hf_token:\n",
    "    raise ValueError(\"Hugging Face API Token is missing! Please set HF_TOKEN in .env file.\")\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "dataset_dict = DatasetDict({\"test\": filtered_train})\n",
    "\n",
    "repo_name = \"passionMan/diabetes_sni\"  \n",
    "\n",
    "dataset_dict.push_to_hub(repo_name, token=hf_token)\n",
    "\n",
    "print(f\"‚úÖ Dataset successfully uploaded to: https://huggingface.co/datasets/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97f3eeb7d004bea97c5f63b532606bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1a55bb64c24f79a1d353efaf3c3f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/575k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84132bd80c64073a0436fdce42b4d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "sni = load_dataset(\"passionMan/sni\")\n",
    "sni\n",
    "\n",
    "df = pd.DataFrame(sni['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>task_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this task, you're given passages that conta...</td>\n",
       "      <td>In what city is the only Level 1 trauma center...</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this task, you're given passages that conta...</td>\n",
       "      <td>What is the full name of the person who states...</td>\n",
       "      <td>Coreference Resolution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  In this task, you're given passages that conta...   \n",
       "1  In this task, you're given passages that conta...   \n",
       "\n",
       "                                            response            task_category  \n",
       "0  In what city is the only Level 1 trauma center...  Question Answering (QA)  \n",
       "1  What is the full name of the person who states...   Coreference Resolution  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_category\n",
       "Question Answering (QA)             517\n",
       "Text Classification                 268\n",
       "Text Generation                     161\n",
       "Named Entity Recognition (NER)      125\n",
       "Summarization                        99\n",
       "Information Extraction (IE)          76\n",
       "Relation Extraction (RE)             70\n",
       "Commonsense Reasoning                22\n",
       "Natural Language Inference (NLI)     19\n",
       "Other                                11\n",
       "Text Summarization                    4\n",
       "Coreference Resolution                4\n",
       "Question Generation                   3\n",
       "Named Entity Recognition              2\n",
       "Text Simplification                   1\n",
       "Sentiment Analysis                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['task_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  You are given a background paragraph that desc...   \n",
      "1  You are given a background paragraph that desc...   \n",
      "2  You are given a background paragraph that desc...   \n",
      "3  You are given a science question (easy-level) ...   \n",
      "4  You are given a background paragraph that desc...   \n",
      "\n",
      "                                            response            task_category  \n",
      "0                                             Brown.  Question Answering (QA)  \n",
      "1                                             Tammy.  Question Answering (QA)  \n",
      "2  Which sibling did not spend a lot of time on t...  Question Answering (QA)  \n",
      "3                                                  D  Question Answering (QA)  \n",
      "4                                           group B.  Question Answering (QA)  \n",
      "‚úÖ Ï¥ù ÏÉòÌîå Í∞úÏàò: 179\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_samples_main = 20  \n",
    "num_samples_nli = 19   # NLIÏóêÏÑú ÏÉòÌîåÎßÅÌï† Í∞úÏàò\n",
    "\n",
    "main_tasks = [\n",
    "    \"Question Answering (QA)\", \"Text Classification\", \"Text Generation\",\n",
    "    \"Named Entity Recognition (NER)\", \"Summarization\", \"Information Extraction (IE)\",\n",
    "    \"Relation Extraction (RE)\", \"Commonsense Reasoning\"\n",
    "]\n",
    "\n",
    "sampled_dfs = []\n",
    "\n",
    "for task in main_tasks:\n",
    "    sampled_dfs.append(df[df[\"task_category\"] == task].sample(n=num_samples_main, random_state=30, replace=False))\n",
    "\n",
    "sampled_dfs.append(df[df[\"task_category\"] == \"Natural Language Inference (NLI)\"].sample(n=num_samples_nli, random_state=42, replace=False))\n",
    "\n",
    "df_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "print(df_sampled.head())\n",
    "print(f\"‚úÖ Ï¥ù ÏÉòÌîå Í∞úÏàò: {len(df_sampled)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>task_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>Brown.</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>Tammy.</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>Which sibling did not spend a lot of time on t...</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are given a science question (easy-level) ...</td>\n",
       "      <td>D</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>group B.</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>In this task, you are given an ambiguous quest...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Natural Language Inference (NLI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>In this task, you are given an ambiguous quest...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Natural Language Inference (NLI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>In this task, you are given an ambiguous quest...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Natural Language Inference (NLI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>The input is taken from a negotiation between ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Natural Language Inference (NLI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>In this task, you are given an ambiguous quest...</td>\n",
       "      <td>No</td>\n",
       "      <td>Natural Language Inference (NLI)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    You are given a background paragraph that desc...   \n",
       "1    You are given a background paragraph that desc...   \n",
       "2    You are given a background paragraph that desc...   \n",
       "3    You are given a science question (easy-level) ...   \n",
       "4    You are given a background paragraph that desc...   \n",
       "..                                                 ...   \n",
       "174  In this task, you are given an ambiguous quest...   \n",
       "175  In this task, you are given an ambiguous quest...   \n",
       "176  In this task, you are given an ambiguous quest...   \n",
       "177  The input is taken from a negotiation between ...   \n",
       "178  In this task, you are given an ambiguous quest...   \n",
       "\n",
       "                                              response  \\\n",
       "0                                               Brown.   \n",
       "1                                               Tammy.   \n",
       "2    Which sibling did not spend a lot of time on t...   \n",
       "3                                                    D   \n",
       "4                                             group B.   \n",
       "..                                                 ...   \n",
       "174                                                Yes   \n",
       "175                                                Yes   \n",
       "176                                                Yes   \n",
       "177                                                 No   \n",
       "178                                                 No   \n",
       "\n",
       "                        task_category  \n",
       "0             Question Answering (QA)  \n",
       "1             Question Answering (QA)  \n",
       "2             Question Answering (QA)  \n",
       "3             Question Answering (QA)  \n",
       "4             Question Answering (QA)  \n",
       "..                                ...  \n",
       "174  Natural Language Inference (NLI)  \n",
       "175  Natural Language Inference (NLI)  \n",
       "176  Natural Language Inference (NLI)  \n",
       "177  Natural Language Inference (NLI)  \n",
       "178  Natural Language Inference (NLI)  \n",
       "\n",
       "[179 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-01 13:52:20 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f0f8fd657145c1af0940744b87d8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "Generating Responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [12:55<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  You are given a background paragraph that desc...   \n",
      "1  You are given a background paragraph that desc...   \n",
      "2  You are given a background paragraph that desc...   \n",
      "3  You are given a science question (easy-level) ...   \n",
      "4  You are given a background paragraph that desc...   \n",
      "\n",
      "                                            response            task_category  \\\n",
      "0                                             Brown.  Question Answering (QA)   \n",
      "1                                             Tammy.  Question Answering (QA)   \n",
      "2  Which sibling did not spend a lot of time on t...  Question Answering (QA)   \n",
      "3                                                  D  Question Answering (QA)   \n",
      "4                                           group B.  Question Answering (QA)   \n",
      "\n",
      "                             llama3_output  \\\n",
      "0               Group Brown<|end_of_text|>   \n",
      "1                       Pat<|end_of_text|>   \n",
      "2  Why is Mary nearsighted?<|end_of_text|>   \n",
      "3                   C: milk<|end_of_text|>   \n",
      "4                   Group A<|end_of_text|>   \n",
      "\n",
      "                                        gpt4o_output  \n",
      "0                                        Group Brown  \n",
      "1                                              Tammy  \n",
      "2  Based on the relationship between heredity and...  \n",
      "3                                                  D  \n",
      "4  Group B had more people that do a lot of visua...  \n",
      "‚úÖ LLaMA3 & GPT-4o outputs successfully added and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API Key is missing! Please set OPENAI_API_KEY in .env file.\")\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial8/train/llama3_8B/outputs/real_seed_IFD_rIFD14/checkpoint-474\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"),\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<eot_id>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def get_gpt4o_response(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in GPT-4o response: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "df_sampled[\"llama3_output\"] = \"Pending\"\n",
    "df_sampled[\"gpt4o_output\"] = \"Pending\"\n",
    "\n",
    "for index, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Generating Responses\"):\n",
    "    prompt = row[\"prompt\"]\n",
    "\n",
    "    df_sampled.at[index, \"llama3_output\"] = generate_response(instruction_text=\"\", input_text=prompt)\n",
    "\n",
    "    df_sampled.at[index, \"gpt4o_output\"] = get_gpt4o_response(prompt)\n",
    "\n",
    "    time.sleep(1) \n",
    "\n",
    "print(df_sampled.head())\n",
    "\n",
    "df_sampled.to_csv(\"df_sampled_with_outputs_bio2.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ LLaMA3 & GPT-4o outputs successfully added and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>task_category</th>\n",
       "      <th>llama3_output</th>\n",
       "      <th>gpt4o_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>Brown.</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "      <td>Group Brown&lt;|end_of_text|&gt;</td>\n",
       "      <td>Group Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are given a background paragraph that desc...</td>\n",
       "      <td>Tammy.</td>\n",
       "      <td>Question Answering (QA)</td>\n",
       "      <td>Pat&lt;|end_of_text|&gt;</td>\n",
       "      <td>Tammy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response  \\\n",
       "0  You are given a background paragraph that desc...   Brown.   \n",
       "1  You are given a background paragraph that desc...   Tammy.   \n",
       "\n",
       "             task_category               llama3_output gpt4o_output  \n",
       "0  Question Answering (QA)  Group Brown<|end_of_text|>  Group Brown  \n",
       "1  Question Answering (QA)          Pat<|end_of_text|>        Tammy  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [05:02<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API Key is missing! Please set OPENAI_API_KEY in .env file.\")\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def evaluate_response(prompt, llama3_output, gpt4o_output):\n",
    "    system_prompt = \"\"\"You are an AI evaluator that compares two model-generated responses to a given prompt.\n",
    "You will decide which response is better or if they are equally good.\n",
    "\n",
    "Format your response as:\n",
    "- \"Llama3\" if the Llama3 response is better\n",
    "- \"GPT-4o\" if the GPT-4o response is better\n",
    "- \"Tie\" if both are equally good\n",
    "\n",
    "Here is the comparison:\n",
    "\n",
    "Prompt: {prompt}\n",
    "Llama3 Response: {llama3_output}\n",
    "GPT-4o Response: {gpt4o_output}\n",
    "\n",
    "Which response is better?\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt.format(\n",
    "                prompt=prompt,\n",
    "                llama3_output=llama3_output,\n",
    "                gpt4o_output=gpt4o_output\n",
    "            )}]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Evaluation failed: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "df_sampled[\"evaluation\"] = \"Pending\"\n",
    "\n",
    "for index, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc=\"Evaluating responses\"):\n",
    "    prompt = row[\"prompt\"]\n",
    "    llama3_output = row[\"llama3_output\"]\n",
    "    gpt4o_output = row[\"gpt4o_output\"]\n",
    "\n",
    "    df_sampled.at[index, \"evaluation\"] = evaluate_response(prompt, llama3_output, gpt4o_output)\n",
    "    time.sleep(1)  # API Rate Limit Î∞©ÏßÄ\n",
    "\n",
    "task_win_counts = df_sampled.groupby(\"task_category\")[\"evaluation\"].value_counts().unstack().fillna(0)\n",
    "\n",
    "task_win_counts[\"Total Matches\"] = task_win_counts[\"Llama3\"] + task_win_counts[\"GPT-4o\"]\n",
    "task_win_counts[\"Llama3 Win-Rate\"] = task_win_counts[\"Llama3\"] / task_win_counts[\"Total Matches\"]\n",
    "\n",
    "df_sampled.to_csv(\"df_sampled_with_outputs_evaluations_bio2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>evaluation</th>\n",
       "      <th>\"GPT-4o\"</th>\n",
       "      <th>\"GPT-4o\" if the GPT-4o response is better</th>\n",
       "      <th>\"GPT-4o\" is better</th>\n",
       "      <th>\"GPT-4o\" is better.</th>\n",
       "      <th>\"Llama3\"</th>\n",
       "      <th>\"Tie\"</th>\n",
       "      <th>\"Tie\" if both are equally good</th>\n",
       "      <th>- \"GPT-4o\"</th>\n",
       "      <th>- \"GPT-4o\" if the GPT-4o response is better</th>\n",
       "      <th>GPT-4o</th>\n",
       "      <th>Llama3</th>\n",
       "      <th>Tie</th>\n",
       "      <th>Total Matches</th>\n",
       "      <th>Llama3 Win-Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Commonsense Reasoning</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Information Extraction (IE)</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Named Entity Recognition (NER)</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Natural Language Inference (NLI)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Question Answering (QA)</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relation Extraction (RE)</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summarization</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text Classification</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text Generation</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "evaluation                        \"GPT-4o\"  \\\n",
       "task_category                                \n",
       "Commonsense Reasoning                  8.0   \n",
       "Information Extraction (IE)            3.0   \n",
       "Named Entity Recognition (NER)         8.0   \n",
       "Natural Language Inference (NLI)       5.0   \n",
       "Question Answering (QA)                6.0   \n",
       "Relation Extraction (RE)               4.0   \n",
       "Summarization                          4.0   \n",
       "Text Classification                    7.0   \n",
       "Text Generation                        6.0   \n",
       "\n",
       "evaluation                        \"GPT-4o\" if the GPT-4o response is better  \\\n",
       "task_category                                                                 \n",
       "Commonsense Reasoning                                                   1.0   \n",
       "Information Extraction (IE)                                             0.0   \n",
       "Named Entity Recognition (NER)                                          0.0   \n",
       "Natural Language Inference (NLI)                                        0.0   \n",
       "Question Answering (QA)                                                 0.0   \n",
       "Relation Extraction (RE)                                                0.0   \n",
       "Summarization                                                           0.0   \n",
       "Text Classification                                                     1.0   \n",
       "Text Generation                                                         0.0   \n",
       "\n",
       "evaluation                        \"GPT-4o\" is better  \"GPT-4o\" is better.  \\\n",
       "task_category                                                               \n",
       "Commonsense Reasoning                            2.0                  0.0   \n",
       "Information Extraction (IE)                      0.0                  0.0   \n",
       "Named Entity Recognition (NER)                   0.0                  0.0   \n",
       "Natural Language Inference (NLI)                 0.0                  0.0   \n",
       "Question Answering (QA)                          1.0                  2.0   \n",
       "Relation Extraction (RE)                         0.0                  0.0   \n",
       "Summarization                                    0.0                  0.0   \n",
       "Text Classification                              0.0                  0.0   \n",
       "Text Generation                                  0.0                  0.0   \n",
       "\n",
       "evaluation                        \"Llama3\"  \"Tie\"  \\\n",
       "task_category                                       \n",
       "Commonsense Reasoning                  0.0    2.0   \n",
       "Information Extraction (IE)            1.0    0.0   \n",
       "Named Entity Recognition (NER)         0.0    4.0   \n",
       "Natural Language Inference (NLI)       2.0    7.0   \n",
       "Question Answering (QA)                1.0    3.0   \n",
       "Relation Extraction (RE)               0.0    1.0   \n",
       "Summarization                          0.0    1.0   \n",
       "Text Classification                    0.0    2.0   \n",
       "Text Generation                        0.0    0.0   \n",
       "\n",
       "evaluation                        \"Tie\" if both are equally good  - \"GPT-4o\"  \\\n",
       "task_category                                                                  \n",
       "Commonsense Reasoning                                        0.0         1.0   \n",
       "Information Extraction (IE)                                  0.0         0.0   \n",
       "Named Entity Recognition (NER)                               0.0         0.0   \n",
       "Natural Language Inference (NLI)                             0.0         0.0   \n",
       "Question Answering (QA)                                      1.0         0.0   \n",
       "Relation Extraction (RE)                                     0.0         0.0   \n",
       "Summarization                                                0.0         0.0   \n",
       "Text Classification                                          1.0         0.0   \n",
       "Text Generation                                              0.0         0.0   \n",
       "\n",
       "evaluation                        - \"GPT-4o\" if the GPT-4o response is better  \\\n",
       "task_category                                                                   \n",
       "Commonsense Reasoning                                                     0.0   \n",
       "Information Extraction (IE)                                               0.0   \n",
       "Named Entity Recognition (NER)                                            1.0   \n",
       "Natural Language Inference (NLI)                                          0.0   \n",
       "Question Answering (QA)                                                   0.0   \n",
       "Relation Extraction (RE)                                                  0.0   \n",
       "Summarization                                                             0.0   \n",
       "Text Classification                                                       0.0   \n",
       "Text Generation                                                           0.0   \n",
       "\n",
       "evaluation                        GPT-4o  Llama3  Tie  Total Matches  \\\n",
       "task_category                                                          \n",
       "Commonsense Reasoning                3.0     1.0  2.0            4.0   \n",
       "Information Extraction (IE)         16.0     0.0  0.0           16.0   \n",
       "Named Entity Recognition (NER)       2.0     1.0  4.0            3.0   \n",
       "Natural Language Inference (NLI)     0.0     0.0  5.0            0.0   \n",
       "Question Answering (QA)              2.0     1.0  3.0            3.0   \n",
       "Relation Extraction (RE)             9.0     1.0  5.0           10.0   \n",
       "Summarization                       15.0     0.0  0.0           15.0   \n",
       "Text Classification                  4.0     0.0  5.0            4.0   \n",
       "Text Generation                     11.0     1.0  2.0           12.0   \n",
       "\n",
       "evaluation                        Llama3 Win-Rate  \n",
       "task_category                                      \n",
       "Commonsense Reasoning                    0.250000  \n",
       "Information Extraction (IE)              0.000000  \n",
       "Named Entity Recognition (NER)           0.333333  \n",
       "Natural Language Inference (NLI)              NaN  \n",
       "Question Answering (QA)                  0.333333  \n",
       "Relation Extraction (RE)                 0.100000  \n",
       "Summarization                            0.000000  \n",
       "Text Classification                      0.000000  \n",
       "Text Generation                          0.083333  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_win_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
