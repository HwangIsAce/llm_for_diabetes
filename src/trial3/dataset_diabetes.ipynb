{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f62cf4c67a04dffa31df8c107a35aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq = load_dataset(\"allenai/sciq\")\n",
    "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "pubmedqa = load_dataset(\"bigbio/pubmed_qa\", trust_remote_code=True)\n",
    "bionli = load_dataset(\"clinicalnlplab/BioNLI_test\")\n",
    "scitail = load_dataset(\"allenai/scitail\", \"predictor_format\")\n",
    "mednli = load_dataset(\"presencesw/mednli\")\n",
    "chemdner = load_dataset(\"kjappelbaum/chemnlp-chemdner\")\n",
    "ddi = load_dataset(\"YufeiHFUT/DDI2013\")\n",
    "mts = load_dataset(\"har1/MTS_Dialogue-Clinical_Note\")\n",
    "chat_doctor = load_dataset(\"LinhDuong/chatdoctor-5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 11679\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 10178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1273\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 182822\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 6150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 4183\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 11269\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 5544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 12806\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 6308\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "scitail\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 23587\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 2126\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 1304\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mednli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 11232\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1422\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1395\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entities', 'text', 'split'],\n",
      "        num_rows: 19440\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ddi\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 18779\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 5761\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 7244\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mts\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'section_header', 'section_text', 'dialogue'],\n",
      "        num_rows: 1301\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chat_doctor\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'input'],\n",
      "        num_rows: 5452\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"sciq\")\n",
    "print(sciq)\n",
    "print(\"------\" * 100)\n",
    "print(\"medqa\")\n",
    "print(medqa)\n",
    "print(\"------\" * 100)\n",
    "print(\"medmcqa\")\n",
    "print(medmcqa)\n",
    "print(\"------\" * 100)\n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa)\n",
    "print(\"------\" * 100)\n",
    "print(\"bionli\")\n",
    "print(bionli)\n",
    "print(\"------\" * 100)\n",
    "print(\"scitail\")\n",
    "print(scitail)\n",
    "print(\"------\" * 100)\n",
    "print(\"mednli\")\n",
    "print(mednli)\n",
    "print(\"------\" * 100)\n",
    "print(\"chemdner\")\n",
    "print(chemdner)\n",
    "print(\"------\" * 100)\n",
    "print(\"ddi\")\n",
    "print(ddi)\n",
    "print(\"------\" * 100)\n",
    "print(\"mts\")\n",
    "print(mts)\n",
    "print(\"------\" * 100)\n",
    "print(\"chat_doctor\")\n",
    "print(chat_doctor)\n",
    "print(\"------\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"diabetic\", \"diabetes\"]\n",
    "\n",
    "sciq_filtered = sciq.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"question\", \"support\"] for kw in keywords))\n",
    "medqa_filtered = medqa.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"question\"] for kw in keywords))\n",
    "medmcqa_filtered = medmcqa.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"question\"] for kw in keywords))\n",
    "pubmedqa_filtered = pubmedqa.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"QUESTION\", \"CONTEXTS\"] for kw in keywords))\n",
    "bionli_filtered = bionli.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"query\"] for kw in keywords))\n",
    "scitail_filtered = scitail.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"sentence1\", \"sentence2\", \"question\"] for kw in keywords))\n",
    "mednli_filtered = mednli.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"sentence1\", \"sentence2\"] for kw in keywords))\n",
    "chemdner_filtered = chemdner.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"text\", \"entities\"] for kw in keywords))\n",
    "ddi_filtered = ddi.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"conversations\", \"text\"] for kw in keywords))\n",
    "mts_filtered = mts.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"dialogue\", \"section_text\"] for kw in keywords))\n",
    "chat_doctor_filtered = chat_doctor.filter(lambda ex: any(kw in str(ex[col]).lower() for col in [\"instruction\", \"input\", \"output\"] for kw in keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 58\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 4\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1050\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 122\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 1373\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 34\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 12505\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 686\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 221\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 537\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 224\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "scitail\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 108\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 22\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mednli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 315\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 49\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 54\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entities', 'text', 'split'],\n",
      "        num_rows: 650\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ddi\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 289\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 61\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 45\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mts\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'section_header', 'section_text', 'dialogue'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chat_doctor\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'input'],\n",
      "        num_rows: 77\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"sciq\")\n",
    "print(sciq_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"medqa\")\n",
    "print(medqa_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"medmcqa\")\n",
    "print(medmcqa_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"bionli\")\n",
    "print(bionli_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"scitail\")\n",
    "print(scitail_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"mednli\")\n",
    "print(mednli_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"chemdner\")\n",
    "print(chemdner_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"ddi\")\n",
    "print(ddi_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"mts\")\n",
    "print(mts_filtered)\n",
    "print(\"------\" * 100)\n",
    "print(\"chat_doctor\")\n",
    "print(chat_doctor_filtered)\n",
    "print(\"------\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    train_size = int(len(dataset[\"train\"]) * 0.65)  # 70% 선택\n",
    "    test_size = len(dataset[\"train\"]) - train_size  # 나머지를 test로 사용\n",
    "\n",
    "    split_dataset = dataset[\"train\"].train_test_split(test_size=test_size, seed=42)  # 재현 가능하도록 seed 설정\n",
    "\n",
    "    new_dataset = DatasetDict({\n",
    "        \"train\": split_dataset[\"train\"],\n",
    "        \"test\": split_dataset[\"test\"]\n",
    "    })\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciq_filtered2 = train_test_split(sciq_filtered)\n",
    "medqa_filtered2 = train_test_split(medqa_filtered)\n",
    "medmcqa_filtered2 = train_test_split(medmcqa_filtered)\n",
    "pubmedqa_filtered2 = train_test_split(pubmedqa_filtered)\n",
    "bionli_filtered2 = train_test_split(bionli_filtered)\n",
    "scitail_filtered2 = train_test_split(scitail_filtered)\n",
    "mednli_filtered2 = train_test_split(mednli_filtered)\n",
    "chemdner_filtered2 = train_test_split(chemdner_filtered)\n",
    "ddi_filtered2 = train_test_split(ddi_filtered)\n",
    "mts_filtered2 = train_test_split(mts_filtered)\n",
    "chat_doctor_filtered2 = train_test_split(chat_doctor_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 37\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
      "        num_rows: 21\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 368\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 481\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 8128\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 4377\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 143\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 78\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "scitail\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 70\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answer', 'sentence2_structure', 'sentence1', 'sentence2', 'gold_label', 'question'],\n",
      "        num_rows: 38\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mednli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 204\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'sentence1', 'sentence2'],\n",
      "        num_rows: 111\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entities', 'text', 'split'],\n",
      "        num_rows: 422\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['entities', 'text', 'split'],\n",
      "        num_rows: 228\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ddi\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 187\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mts\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'section_header', 'section_text', 'dialogue'],\n",
      "        num_rows: 59\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'section_header', 'section_text', 'dialogue'],\n",
      "        num_rows: 32\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chat_doctor\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output', 'input'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'output', 'input'],\n",
      "        num_rows: 27\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"sciq\")\n",
    "print(sciq_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"medqa\")\n",
    "print(medqa_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"medmcqa\")\n",
    "print(medmcqa_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"bionli\")\n",
    "print(bionli_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"scitail\")\n",
    "print(scitail_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"mednli\")\n",
    "print(mednli_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"chemdner\")\n",
    "print(chemdner_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"ddi\")\n",
    "print(ddi_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"mts\")\n",
    "print(mts_filtered2)\n",
    "print(\"------\" * 100)\n",
    "print(\"chat_doctor\")\n",
    "print(chat_doctor_filtered2)\n",
    "print(\"------\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    # options의 {} 제거 및 깔끔한 형식으로 변환\n",
    "    options_str = \", \".join([f\"{key}: {value}\" for key, value in example[\"options\"].items()])\n",
    "\n",
    "    return {\n",
    "        \"source\": f\"Question: {example['question']} Options: {options_str}\",\n",
    "        \"target\": f\"{example['answer_idx']}: {example['answer']}\",\n",
    "        \"task\": \"task0_medqa\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "medqa_filtered3 = medqa_filtered2.map(transform_data, remove_columns=medqa_filtered2[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    options_dict = {0: example[\"opa\"], 1: example[\"opb\"], 2: example[\"opc\"], 3: example[\"opd\"]}\n",
    "    options_str = \", \".join([f\"{chr(65+key)}) {value}\" for key, value in options_dict.items()])\n",
    "\n",
    "    correct_index = int(example[\"cop\"])\n",
    "    correct_answer = f\"{chr(65+correct_index)}) {options_dict[correct_index]}\"\n",
    "\n",
    "    return {\n",
    "        \"source\": f\"Question: {example['question']} Options: {options_str}\",\n",
    "        \"target\": correct_answer,  # A) 정답 텍스트 형식\n",
    "        \"task\": \"task0_medcqa\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "medmcqa_filtered3 = medmcqa_filtered2.map(transform_data, remove_columns=medmcqa_filtered2[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    # CONTEXTS 리스트를 하나의 문자열로 변환\n",
    "    context_str = \" \".join(example[\"CONTEXTS\"]) if isinstance(example[\"CONTEXTS\"], list) else example[\"CONTEXTS\"]\n",
    "\n",
    "    return {\n",
    "        \"source\": f\"Question: {example['QUESTION']} Context: {context_str}\",\n",
    "        \"target\": example[\"final_decision\"],\n",
    "        \"task\": \"task0_pubmedqa\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "pubmedqa_filtered3 = pubmedqa_filtered2.map(transform_data, remove_columns=pubmedqa_filtered2[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    return {\n",
    "        \"source\": example['query'].split('\\nOUTPUT:')[0].replace(\"entailment, contradiction, or neutral. Return only the label.\", \"entailment or contradiction. Return only the label.\").split(\"INPUT: \")[1],\n",
    "        \"target\": example['answer'],\n",
    "        \"task\": \"task0_bionli\"\n",
    "    }\n",
    "\n",
    "bionli_filtered3 = bionli_filtered2.map(transform_data, remove_columns=bionli_filtered2[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a 3-hour primed-continuous infusion of [3-3H]glucose and [2-13C]glycerol, we measured glucose production, gluconeogenesis from glycerol, and total gluconeogenesis (using mass isotopomer distribution analysis [MIDA] of glucose) in postabsorptive and starved normal and streptozotocin-diabetic rats. In normal rats, 48 hours of starvation increased (P < .01) the percent contribution of both gluconeogenesis from glycerol (from 14.4% +/- 1.8% to 25.5% +/- 4.0%) and total gluconeogenesis (from 52.2% +/- 3.9% to 89.8% +/- 1.3%) to glucose production, but the absolute gluconeogenic fluxes were not modified, since glucose production decreased. Diabetic rats showed increased glucose production in the postabsorptive state; this decreased with starvation and was comparable to the of controls after 48 hours of starvation. Gluconeogenesis was increased in postabsorptive diabetic rats (69.0% +/- 1.3%, P < .05 v controls). Surprisingly, this contribution of gluconeogenesis to glucose production was not found to be increased in 24-hour starved diabetic rats (64.4% +/- 2.4%). These rats had significant liver glycogen stores, but gluconeogenesis was also low (42.8% +/- 2.1%) in 48-hour starved diabetic rats deprived of glycogen stores. Moreover, in 24-hour starved diabetic rats infused with [3-13C]lactate, gluconeogenesis was 100% when determined by comparing circulating glucose and liver pyruvate enrichment, but only 47% +/- 3% when calculated from the MIDA of glucose. Therefore, MIDA is not a valid method to measure gluconeogenesis in starved diabetic rats. This was not explained by differences in the labeling of liver and kidney triose phosphates: functional nephrectomy of starved diabetic rats decreased glucose production, but gluconeogenesis calculated by the MIDA method was only 48% +/- 3.3%. '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bionli_filtered3['train'][1]['source'].split('[HYP] ')[0].split('[PRE] ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_templates = [\"Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\nDoes the premise entail the hypothesis?\\n\\n{options_}\",\n",
    "\"Premise: {premise}\\nHypothesis: {hypothesis}\\nIs the hypothesis entailed by the premise?\\n{options_}\",\n",
    "\"Here is a premise:\\n{premise}\\n\\nHere is a hypothesis:\\n{hypothesis}\\n\\nIs it possible to conclude that if the premise is true, then so is the hypothesis?\\n{options_}\",\n",
    "\"Sentence 1: {premise}\\n\\nSentence 2: {hypothesis}\\nIs this second sentence entailed by the first sentence?\\n\\n{options_}\",\n",
    "\"Sentence 1: {premise}\\n\\nSentence 2: {hypothesis}\\n\\nIf the first sentence is true, then is the second sentence true?\\n{options_}\",\n",
    "\"Based on the premise \\\"{premise}\\\", can we conclude the hypothesis \\\"{hypothesis}\\\" is true?\\n\\n{options_}\",\n",
    "\"Premise: \\\"{premise}\\\" If this premise is true, what does that tell us about whether it entails the hypothesis \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"Premise:\\n\\\"{premise}\\\" Based on this premise, is the hypothesis \\\"{hypothesis}\\\" true?\\n{options_}\",\n",
    "\"If {premise}, can we conclude that \\\"{hypothesis}\\\"?\\n{options_}\",\n",
    "\"{premise}\\n\\nDoes it follow that \\\"{hypothesis}\\\"?\\n{options_}\"]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "        pre = source.split('[HYP] ')[0].split('[PRE] ')[1]  # premise 추출\n",
    "        hyp = source.split('[HYP] ')[1]  # hypothesis 추출\n",
    "        options = f\"'contradiction' or 'entailment'\"\n",
    "\n",
    "        for template in mnli_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(premise=pre, hypothesis=hyp, options_=options))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "bionli_augmented = bionli_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=bionli_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    return {\n",
    "        \"source\": f\"Question: {example['question']} Sentence 1: {example['sentence1']} Sentence 2: {example['sentence2']}\",\n",
    "        \"target\": example[\"gold_label\"],  # entails, contradicts, neutral\n",
    "        \"task\": \"task0_scitail\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "scitail_filtered3 = scitail_filtered2.map(transform_data, remove_columns=scitail_filtered[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Question: How many different main types of diabetes are there? Sentence 1: There are two different types of diabetes. Sentence 2: There are two different main types of diabetes.',\n",
       " 'target': 'entails',\n",
       " 'task': 'task0_scitail'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scitail_filtered3['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnli_templates = [\n",
    "\"If \\\"{sentence1}\\\", can we conclude that \\\"{sentence2}\\\"\\n{options_}\",\n",
    "\"If \\\"{sentence1}\\\", does it follow that \\\"{sentence2}\\\"\\n{options_}\",\n",
    "\"If \\\"{sentence1}\\\", is \\\"{sentence2}\\\" correct?\\n\\n{options_}\",\n",
    "\"Let's say that \\\"{sentence1}\\\"\\n\\nCan we now say that \\\"{sentence2}\\\"?\\n\\n{options_}\",\n",
    "\"\\\"{sentence1}\\\" is a true sentence.\\n\\nDoes this mean that \\\"{sentence2}\\\"?\\n\\n{options_}\",\n",
    "\"Does \\\"{sentence2}\\\" appear to be an accurate statement based on \\\"{sentence1}\\\"?\\n\\n{options_}\",\n",
    "\"Can we conclude that \\\"{sentence2}\\\" if the statement \\\"{sentence1}\\\" is true?\\n\\n{options_}\",\n",
    "\"Is it possible to draw the conclusion that \\\"{sentence2}\\\" if \\\"{sentence1}\\\"?\\n\\n{options_}\",\n",
    "\"Is \\\"{sentence2}\\\" true if \\\"{sentence1}\\\"?\\n\\n{options_}\",\n",
    "\"Sentence 1: \\\"{sentence1}\\\"\\n\\n Sentence 2: \\\"{sentence2}\\\"\\n\\nIs sentence 2 true, based on sentence 1?\\n\\n{options_}\",\n",
    "]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "        sentence1 = source.split('Sentence 2: ')[0].split('Sentence 1: ')[1]\n",
    "        sentence2 = source.split('Sentence 2: ')[1]\n",
    "        options = f\"'neutral' or 'entails'\"\n",
    "\n",
    "        for template in wnli_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(sentence1=sentence1, sentence2=sentence2, options_=options))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "scitail_augmented = scitail_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=scitail_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    return {\n",
    "        \"source\": f\"Premise: {example['sentence1']} Hypothesis: {example['sentence2']}\",\n",
    "        \"target\": example[\"gold_label\"],  # entailment, contradiction, neutral\n",
    "        \"task\": \"task0_mednli\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "mednli_filtered3 = mednli_filtered2.map(transform_data, remove_columns=mednli_filtered2[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_templates = [\n",
    "\"If \\\"{premise}\\\", does this mean that \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"If \\\"{premise}\\\", can we conclude \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"If \\\"{premise}\\\", does it logically follow that \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"Based on the sentence \\\"{premise}\\\", is the sentence \\\"{hypothesis}\\\" a true sentence?\\n\\n{options_}\",\n",
    "\"Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\n.Can we conclude that the hypothesis is true if the premise is true?\\n\\n{options_}\",\n",
    "\"Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\n.Given the premise, can we conclude the hypothesis?\\n\\n{options_}\",\n",
    "\"Here is a premise: \\\"{premise}\\\"\\n\\nHere is a hypothesis: \\\"{hypothesis}\\\"\\n\\n.Does the premise tell us whether the hypothesis is true?\\n\\n{options_}\",\n",
    "\"Is it possible to conclude that \\\"{premise}\\\" if \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"Is the premise \\\"{premise}\\\" true if \\\"{hypothesis}\\\"?\\n\\n{options_}\",\n",
    "\"Write a brief sentence.\",\n",
    "]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "        pre = source.split('Hypothesis: ')[0].split('Premise: ')[1]\n",
    "        hyp = source.split('Hypothesis: ')[1]\n",
    "        options = f\"'contradiction' or 'entailment' or 'neutral'\"\n",
    "\n",
    "        for template in snli_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(premise=pre, hypothesis=hyp, options_=options))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "mednli_augmented = mednli_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=mednli_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def transform_data(example):\n",
    "    entities_json = json.dumps(example[\"entities\"], ensure_ascii=False)  # JSON 문자열 변환\n",
    "\n",
    "    return {\n",
    "        \"source\": f\"Text: {example['text']}\",\n",
    "        \"target\": entities_json,  # JSON 문자열로 저장\n",
    "        \"task\": \"task0_ner\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "chemdner_filtered3 = chemdner_filtered2.map(transform_data, remove_columns=chemdner_filtered2[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_templates = [\n",
    "\"Extract all chemical and drug entities mentioned in the given text.: {text}\",\n",
    "\"Identify all mentions of chemicals and pharmaceutical drugs in the passage.: {text}\",\n",
    "\"Identify all drugs and chemical compounds in the text.: {text}\",\n",
    "\"Find and list all chemical substances and medication names appearing in the text.: {text}\",\n",
    "\"Extract the names of all pharmaceuticals and biochemical compounds from the passage.: {text}\",\n",
    "]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "\n",
    "        text = source.split('Text: ')[1]\n",
    "        \n",
    "        for template in chem_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(text=text))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "chemdner_augmented = chemdner_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=chemdner_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    # 대화 데이터에서 human의 질문 찾기\n",
    "    human_input = next(conv[\"value\"].split(\"\\nOUTPUT:\")[0] for conv in example[\"conversations\"] if conv[\"from\"] == \"human\")\n",
    "    \n",
    "    # 대화 데이터에서 agent의 답변 찾기\n",
    "    agent_output = next(conv[\"value\"] for conv in example[\"conversations\"] if conv[\"from\"] == \"agent\")\n",
    "\n",
    "    return {\n",
    "        \"source\": f\"Question: {human_input}\",\n",
    "        \"target\": agent_output,\n",
    "        \"task\": \"task0_ddi\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "ddi_filtered3 = ddi_filtered2.map(transform_data, remove_columns=ddi_filtered2[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    return {\n",
    "        \"source\": example['dialogue'],\n",
    "        \"target\": example[\"section_text\"],  # entailment, contradiction, neutral\n",
    "        \"task\": \"task0_mts\"\n",
    "    }\n",
    "\n",
    "# 기존 컬럼 삭제 후 변환 적용\n",
    "mts_filtered3 = mts_filtered2.map(transform_data, remove_columns=mts_filtered2[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mts_templates = [\n",
    "\"Convert the following doctor-patient conversation into a structured clinical note.: {text}\",\n",
    "\"Summarize the provided conversation into a well-structured clinical documentation format.: {text}\",\n",
    "\"Transform the given dialogue into a structured medical report, summarizing key points.: {text}\",\n",
    "\"Generate a medical report based on the following patient-doctor conversation.: {text}\"\n",
    "]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "\n",
    "        text = source\n",
    "        \n",
    "        for template in mts_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(text=text))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "mts_augmented = mts_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=mts_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(example):\n",
    "    return {\n",
    "        \"source\": example['input'],\n",
    "        \"target\": example['output'],\n",
    "        \"task\": \"task0_chat_doctor\"\n",
    "    }\n",
    "\n",
    "chat_doctor_filtered3 = chat_doctor_filtered2.map(transform_data, remove_columns=chat_doctor_filtered2[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_doctor_templates = [\n",
    "\"Provide a medical response to the following patient inquiry.: {text}\",\n",
    "\"A patient presents with the following complaint. As a doctor, respond appropriately.: {text}\",\n",
    "\"You are an AI doctor. Based on the patient’s input, provide a comprehensive medical response.: {text}\",\n",
    "\"Given the patient's medical concern, provide a professional response as a doctor.: {text}\",\n",
    "\"Analyze the patient’s symptoms and generate a medical response accordingly.: {text}\"\n",
    "]\n",
    "\n",
    "def augment_(batch):\n",
    "    augmented_samples = { \"source\": [], \"target\": [], \"task\": [] }\n",
    "\n",
    "    for source, target, task in zip(batch[\"source\"], batch[\"target\"], batch[\"task\"]):\n",
    "\n",
    "        text = source\n",
    "        \n",
    "        for template in chat_doctor_templates:\n",
    "            augmented_samples[\"source\"].append(template.format(text=text))\n",
    "            augmented_samples[\"target\"].append(target)  # 기존 target 유지\n",
    "            augmented_samples[\"task\"].append(task)  # 기존 task 유지\n",
    "\n",
    "    return augmented_samples  # 🔥 딕셔너리 형태의 리스트 반환\n",
    "\n",
    "# 데이터 증강 적용 (batched=True)\n",
    "chat_doctor_augmented = chat_doctor_filtered3.map(\n",
    "    augment_, \n",
    "    batched=True, \n",
    "    remove_columns=chat_doctor_filtered3[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 682\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 368\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 481\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 8128\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 4377\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 1430\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 780\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "scitail\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 380\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mednli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 2040\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 1110\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 2110\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "ddi\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 187\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "mts\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 236\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 128\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chat_doctor\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'target', 'task'],\n",
      "        num_rows: 135\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"medqa\")\n",
    "print(medqa_filtered3)\n",
    "print(\"------\" * 100)\n",
    "print(\"medmcqa\")\n",
    "print(medmcqa_filtered3)\n",
    "print(\"------\" * 100)\n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa_filtered3)\n",
    "print(\"------\" * 100)\n",
    "print(\"bionli\")\n",
    "print(bionli_augmented)\n",
    "print(\"------\" * 100)\n",
    "print(\"scitail\")\n",
    "print(scitail_augmented)\n",
    "print(\"------\" * 100)\n",
    "print(\"mednli\")\n",
    "print(mednli_augmented)\n",
    "print(\"------\" * 100)\n",
    "print(\"chemdner\")\n",
    "print(chemdner_augmented)\n",
    "print(\"------\" * 100)\n",
    "print(\"ddi\")\n",
    "print(ddi_filtered3)\n",
    "print(\"------\" * 100)\n",
    "print(\"mts\")\n",
    "print(mts_augmented)\n",
    "print(\"------\" * 100)\n",
    "print(\"chat_doctor\")\n",
    "print(chat_doctor_augmented)\n",
    "print(\"------\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target', 'task'],\n",
       "        num_rows: 9790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "train_augmented = concatenate_datasets([medqa_filtered3['train'], medmcqa_filtered3['train'], pubmedqa_filtered3['train'].shuffle(seed=42).select(range(1000)), bionli_augmented['train'], scitail_augmented['train'], mednli_augmented['train'], chemdner_augmented['train'], ddi_filtered3['train'], mts_augmented['train'], mts_augmented['test'], chat_doctor_augmented['train'], chat_doctor_augmented['test']])\n",
    "\n",
    "augmented_dataset = DatasetDict({\n",
    "    \"train\": train_augmented,\n",
    "})\n",
    "\n",
    "augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task', 'instruction', 'input', 'output'],\n",
       "    num_rows: 9790\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset = augmented_dataset['train']\n",
    "\n",
    "def transform_data(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"source\"], \n",
    "        \"input\": \"\",  \n",
    "        \"output\": example[\"target\"], \n",
    "        \"task\": example[\"task\"], \n",
    "    }\n",
    "\n",
    "transformed_dataset = augmented_dataset.map(transform_data, remove_columns=augmented_dataset.column_names)\n",
    "\n",
    "transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['task', 'instruction', 'input', 'output'],\n",
       "        num_rows: 9790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\"train\": transformed_dataset})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'task'],\n",
       "    num_rows: 27711\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flan = load_dataset(\"passionMan/flan_diabetes\", split = \"train\")\n",
    "flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'task', 'instruction', 'output'],\n",
      "    num_rows: 37501\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# 데이터셋 정렬: 공통 컬럼 맞추기\n",
    "common_columns = list(set(dataset_dict[\"train\"].column_names) & set(flan.column_names))\n",
    "\n",
    "# 필요한 컬럼만 선택 (공통된 컬럼만 유지)\n",
    "dataset_dict_aligned = dataset_dict[\"train\"].select_columns(common_columns)\n",
    "flan_aligned = flan.select_columns(common_columns)\n",
    "\n",
    "# 두 데이터셋 합치기\n",
    "merged_dataset = concatenate_datasets([dataset_dict_aligned, flan_aligned])\n",
    "\n",
    "# 결과 확인\n",
    "print(merged_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fb9857b8ef427e93d223e777a87aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c600d95ae11148b193deabf56b5c6ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/38 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/passionMan/flan_diabetes_augmented/commit/4a52887156b11d1c41206cb38ac47514248a90e9', commit_message='Upload dataset', commit_description='', oid='4a52887156b11d1c41206cb38ac47514248a90e9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/passionMan/flan_diabetes_augmented', endpoint='https://huggingface.co', repo_type='dataset', repo_id='passionMan/flan_diabetes_augmented'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "merged_dataset.push_to_hub(\"passionMan/flan_diabetes_augmented\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d54c56cae9b4cc1a82ea88713207934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0552ec13ae4f406a85e312e3f4f82ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/63.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32a498d48b44c1eb520465b4703b920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/37501 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'task', 'instruction', 'output'],\n",
       "        num_rows: 37501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "merged = load_dataset(\"passionMan/flan_diabetes_augmented\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'task', 'instruction', 'output'],\n",
       "    num_rows: 37501\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"passionMan/flan_diabetes_augmented\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'task': 'task0_medcqa',\n",
       " 'instruction': 'Question: Screening for Diabetic Retinopathy must be done at what duration from the time of diagnosis of Diabetes? Options: A) Type 2 DM Immediately, B) Type 2 DM Within 6 months, C) Type 2 DM Within 1 year, D) Type 1 DM Within one week',\n",
       " 'output': 'A) Type 2 DM Immediately',\n",
       " 'is_related': 'LABEL_0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 원본 데이터 개수: 37501\n",
      "📌 MMR 적용 후 남은 데이터 개수: 4169\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dataset = load_dataset(\"passionMan/flan_diabetes_augmented\", split=\"train\")\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df[\"input\"] = df[\"input\"].str.replace(\"\\n\", \" \").str.strip()\n",
    "df[\"instruction\"] = df[\"instruction\"].str.replace(\"\\n\", \" \").str.strip()\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "texts = df[\"input\"] + \" \" + df[\"instruction\"]\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "def mmr_filtering(embeddings, similarity_matrix, threshold=0.40):\n",
    "    selected_indices = []\n",
    "    total_samples = len(embeddings)\n",
    "    \n",
    "    for i in range(total_samples):\n",
    "        if not any(similarity_matrix[i][j] > threshold for j in selected_indices):\n",
    "            selected_indices.append(i)\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "filtered_indices = mmr_filtering(embeddings, similarity_matrix, threshold=0.50)\n",
    "filtered_df = df.iloc[filtered_indices]\n",
    "\n",
    "print(f\"📌 원본 데이터 개수: {len(df)}\")\n",
    "print(f\"📌 MMR 적용 후 남은 데이터 개수: {len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 67-year-old woman comes to the phy...</td>\n",
       "      <td>B: Administration of ibuprofen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 52-year-old woman with type 2 diab...</td>\n",
       "      <td>A: Pemphigus vulgaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 72-year-old woman with a medical h...</td>\n",
       "      <td>C: Serum intact parathyroid hormone level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 48-year-old woman presents to her ...</td>\n",
       "      <td>C: Varenicline and nicotine gum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 58-year-old obese woman presents w...</td>\n",
       "      <td>D: Multiple pregnancies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37485</th>\n",
       "      <td></td>\n",
       "      <td>multi_news_1_0_0</td>\n",
       "      <td>– After suffering 10 deaths from the West Nile...</td>\n",
       "      <td>Dallas Mayor Mike Rawlings declared Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37486</th>\n",
       "      <td></td>\n",
       "      <td>multi_news_1_0_0</td>\n",
       "      <td>News article:       Sam Sianis, owner of the B...</td>\n",
       "      <td>– The Cubs beat the Brewers 4-0 last night, ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37487</th>\n",
       "      <td></td>\n",
       "      <td>multi_news_1_0_0</td>\n",
       "      <td>Women with insomnia or other sleep problems ha...</td>\n",
       "      <td>– A new study suggests that one way to cut dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37489</th>\n",
       "      <td></td>\n",
       "      <td>multi_news_1_0_0</td>\n",
       "      <td>News article:  This photo provided by the Long...</td>\n",
       "      <td>– A firefighter in Long Beach, Calif., was kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37496</th>\n",
       "      <td></td>\n",
       "      <td>task291_semeval_2020_task4_commonsense_validation</td>\n",
       "      <td>In this task, you are given two natural langua...</td>\n",
       "      <td>second\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4169 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      input                                               task  \\\n",
       "0                                                  task0_medqa   \n",
       "1                                                  task0_medqa   \n",
       "2                                                  task0_medqa   \n",
       "3                                                  task0_medqa   \n",
       "6                                                  task0_medqa   \n",
       "...     ...                                                ...   \n",
       "37485                                         multi_news_1_0_0   \n",
       "37486                                         multi_news_1_0_0   \n",
       "37487                                         multi_news_1_0_0   \n",
       "37489                                         multi_news_1_0_0   \n",
       "37496        task291_semeval_2020_task4_commonsense_validation   \n",
       "\n",
       "                                             instruction  \\\n",
       "0      Question: A 67-year-old woman comes to the phy...   \n",
       "1      Question: A 52-year-old woman with type 2 diab...   \n",
       "2      Question: A 72-year-old woman with a medical h...   \n",
       "3      Question: A 48-year-old woman presents to her ...   \n",
       "6      Question: A 58-year-old obese woman presents w...   \n",
       "...                                                  ...   \n",
       "37485  – After suffering 10 deaths from the West Nile...   \n",
       "37486  News article:       Sam Sianis, owner of the B...   \n",
       "37487  Women with insomnia or other sleep problems ha...   \n",
       "37489  News article:  This photo provided by the Long...   \n",
       "37496  In this task, you are given two natural langua...   \n",
       "\n",
       "                                                  output  \n",
       "0                         B: Administration of ibuprofen  \n",
       "1                                  A: Pemphigus vulgaris  \n",
       "2              C: Serum intact parathyroid hormone level  \n",
       "3                        C: Varenicline and nicotine gum  \n",
       "6                                D: Multiple pregnancies  \n",
       "...                                                  ...  \n",
       "37485  Dallas Mayor Mike Rawlings declared Wednesday ...  \n",
       "37486  – The Cubs beat the Brewers 4-0 last night, ga...  \n",
       "37487  – A new study suggests that one way to cut dow...  \n",
       "37489  – A firefighter in Long Beach, Calif., was kil...  \n",
       "37496                                         second\\n\\n  \n",
       "\n",
       "[4169 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4169/4169 [33:05<00:00,  2.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to filtered_df_with_is_related.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 설정\n",
    "API_KEY = os.getenv(\"OPENAI_API\")\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# 진행 상황을 저장할 체크포인트 파일\n",
    "checkpoint_file = \"classification_checkpoint.txt\"\n",
    "output_file = \"filtered_df_with_is_related.jsonl\"\n",
    "\n",
    "def classify_instruction(instruction, output):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following instruction and its corresponding output, determine if it is related to diabetes.\n",
    "    \n",
    "    Instruction: \"{instruction}\"\n",
    "    Output: \"{output}\"\n",
    "    \n",
    "    If the instruction or output is related to diabetes in any way, return 1.\n",
    "    If both are completely unrelated to diabetes, return 0.\n",
    "    Only return a single integer value (1 or 0) with no additional text.\n",
    "    \n",
    "    A topic is considered related to diabetes if it falls into one of the following categories or has any indirect connection:\n",
    "    - Causes, symptoms, treatments, or prevention methods of diabetes\n",
    "    - Nutrition management, exercise, and lifestyle habits for diabetic patients\n",
    "    - Diabetes-related complications (e.g., kidney disease, diabetic retinopathy)\n",
    "    - Any discussion on blood sugar levels, insulin, glucose metabolism, or endocrinology\n",
    "    - General discussions about metabolic disorders, obesity, or conditions that may influence diabetes\n",
    "    - Medications, medical devices, or technologies used for diabetes treatment or monitoring\n",
    "    - Any experimental setting, research study, or medical discussion mentioning diabetes\n",
    "    \n",
    "    Be strict in your classification. If diabetes is only vaguely mentioned but not a significant aspect, classify it as 0.\n",
    "    Only return 1 if diabetes is clearly relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a strict and precise classifier.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return int(response.choices[0].message.content.strip())\n",
    "    except openai.BadRequestError as e:\n",
    "        # 토큰 길이 초과 오류 처리 (related=0으로 설정하고 넘어감)\n",
    "        if \"context_length_exceeded\" in str(e):\n",
    "            return 0\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 체크포인트에서 마지막으로 처리한 index 불러오기\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            return int(f.read().strip())\n",
    "    return 0\n",
    "\n",
    "def save_checkpoint(index):\n",
    "    with open(checkpoint_file, \"w\") as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "# 데이터 불러오기 (filtered_df 가 이미 로드되어 있다고 가정)\n",
    "tqdm.pandas()\n",
    "start_index = load_checkpoint()\n",
    "\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in tqdm(filtered_df.iloc[start_index:].iterrows(), total=len(filtered_df) - start_index):\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict[\"is_related\"] = classify_instruction(row[\"instruction\"], row[\"output\"])\n",
    "        f.write(json.dumps(row_dict, ensure_ascii=False) + \"\\n\")\n",
    "        save_checkpoint(idx)  # 진행 상태 저장\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"filtered_df_with_is_related.jsonl\"\n",
    "\n",
    "# JSONL 파일을 DataFrame으로 로드\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# 'is_related' 값이 1인 데이터 필터링\n",
    "filtered_df = df[df[\"is_related\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 가장 긴 input의 길이: 4433.304160688665 글자\n"
     ]
    }
   ],
   "source": [
    "# input 컬럼에서 가장 긴 문자열 길이 찾기\n",
    "max_input_length = filtered_df[\"instruction\"].str.len().mean()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"📌 가장 긴 input의 길이: {max_input_length} 글자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 토큰 수가 3072 이하인 행 개수: 2027개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1601349/1451538927.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"num_tokens\"] = filtered_df[\"instruction\"].apply(lambda x: len(tokenizer.tokenize(x)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 로드 (모델에 따라 변경 가능)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 'instruction' 컬럼의 토큰 수 계산\n",
    "filtered_df[\"num_tokens\"] = filtered_df[\"instruction\"].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# 3072 이하인 행만 필터링\n",
    "filtered_df = filtered_df[filtered_df[\"num_tokens\"] <= 3072]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"📌 토큰 수가 3072 이하인 행 개수: {len(filtered_df)}개\")\n",
    "\n",
    "# JSONL로 저장\n",
    "# filtered_df.drop(columns=[\"num_tokens\"]).to_json(\"/mnt/data/filtered_df_3072.jsonl\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>is_related</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 52-year-old woman with type 2 diab...</td>\n",
       "      <td>A: Pemphigus vulgaris</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 72-year-old woman with a medical h...</td>\n",
       "      <td>C: Serum intact parathyroid hormone level</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input         task                                        instruction  \\\n",
       "1        task0_medqa  Question: A 52-year-old woman with type 2 diab...   \n",
       "2        task0_medqa  Question: A 72-year-old woman with a medical h...   \n",
       "\n",
       "                                      output  is_related  num_tokens  \n",
       "1                      A: Pemphigus vulgaris           1         156  \n",
       "2  C: Serum intact parathyroid hormone level           1         218  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'task': 'task0_medqa',\n",
       " 'instruction': 'Question: A 52-year-old woman with type 2 diabetes mellitus comes to the physician because of a 2-day history of blisters on her forearms and pain during sexual intercourse. Her only medications are metformin and glyburide. Examination reveals multiple, flaccid blisters on the volar surface of the forearms and ulcers on the buccal, gingival, and vulvar mucosa. The epidermis on the forearm separates when the skin is lightly stroked. Which of the following is the most likely diagnosis? Options: A: Pemphigus vulgaris, B: Behcet disease, C: Dermatitis herpetiformis, D: Toxic epidermal necrolysis',\n",
       " 'output': 'A: Pemphigus vulgaris',\n",
       " 'is_related': 1,\n",
       " 'num_tokens': 156}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n",
      "task0_pubmedqa                                       165\n",
      "task848_pubmedqa_classification                       67\n",
      "task1587_scifact_classification                       62\n",
      "gem_wiki_lingua_english_en_1_1_0                      56\n",
      "task0_medcqa                                          50\n",
      "                                                    ... \n",
      "task604_flores_translation_entosn                      1\n",
      "wmt16_translate_fi_en_1_0_0                            1\n",
      "task042_qasc_incorrect_option_generation               1\n",
      "quoref_Guess_Title_For_Context                         1\n",
      "task291_semeval_2020_task4_commonsense_validation      1\n",
      "Name: count, Length: 350, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df['task'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(filtered_df['task'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['task0_medqa', 'task0_medcqa', 'task0_pubmedqa', 'task0_bionli',\n",
       "       'task0_scitail', 'task0_mednli', 'task0_ner', 'task0_mts',\n",
       "       'task0_chat_doctor',\n",
       "       'task298_storycloze_correct_end_classification',\n",
       "       'huggingface_xsum', 'task904_hate_speech_offensive_classification',\n",
       "       'task1232_ted_translation_ar_es',\n",
       "       'task238_iirc_answer_from_passage_answer_generation',\n",
       "       'task1288_glue_mrpc_paraphrasing',\n",
       "       'task1434_head_qa_classification', 'cot_strategyqa',\n",
       "       'task653_parsinlu_fa_en_translation', 'fix_punct',\n",
       "       'task041_qasc_answer_generation', 'unified_qa_science_inst',\n",
       "       'quail_context_question_description_answer_id',\n",
       "       'adversarial_qa_dbert_generate_question',\n",
       "       'task230_iirc_passage_classification',\n",
       "       'task1285_kpa_keypoint_matching',\n",
       "       'task1435_ro_sts_parallel_language_translation_ro_to_en',\n",
       "       'task349_squad2_0_answerable_unanswerable_question_classification',\n",
       "       'task846_pubmedqa_classification',\n",
       "       'quail_description_context_question_answer_id',\n",
       "       'race_high_Select_the_best_answer_generate_span_',\n",
       "       'wiki_qa_Direct_Answer_to_Question',\n",
       "       'duorc_ParaphraseRC_answer_question',\n",
       "       'task469_mrqa_answer_generation',\n",
       "       'race_middle_Select_the_best_answer_generate_span_',\n",
       "       'task1308_amazonreview_category_classification',\n",
       "       'qasc_qa_with_separated_facts_3', 'task591_sciq_answer_generation',\n",
       "       'task421_persent_sentence_sentiment_classification',\n",
       "       'task1594_yahoo_answers_topics_question_generation',\n",
       "       'bool_q_1_0_0', 'task1090_ted_translation_en_gl',\n",
       "       'task218_rocstories_swap_order_answer_generation',\n",
       "       'quail_context_description_question_text',\n",
       "       'task1287_glue_qqp_paraphrasing',\n",
       "       'task169_strategyqa_sentence_generation',\n",
       "       'ropes_given_background_situation',\n",
       "       'race_high_Read_the_article_and_answer_the_question_no_option_',\n",
       "       'wiqa_effect_with_label_answer', 'glue_qnli_2_0_0',\n",
       "       'task1529_scitail1_1_classification',\n",
       "       'adversarial_qa_dbidaf_answer_the_following_q',\n",
       "       'task558_alt_translation_en_hi',\n",
       "       'task1432_head_qa_language_translation_en_to_es',\n",
       "       'task522_news_editorial_summary',\n",
       "       'quartz_use_info_from_question_paragraph',\n",
       "       'task470_mrqa_question_generation', 'wiki_qa_Decide_good_answer',\n",
       "       'task1536_daily_dialog_happiness_classification',\n",
       "       'cot_strategyqa_ii',\n",
       "       'adversarial_qa_droberta_question_context_answer',\n",
       "       'task033_winogrande_answer_generation',\n",
       "       'task1115_alt_ja_id_translation', 'cot_creak_ii',\n",
       "       'task038_qasc_combined_fact', 'task1661_super_glue_classification',\n",
       "       'wiki_bio_key_content',\n",
       "       'task1167_penn_treebank_coarse_pos_tagging',\n",
       "       'task1256_ted_translation_pl_en', 'qasc_qa_with_separated_facts_1',\n",
       "       'adversarial_qa_dbidaf_question_context_answer',\n",
       "       'task1271_ted_translation_fa_it',\n",
       "       'task339_record_answer_generation',\n",
       "       'task1093_ted_translation_en_fa',\n",
       "       'ropes_background_new_situation_answer',\n",
       "       'cos_e_v1_11_question_option_description_text',\n",
       "       'task1164_coda19_section_correction_classification',\n",
       "       'wiki_hop_original_generate_subject',\n",
       "       'adversarial_qa_dbert_answer_the_following_q',\n",
       "       'task521_trivia_question_classification',\n",
       "       'task1593_yahoo_answers_topics_classification',\n",
       "       'duorc_ParaphraseRC_build_story_around_qa',\n",
       "       'quoref_Context_Contains_Answer',\n",
       "       'kilt_tasks_hotpotqa_straighforward_qa',\n",
       "       'task1375_newscomm_translation',\n",
       "       'duorc_ParaphraseRC_generate_question',\n",
       "       'task769_qed_summarization', 'glue_stsb_2_0_0',\n",
       "       'ropes_prompt_bottom_hint_beginning',\n",
       "       'social_i_qa_Show_choices_and_generate_answer', 'squad_v1_1_3_0_0',\n",
       "       'winogrande_1_1_0', 'task540_alt_translation_la_en',\n",
       "       'task388_torque_token_classification',\n",
       "       'task310_race_classification',\n",
       "       'task593_sciq_explanation_generation',\n",
       "       'task854_hippocorpus_classification',\n",
       "       'task279_stereoset_classification_stereotype',\n",
       "       'task503_scruples_anecdotes_isanswerable',\n",
       "       'quail_context_description_question_answer_text',\n",
       "       'task358_casino_classification_negotiation_uv_part',\n",
       "       'task025_cosmosqa_incorrect_answer_generation', 'glue_mrpc_2_0_0',\n",
       "       'task1658_billsum_summarization',\n",
       "       'adversarial_qa_dbidaf_tell_what_it_is',\n",
       "       'quartz_having_read_above_passage',\n",
       "       'task621_ohsumed_yes_no_numerical_answer_generation', 'cot_creak',\n",
       "       'task002_quoref_answer_generation',\n",
       "       'task224_scruples_anecdotes_ethical_judgment',\n",
       "       'task1729_personachat_generate_next',\n",
       "       'task870_msmarco_answer_generation',\n",
       "       'quail_context_question_description_text',\n",
       "       'race_high_Is_this_the_right_answer',\n",
       "       'task1433_head_qa_language_translation_es_to_en',\n",
       "       'wiki_hop_original_choose_best_object_affirmative_3',\n",
       "       'task030_winogrande_full_person', 'task435_alt_en_ja_translation',\n",
       "       'task181_outcome_extraction', 'task309_race_answer_generation',\n",
       "       'adversarial_qa_dbidaf_based_on',\n",
       "       'task1449_disease_entity_extraction_bc5cdr_dataset',\n",
       "       'task325_jigsaw_classification_identity_attack',\n",
       "       'wiqa_which_of_the_following_is_the_supposed_perturbation',\n",
       "       'wiki_qa_Topic_Prediction_Question_and_Answer_Pair',\n",
       "       'task842_para_pdt_cs_en_translation', 'cot_qasc',\n",
       "       'task847_pubmedqa_question_generation',\n",
       "       'task074_squad1_1_question_generation',\n",
       "       'cos_e_v1_11_explain_why_human',\n",
       "       'task1540_parsed_pdfs_summarization',\n",
       "       'task184_break_generate_question',\n",
       "       'task618_amazonreview_summary_text_generation',\n",
       "       'task494_review_polarity_answer_generation',\n",
       "       'task1294_wiki_qa_answer_verification',\n",
       "       'qasc_qa_with_separated_facts_5', 'task1447_drug_extraction_ade',\n",
       "       'ropes_prompt_bottom_no_hint', 'sciq_Multiple_Choice',\n",
       "       'social_i_qa_Check_if_a_random_answer_is_valid_or_not',\n",
       "       'task303_record_incorrect_answer_generation',\n",
       "       'task586_amazonfood_polarity_classification', 'piqa_1_0_0',\n",
       "       'kilt_tasks_hotpotqa_combining_facts',\n",
       "       'task624_ohsumed_question_answering',\n",
       "       'task1291_multi_news_summarization',\n",
       "       'duorc_ParaphraseRC_movie_director',\n",
       "       'task024_cosmosqa_answer_generation', 'hellaswag_1_1_0',\n",
       "       'task058_multirc_question_answering', 'cnn_dailymail_3_4_0',\n",
       "       'task423_persent_document_sentiment_verification',\n",
       "       'task1553_cnn_dailymail_summarization',\n",
       "       'task848_pubmedqa_classification',\n",
       "       'task1221_ted_translation_en_he',\n",
       "       'quail_context_question_answer_description_id',\n",
       "       'task075_squad1_1_answer_generation',\n",
       "       'task937_defeasible_nli_social_classification',\n",
       "       'quartz_given_the_fact_answer_the_q',\n",
       "       'wiki_hop_original_choose_best_object_interrogative_1',\n",
       "       'ropes_background_situation_middle',\n",
       "       'task1369_healthfact_sentence_generation',\n",
       "       'task668_extreme_abstract_summarization', 'glue_qqp_2_0_0',\n",
       "       'race_middle_Is_this_the_right_answer',\n",
       "       'wiki_hop_original_explain_relation',\n",
       "       'task1110_ted_translation_he_gl', 'ropes_plain_bottom_hint',\n",
       "       'duorc_ParaphraseRC_question_answering',\n",
       "       'task1592_yahoo_answers_topics_classfication', 'quac_1_0_0',\n",
       "       'task1334_sqac_answer_generation', 'task433_alt_hi_en_translation',\n",
       "       'task418_persent_title_generation',\n",
       "       'task1085_pib_translation_english_marathi',\n",
       "       'wiqa_does_the_supposed_perturbation_have_an_effect',\n",
       "       'aeslc_1_0_0',\n",
       "       'task587_amazonfood_polarity_correction_classification',\n",
       "       'task620_ohsumed_medical_subject_headings_answer_generation',\n",
       "       'wiki_bio_what_content', 'quail_description_context_question_text',\n",
       "       'cot_esnli_ii', 'word_segment',\n",
       "       'task604_flores_translation_entosn', 'wmt16_translate_fi_en_1_0_0',\n",
       "       'task623_ohsumed_yes_no_answer_generation',\n",
       "       'task042_qasc_incorrect_option_generation', 'ropes_prompt_mix',\n",
       "       'adversarial_qa_dbert_question_context_answer',\n",
       "       'quoref_Guess_Title_For_Context', 'task1290_xsum_summarization',\n",
       "       'task1162_coda19_title_classification',\n",
       "       'task501_scruples_anecdotes_post_type_verification',\n",
       "       'task379_agnews_topic_classification',\n",
       "       'task1587_scifact_classification',\n",
       "       'task1340_msr_text_compression_compression',\n",
       "       'task1292_yelp_review_full_text_categorization',\n",
       "       'task269_csrg_counterfactual_story_generation',\n",
       "       'wiki_qa_Is_This_True_', 'task471_haspart_answer_generation',\n",
       "       'race_middle_Read_the_article_and_answer_the_question_no_option_',\n",
       "       'task1099_ted_translation_ja_pt', 'gem_common_gen_1_1_0',\n",
       "       'para_crawl_enes',\n",
       "       'task1645_medical_question_pair_dataset_text_classification',\n",
       "       'quoref_What_Is_The_Answer', 'anli_r1_0_1_0',\n",
       "       'sciq_Multiple_Choice_Question_First', 'trec_1_0_0',\n",
       "       'wmt14_translate_fr_en_1_0_0', 'task1712_poki_classification',\n",
       "       'drop_2_0_0', 'coqa_1_0_0',\n",
       "       'race_high_Select_the_best_answer_no_instructions_',\n",
       "       'race_high_Write_a_multi_choice_question_for_the_following_article',\n",
       "       'task564_discofuse_classification',\n",
       "       'task1095_ted_translation_ja_gl',\n",
       "       'task1586_scifact_title_generation',\n",
       "       'adversarial_qa_dbert_tell_what_it_is',\n",
       "       'task425_hindienglish_corpora_en_hi_translation',\n",
       "       'task887_quail_answer_generation',\n",
       "       'ropes_plain_background_situation', 'qasc_is_correct_2',\n",
       "       'ai2_arc_ARC_Easy_1_0_0',\n",
       "       'task226_english_language_answer_relevance_classification',\n",
       "       'task1357_xlsum_summary_generation', 'cot_sensemaking_ii',\n",
       "       'task268_casehold_legal_answer_generation',\n",
       "       'task1274_ted_translation_pt_en', 'cot_sensemaking',\n",
       "       'task511_reddit_tifu_long_text_summarization',\n",
       "       'duorc_SelfRC_movie_director',\n",
       "       'task437_alt_en_ja_answer_generation',\n",
       "       'ropes_read_background_situation',\n",
       "       'sciq_Direct_Question_Closed_Book_', 'quail_no_prompt_text',\n",
       "       'task301_record_question_generation',\n",
       "       'race_middle_Write_a_multi_choice_question_for_the_following_article',\n",
       "       'stream_qed', 'task1328_qa_zre_relation_generation_from_question',\n",
       "       'super_glue_wic_1_0_2', 'qasc_qa_with_separated_facts_2',\n",
       "       'task589_amazonfood_summary_text_generation',\n",
       "       'race_middle_Taking_a_test', 'super_glue_rte_1_0_2',\n",
       "       'task1725_civil_comments_severtoxicity_classification',\n",
       "       'adversarial_qa_dbert_based_on',\n",
       "       'task1344_glue_entailment_classification',\n",
       "       'task326_jigsaw_classification_obscene',\n",
       "       'task419_persent_answer_generation', 'quail_no_prompt_id',\n",
       "       'task1495_adverse_drug_event_classification',\n",
       "       'task498_scruples_anecdotes_whoiswrong_classification',\n",
       "       'task594_sciq_question_generation', 'ropes_plain_no_background',\n",
       "       'task1236_ted_translation_he_es',\n",
       "       'task1640_aqa1_0_answerable_unanswerable_question_classification',\n",
       "       'task170_hotpotqa_answer_generation', 'cot_ecqa_ii',\n",
       "       'task619_ohsumed_abstract_title_generation', 'anli_r3_0_1_0',\n",
       "       'task1118_alt_ja_fil_translation',\n",
       "       'adversarial_qa_droberta_generate_question',\n",
       "       'duorc_SelfRC_build_story_around_qa',\n",
       "       'race_middle_Select_the_best_answer',\n",
       "       'task192_hotpotqa_sentence_generation',\n",
       "       'task512_twitter_emotion_classification',\n",
       "       'race_middle_Write_a_multi_choice_question_options_given_',\n",
       "       'race_high_Write_a_multi_choice_question_options_given_',\n",
       "       'cosmos_qa_1_0_0', 'yelp_polarity_reviews_0_2_0',\n",
       "       'task1659_title_generation', 'task1431_head_qa_answer_generation',\n",
       "       'paws_wiki_1_1_0', 'glue_mnli_2_0_0', 'wiki_qa_found_on_google',\n",
       "       'task853_hippocorpus_long_text_generation',\n",
       "       'gem_wiki_lingua_english_en_1_1_0',\n",
       "       'adversarial_qa_droberta_tell_what_it_is',\n",
       "       'task849_pubmedqa_answer_generation', 'wiki_bio_who',\n",
       "       'qasc_is_correct_1', 'race_high_Select_the_best_answer',\n",
       "       'task447_opus_paracrawl_classification',\n",
       "       'duorc_ParaphraseRC_decide_worth_it',\n",
       "       'task1368_healthfact_sentence_generation',\n",
       "       'quail_context_question_answer_description_text',\n",
       "       'task1295_adversarial_qa_question_answering',\n",
       "       'task237_iirc_answer_from_subtext_answer_generation', 'true_case',\n",
       "       'super_glue_multirc_1_0_2', 'super_glue_record_1_0_2',\n",
       "       'task201_mnli_neutral_classification',\n",
       "       'adversarial_qa_droberta_answer_the_following_q',\n",
       "       'task302_record_classification',\n",
       "       'task1354_sent_comp_classification',\n",
       "       'task323_jigsaw_classification_sexually_explicit',\n",
       "       'task1608_xquad_en_answer_generation',\n",
       "       'task289_gigaword_summarization', 'ropes_prompt_beginning',\n",
       "       'task601_flores_translation_sntoen',\n",
       "       'duorc_ParaphraseRC_title_generation',\n",
       "       'wiqa_effect_with_string_answer',\n",
       "       'task401_numeric_fused_head_reference',\n",
       "       'task1589_scifact_classification', 'task1389_hellaswag_completion',\n",
       "       'task578_curiosity_dialogs_answer_generation',\n",
       "       'task590_amazonfood_summary_correction_classification',\n",
       "       'task738_perspectrum_classification',\n",
       "       'task111_asset_sentence_simplification', 'squad_v2_0_3_0_0',\n",
       "       'duorc_ParaphraseRC_extract_answer', 'ag_news_subset_1_0_0',\n",
       "       'adversarial_qa_droberta_based_on',\n",
       "       'wiki_hop_original_generate_subject_and_object',\n",
       "       'task461_qasper_question_generation',\n",
       "       'task178_quartz_question_answering',\n",
       "       'task1415_youtube_caption_corrections_grammar_correction',\n",
       "       'task432_alt_en_hi_translation',\n",
       "       'task588_amazonfood_rating_classification',\n",
       "       'quoref_Answer_Friend_Question', 'social_i_qa_Generate_answer',\n",
       "       'wmt16_translate_tr_en_1_0_0', 'task1234_ted_translation_he_en',\n",
       "       'task493_review_polarity_classification', 'sciq_Direct_Question',\n",
       "       'task112_asset_simple_sentence_identification',\n",
       "       'task542_alt_translation_ja_en',\n",
       "       'task1355_sent_comp_summarization',\n",
       "       'task569_recipe_nlg_text_generation',\n",
       "       'task1161_coda19_title_generation',\n",
       "       'task390_torque_text_span_selection',\n",
       "       'task028_drop_answer_generation',\n",
       "       'task311_race_question_generation', 'task934_turk_simplification',\n",
       "       'task1485_organ_extraction_anem_dataset', 'quoref_Guess_Answer',\n",
       "       'task1366_healthfact_classification',\n",
       "       'task049_multirc_questions_needed_to_answer',\n",
       "       'task845_pubmedqa_question_generation',\n",
       "       'task1356_xlsum_title_generation', 'wiki_qa_automatic_system',\n",
       "       'stream_qed_ii', 'task402_grailqa_paraphrase_generation',\n",
       "       'task649_race_blank_question_generation', 'wiki_bio_comprehension',\n",
       "       'race_high_Taking_a_test', 'task871_msmarco_question_generation',\n",
       "       'task840_para_pdt_en_es_translation', 'multi_news_1_0_0',\n",
       "       'task291_semeval_2020_task4_commonsense_validation'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['task'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>is_related</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 52-year-old woman with type 2 diab...</td>\n",
       "      <td>A: Pemphigus vulgaris</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>task0_medqa</td>\n",
       "      <td>Question: A 72-year-old woman with a medical h...</td>\n",
       "      <td>C: Serum intact parathyroid hormone level</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input         task                                        instruction  \\\n",
       "1        task0_medqa  Question: A 52-year-old woman with type 2 diab...   \n",
       "2        task0_medqa  Question: A 72-year-old woman with a medical h...   \n",
       "\n",
       "                                      output  is_related  num_tokens  \n",
       "1                      A: Pemphigus vulgaris           1         156  \n",
       "2  C: Serum intact parathyroid hormone level           1         218  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 예상 실행 시간: 약 40.54 분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/2027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 2027/2027 [50:12<00:00,  1.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료! ⏱ 실제 실행 시간: 50.22 분\n",
      "📂 데이터가 processed_dataset_diabetes.jsonl 파일에 실시간으로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수로부터 API 키 로드\n",
    "load_dotenv()  \n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API\")\n",
    "openai.api_key = OPENAI_API_KEY  # openai API에 인증 정보 설정\n",
    "\n",
    "# 출력 파일 이름 설정\n",
    "output_file = \"processed_dataset_diabetes.jsonl\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def clean_text(text):\n",
    "    # 불필요한 공백, 탭, 개행 문자 및 특수문자 제거\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace('\"', \"'\")\n",
    "    \n",
    "    # JSON 파싱에 방해가 되는 특수 문자를 제거\n",
    "    cleaned_text = ''.join([i if ord(i) > 31 and ord(i) < 127 else ' ' for i in cleaned_text])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# OpenAI API로부터 응답을 받아오는 함수\n",
    "def get_instruction_input(instruction, output):\n",
    "    # 입력과 출력을 정리 (공백 제거, 따옴표 처리)\n",
    "    cleaned_instruction = clean_text(instruction)\n",
    "    text = f\"\"\"\n",
    "    \"input\": {cleaned_instruction}\\n\"output\": {output}\n",
    "    \"\"\"\n",
    "\n",
    "    # GPT 모델을 호출할 프롬프트 설정\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping to reformat dataset instructions. \n",
    "    Given the provided input and its corresponding output, generate an appropriate instruction (\"instruction2\") based on the context. \n",
    "\n",
    "    Your task is to generalize the provided question and its corresponding output into a broader instruction. Ensure that your output reflects the general theme or concept of the input-output pair.\n",
    "    \n",
    "    Your output must always include \"instruction2\".\n",
    "\n",
    "    ### Example:\n",
    "    ---\n",
    "    #### Input:\n",
    "    \"input\": \"Question: A 67-year-old woman comes to the physician for the evaluation of bilateral knee pain for the past year. She reports that the pain is worse with movement and is relieved with rest. She has type 2 diabetes mellitus. The patient says her mother takes leflunomide for a ‘joint condition.’ The patient's medications include metformin and a multivitamin. She is 165 cm (5 ft 5 in) tall and weighs 85 kg (187 lb); BMI is 31.2 kg/m2. Vital signs are within normal limits. Physical examination shows pain in complete flexion and extension, crepitus on joint movement, joint stiffness, and restricted range of motion in both knees. X-ray of the knee joints shows irregular joint space narrowing, subchondral sclerosis, osteophytes, and several subchondral cysts. There is no reddening or swelling. Which of the following is the most appropriate pharmacotherapy? Options: A: Intra-articular glucocorticoid injections, B: Administration of ibuprofen, C: Administration of celecoxib, D: Administration of methotrexate\"\n",
    "    \"output\": \"B: Administration of ibuprofen\"\n",
    "    #### Output: \n",
    "    {{\n",
    "    \"instruction2\": \"What are the recommended pharmacological treatment options for joint pain in patients with metabolic conditions?\"\n",
    "    }}\n",
    "    ---\n",
    "\n",
    "    Now process the following text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # OpenAI API를 호출하여 응답을 받음\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "            max_tokens=4096, \n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "\n",
    "        # 응답이 JSON 형식이 아니면 강제로 JSON 형태로 감쌈\n",
    "        if not result.startswith(\"{\"):\n",
    "            result = f'{{\"instruction2\": \"{result}\"}}'\n",
    "\n",
    "        # 응답을 JSON으로 파싱\n",
    "        result_json = json.loads(result)\n",
    "\n",
    "        # instruction2가 없을 경우 기본값을 사용\n",
    "        instruction2 = result_json.get(\"instruction2\", \"No instruction2 found.\")\n",
    "\n",
    "        return {\"instruction2\": instruction2}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {str(e)}\")\n",
    "        return {\"instruction2\": \"Error in processing\"}\n",
    "\n",
    "# 데이터프레임 로딩 (필요한 경우)\n",
    "df = filtered_df.copy()\n",
    "\n",
    "# 예상 실행 시간 계산\n",
    "num_rows = len(df)\n",
    "avg_time_per_request = 1.2 \n",
    "estimated_time = num_rows * avg_time_per_request / 60  \n",
    "print(f\"🔹 예상 실행 시간: 약 {estimated_time:.2f} 분\")\n",
    "\n",
    "# 진행률 표시 및 데이터 처리 시작\n",
    "tqdm.pandas()\n",
    "start_time = time.time()\n",
    "\n",
    "# 출력 파일에 결과 저장\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "        # 'instruction' 열이 없을 경우 처리\n",
    "        if \"instruction\" not in row:\n",
    "            print(\"❌ 'instruction' 열이 없습니다!\")\n",
    "            continue  # 'instruction' 열이 없으면 해당 행을 건너뜁니다\n",
    "\n",
    "        # API 호출로 응답 처리\n",
    "        result = get_instruction_input(row[\"instruction\"], row[\"output\"])\n",
    "\n",
    "        # 결과를 dict 형태로 변환하여 파일에 저장\n",
    "        row_dict = {\n",
    "            \"task\": row[\"task\"],\n",
    "            \"input\": row[\"instruction\"],\n",
    "            \"output\": row[\"output\"],\n",
    "            \"instruction2\": result[\"instruction2\"],\n",
    "        }\n",
    "        f.write(json.dumps(row_dict, ensure_ascii=False) + \"\\n\")  # JSONL로 저장\n",
    "\n",
    "# 처리 완료 시간 계산\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60  \n",
    "print(f\"✅ 변환 완료! ⏱ 실제 실행 시간: {elapsed_time:.2f} 분\")\n",
    "\n",
    "# 데이터가 파일에 실시간으로 저장되었음을 출력\n",
    "print(f\"📂 데이터가 {output_file} 파일에 실시간으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_gpu_kernel",
   "language": "python",
   "name": "faiss_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
