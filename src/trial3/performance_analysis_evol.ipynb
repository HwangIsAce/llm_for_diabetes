{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# \n",
    "file_paths = [\n",
    "    \"/data/jaesung/llm_for_diabetes/src/trial3/model_response/model_output_evol.jsonl\",\n",
    "    # \"/data/jaesung/llm_for_diabetes/src/trial/CoT_collection/model_response/test_1.jsonl\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# medqa\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa1 = df[df['task'] == 'qa1']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa1)\n",
    "\n",
    "for _, row in qa1.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "# medmcqa\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa2 = df[df['task'] == 'qa2']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa2)\n",
    "\n",
    "for _, row in qa2.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "# pubmedqa\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa3 = df[df['task'] == 'qa3']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa3)\n",
    "\n",
    "for _, row in qa3.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.3667\n"
     ]
    }
   ],
   "source": [
    "# bionli\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "nli = df[df['task'] == 'nli']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(nli)\n",
    "\n",
    "for _, row in nli.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Total Precision: 0.2000\n",
      "GPT-based Total Recall: 0.2000\n",
      "GPT-based Total F1-Score: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# ddi\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_relation_correctness(true_relation, model_relation):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_relation과 model_relation이 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated relationship extraction model.\n",
    "    Compare the following two sets of extracted relationships and determine if they contain the same meaning.\n",
    "\n",
    "    **Correct Relationships**: {true_relation}\n",
    "    **Model Predicted Relationships**: {model_relation}\n",
    "\n",
    "    If the model output correctly conveys the same relationships as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "re_df = df[df['task'] == 're']\n",
    "\n",
    "# 정답 판별 수행 및 메트릭 계산\n",
    "total_true_positive = 0\n",
    "total_false_positive = 0\n",
    "total_false_negative = 0\n",
    "\n",
    "for _, row in re_df.iterrows():\n",
    "    true_relations = set(row['output'].split(', '))\n",
    "    model_relations = set(row['model_output_128'].split(', '))\n",
    "\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    # 모델 예측과 정답을 비교하여 TP, FP, FN 계산\n",
    "    for relation in model_relations:\n",
    "        if any(check_relation_correctness(relation, correct_rel) for correct_rel in true_relations):\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "\n",
    "    for relation in true_relations:\n",
    "        if not any(check_relation_correctness(relation, predicted_rel) for predicted_rel in model_relations):\n",
    "            false_negative += 1\n",
    "\n",
    "    # 누적 합산\n",
    "    total_true_positive += true_positive\n",
    "    total_false_positive += false_positive\n",
    "    total_false_negative += false_negative\n",
    "\n",
    "# Precision, Recall, F1 계산\n",
    "precision = total_true_positive / (total_true_positive + total_false_positive) if (total_true_positive + total_false_positive) > 0 else 0\n",
    "recall = total_true_positive / (total_true_positive + total_false_negative) if (total_true_positive + total_false_negative) > 0 else 0\n",
    "f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"GPT-based Total Precision: {precision:.4f}\")\n",
    "print(f\"GPT-based Total Recall: {recall:.4f}\")\n",
    "print(f\"GPT-based Total F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0091\n",
      "Recall: 0.0100\n",
      "F1-score: 0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_168992/2471990142.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
      "/tmp/ipykernel_168992/2471990142.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n"
     ]
    }
   ],
   "source": [
    "# chemdner\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임 예시 (df_ie 가 주어진 데이터프레임)\n",
    "ie = df[df['task'] == 'ie']\n",
    "\n",
    "ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n",
    "\n",
    "# Precision, Recall, F1-score 계산 함수\n",
    "def calculate_scores(y_true, y_pred):\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "    \n",
    "    for true_vals, pred_vals in zip(y_true, y_pred):\n",
    "        true_set = set(true_vals) if isinstance(true_vals, list) else set()\n",
    "        pred_set = set(pred_vals) if isinstance(pred_vals, list) else set()\n",
    "\n",
    "        TP = len(true_set & pred_set)  # True Positives (정답과 예측이 일치하는 것)\n",
    "        FP = len(pred_set - true_set)  # False Positives (예측했지만 정답이 아닌 것)\n",
    "        FN = len(true_set - pred_set)  # False Negatives (정답이지만 예측하지 못한 것)\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    return sum(all_precisions) / len(all_precisions), sum(all_recalls) / len(all_recalls), sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "# Precision, Recall, F1-score 계산\n",
    "precision, recall, f1 = calculate_scores(ie[\"output\"], ie[\"model_output_128\"])\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Precision: 0.2222, Recall: 0.6667, F1: 0.3333\n",
      "[2/30] Precision: 0.3333, Recall: 0.5000, F1: 0.4000\n",
      "[3/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[4/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[5/30] Precision: 0.2857, Recall: 0.1667, F1: 0.2105\n",
      "[6/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[7/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[8/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[9/30] Precision: 0.5000, Recall: 0.6667, F1: 0.5714\n",
      "[10/30] Precision: 0.2500, Recall: 1.0000, F1: 0.4000\n",
      "[11/30] Precision: 0.3333, Recall: 0.5000, F1: 0.4000\n",
      "[12/30] Precision: 0.0455, Recall: 0.2500, F1: 0.0769\n",
      "[13/30] Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "[14/30] Precision: 0.8000, Recall: 0.6667, F1: 0.7273\n",
      "[15/30] Precision: 0.6667, Recall: 1.3333, F1: 0.8889\n",
      "[16/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[17/30] Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "[18/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[19/30] Precision: 0.0833, Recall: 0.5000, F1: 0.1429\n",
      "[20/30] Precision: 0.1667, Recall: 0.2000, F1: 0.1818\n",
      "[21/30] Precision: 0.2000, Recall: 0.5000, F1: 0.2857\n",
      "[22/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[23/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[24/30] Precision: 0.3333, Recall: 0.5000, F1: 0.4000\n",
      "[25/30] Precision: 0.7273, Recall: 0.8000, F1: 0.7619\n",
      "[26/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[27/30] Precision: 0.2222, Recall: 0.5000, F1: 0.3077\n",
      "[28/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[29/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[30/30] Precision: 0.1429, Recall: 0.1667, F1: 0.1538\n",
      "\n",
      "=== Final Evaluation Scores ===\n",
      "Precision: 0.2437\n",
      "Recall: 0.3639\n",
      "F1-score: 0.2747\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ✅ OpenAI API Key 설정\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# ✅ GPT-3.5를 활용하여 정답과 예측을 비교하는 함수\n",
    "def gpt_judge(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    GPT-3.5를 사용하여 예측값과 정답값이 의미적으로 일치하는지 판단.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in named entity recognition evaluation.\n",
    "    Compare the gold standard answer with the predicted answer and determine whether they match.\n",
    "\n",
    "    Gold Standard: {true_text}\n",
    "    Model Prediction: {pred_text}\n",
    "\n",
    "    Output \"Yes\" if both values are identical, including cases where both are empty (None, \"\", [], or {{}}). Output \"No\" otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return 1 if answer.lower() == \"yes\" else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] OpenAI API call failed: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# ✅ 데이터프레임에서 \"ie\" 태스크 선택\n",
    "ie = df[df['task'] == 'ie'].copy()\n",
    "\n",
    "# ✅ 전처리: 소문자로 변환 후 리스트로 변환\n",
    "ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n",
    "\n",
    "# ✅ GPT-3.5를 이용한 Precision, Recall, F1-score 계산\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "for idx, (true_vals, pred_vals) in enumerate(zip(ie[\"output\"], ie[\"model_output_128\"])):\n",
    "    true_set = set(true_vals) if isinstance(true_vals, list) else set()\n",
    "    pred_set = set(pred_vals) if isinstance(pred_vals, list) else set()\n",
    "\n",
    "    TP = sum(gpt_judge(t, p) for t in true_set for p in pred_set)  # GPT로 TP 판별\n",
    "    FP = len(pred_set) - TP  # False Positives\n",
    "    FN = len(true_set) - TP  # False Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    # ✅ 진행상황 출력\n",
    "    print(f\"[{idx+1}/{len(ie)}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    time.sleep(1)  # OpenAI API Rate Limit 방지\n",
    "\n",
    "# ✅ 최종 성능 계산\n",
    "final_precision = sum(all_precisions) / len(all_precisions)\n",
    "final_recall = sum(all_recalls) / len(all_recalls)\n",
    "final_f1 = sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(\"\\n=== Final Evaluation Scores ===\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1-score: {final_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Precision: 0.2222, Recall: 0.6667, F1: 0.3333\n",
      "[2/30] Precision: 0.6667, Recall: 1.0000, F1: 0.8000\n",
      "[3/30] Precision: 0.1429, Recall: 0.3333, F1: 0.2000\n",
      "[4/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[5/30] Precision: 0.4286, Recall: 0.2500, F1: 0.3158\n",
      "[6/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[7/30] Precision: 0.2500, Recall: 1.0000, F1: 0.4000\n",
      "[8/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[9/30] Precision: 0.7500, Recall: 1.0000, F1: 0.8571\n",
      "[10/30] Precision: 0.2500, Recall: 1.0000, F1: 0.4000\n",
      "[11/30] Precision: 0.3333, Recall: 0.3333, F1: 0.3333\n",
      "[12/30] Precision: 0.0455, Recall: 0.2500, F1: 0.0769\n",
      "[13/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[14/30] Precision: 0.9000, Recall: 0.7500, F1: 0.8182\n",
      "[15/30] Precision: 0.6000, Recall: 1.0000, F1: 0.7500\n",
      "[16/30] Precision: 0.1667, Recall: 0.5000, F1: 0.2500\n",
      "[17/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[18/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[19/30] Precision: 0.0833, Recall: 0.5000, F1: 0.1429\n",
      "[20/30] Precision: 0.1667, Recall: 0.2000, F1: 0.1818\n",
      "[21/30] Precision: 0.3333, Recall: 0.8333, F1: 0.4762\n",
      "[22/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[23/30] Precision: 0.0526, Recall: 0.5000, F1: 0.0952\n",
      "[24/30] Precision: 0.5000, Recall: 0.5000, F1: 0.5000\n",
      "[25/30] Precision: 0.4000, Recall: 0.2000, F1: 0.2667\n",
      "[26/30] Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "[27/30] Precision: 0.1111, Recall: 0.2500, F1: 0.1538\n",
      "[28/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[29/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[30/30] Precision: 0.1667, Recall: 0.1667, F1: 0.1667\n",
      "\n",
      "=== Final Evaluation Scores ===\n",
      "Precision: 0.2357\n",
      "Recall: 0.4078\n",
      "F1-score: 0.2728\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ✅ OpenAI API Key 설정\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# ✅ GPT-4o를 활용하여 정답과 예측을 비교하는 함수\n",
    "def gpt_judge(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    GPT-4o를 사용하여 예측값과 정답값이 의미적으로 일치하는지 판단.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in named entity recognition evaluation.\n",
    "    Compare the gold standard entity with the predicted entity.\n",
    "    \n",
    "    Gold Standard Entity: \"{true_text}\"\n",
    "    Model Predicted Entity: \"{pred_text}\"\n",
    "    \n",
    "    If both entities refer to the same concept, output \"Yes\".\n",
    "    If they are different, output \"No\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return 1 if answer.lower() == \"yes\" else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] OpenAI API call failed: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# ✅ 데이터프레임에서 \"ie\" 태스크 선택\n",
    "ie = df[df['task'] == 'ie'].copy()\n",
    "\n",
    "# ✅ 전처리 함수: 다양한 형태를 리스트로 변환\n",
    "def normalize_to_list(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed_value = json.loads(value)  # JSON 문자열 파싱 시도\n",
    "            if isinstance(parsed_value, list):\n",
    "                return [v.strip().lower() for v in parsed_value]\n",
    "            elif isinstance(parsed_value, dict):\n",
    "                return [v.strip().lower() for v in parsed_value.values()]\n",
    "            else:\n",
    "                return [str(parsed_value).strip().lower()]\n",
    "        except json.JSONDecodeError:\n",
    "            return [v.strip().lower() for v in value.replace(\"\\n\", \",\").split(\",\")]\n",
    "    elif isinstance(value, dict):\n",
    "        return [v.strip().lower() for v in value.values()]\n",
    "    elif isinstance(value, list):\n",
    "        return [v.strip().lower() for v in value]\n",
    "    else:\n",
    "        return [str(value).strip().lower()]\n",
    "\n",
    "# ✅ 데이터 정리\n",
    "ie[\"output\"] = ie[\"output\"].apply(normalize_to_list)\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].apply(normalize_to_list)\n",
    "\n",
    "# ✅ GPT-4o를 이용한 Precision, Recall, F1-score 계산\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "for idx, (true_vals, pred_vals) in enumerate(zip(ie[\"output\"], ie[\"model_output_128\"])):\n",
    "    true_set = set(true_vals)\n",
    "    pred_set = set(pred_vals)\n",
    "\n",
    "    # ✅ GPT-4o로 TP(정답 매칭) 판별\n",
    "    TP = sum(1 for t in true_set if any(gpt_judge(t, p) for p in pred_set))\n",
    "    FP = len(pred_set) - TP  # False Positives (잘못된 예측)\n",
    "    FN = len(true_set) - TP  # False Negatives (누락된 정답)\n",
    "\n",
    "    # ✅ 정밀도, 재현율, F1-score 계산\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    print(f\"[{idx+1}/{len(ie)}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# ✅ 최종 성능 출력\n",
    "print(\"\\n=== Final Evaluation Scores ===\")\n",
    "print(f\"Precision: {sum(all_precisions) / len(all_precisions):.4f}\")\n",
    "print(f\"Recall: {sum(all_recalls) / len(all_recalls):.4f}\")\n",
    "print(f\"F1-score: {sum(all_f1s) / len(all_f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  - The model's response generally aligns with the context, following the sequence of medical evaluation and diagnosis. However, there is a discrepancy in the patient's gender and age, which affects the coherence as compared to the true answer.\n",
      "  \n",
      "- Completeness: 4.5  \n",
      "  - The model's response thoroughly covers most aspects of the patient's case, including symptoms, examination findings, diagnosis, histopathological examination results, and treatment plan. It misses the age and gender detail but captures the clinical details accurately.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  - The response is fluent and human-like, effectively summarizing the medical conversation in a natural manner. Despite being technical, the language is clear and appropriate for a medical summary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is logically aligned with the context provided in the input. It accurately reflects the sequence of events discussed between the doctor and the patient.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response sufficiently answers the main questions regarding the treatment course and outcomes. However, it omits some detailed information from the true answer, such as the age of the patient, their non-hypertensive and non-diabetic status, and the ruling out of pathological and postsurgical causes for retrograde ejaculation.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response is fluent and reads naturally as a clinical note. The phrasing and structure are appropriate for a medical summary, making it sound human-like.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.8\n",
      "- Naturalness: 4.7\n",
      "\n",
      "The model's response is highly coherent, as it logically aligns with the context provided, summarizing the detailed family medical history and suspected condition of systemic mastocytosis, while describing the diagnostic process and treatments accurately. The completeness is also strong, as it includes comprehensive details from the original dialogue, addressing all relevant aspects. The naturalness is high, with the language used being fluent and human-like, maintaining a professional tone suitable for a medical scenario.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0\n",
      "- Completeness: 1.0\n",
      "- Naturalness: 2.5\n",
      "\n",
      "**Explanation**:\n",
      "- **Coherence**: The model's response is incoherent as it suggests Mr. Smith is being informed about his own death, which does not logically align with the conversation context where such information should be conveyed to another person, not the patient.\n",
      "  \n",
      "- **Completeness**: The response is incomplete in addressing the overall medical history and treatment details since it ignores the entire medical discussion and process, instead fixating only on an incorrect outcome.\n",
      "\n",
      "- **Naturalness**: While the language used by the model is somewhat natural, it fails to align with the context of a doctor-patient conversation as it delivers an inappropriate message to the patient about his own demise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It follows the sequence of events and includes key details present in the input dialogue, ensuring that the information is consistently aligned with the patient's history and medical events.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The model's response sufficiently answers the question by covering most of the essential points discussed in the input. However, it omits the detail about \"There was no evidence of pupillary block, intraocular inflammation, iris chafing, or vitreous prolapse\" from the true answer, which slightly impacts the completeness.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The model's response is fluent and human-like. It reads smoothly, using appropriate medical terminology and structuring the information clearly and logically, reflecting how a medical summary would typically be presented by a professional.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is generally coherent and logically aligns with the context provided in the input. It includes most of the key details mentioned in the conversation between the doctor and the patient. There is a consistent narrative that reflects the progression of the patient's condition as discussed.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response captures the core aspects of the patient's medical history, symptoms, and treatments, although it could have been more explicit about certain specific details mentioned in the true answer related to the patient's background (e.g., Assyrian heritage, her move to Sweden), which are not included.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response is fluent and uses appropriate clinical language typical of a well-written clinical note. It reads like a formal documentation of the patient's case, showing a good level of human-like naturalness in its articulation and structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided in the conversation between the doctor and the patient. The clinical note is concise and accurately summarizes the dialogue.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model captures most of the essential details from the conversation, such as the treatment regimen, the significant improvements with ixekizumab, the biopsy results, and advice regarding sun exposure. However, details about the patient's history, such as type 2 diabetes, dyslipidemia, and hypertension, are missing, and the Fitzpatrick skin phototype is not mentioned.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent and reads like a typical clinical note one would expect from a medical professional. The language is precise and appropriate for a medical context, which enhances its naturalness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "  The model's response is coherent with the context provided in the input. It accurately summarizes the interaction between Mrs. Smith and the doctor, including key medical details such as the history of COPD, treatment, and results from the clinical examination. However, there is a minor discrepancy with the patient's age, as the model states she is 65 instead of 69, which slightly impacts coherence.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "  The model covers most of the essential components of Mrs. Smith's medical situation, treatment plans, and test results. It includes details on her dyspnea grade, COPD assessment score, examination findings, and the BODE index. However, certain details from the true answer, such as past smoking history and the mention of specific tests like chest X-ray and HRCT, are missing, impacting completeness.\n",
      "\n",
      "- Naturalness: 4.8\n",
      "\n",
      "  The response is natural and fluent, using appropriate medical terminology and sentence structure that mirrors how a human might summarize the patient-doctor interaction. It effectively conveys the necessary medical information in a smooth and human-like manner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent and logically aligns with the context provided in the input. It accurately reflects the sequence of events and the medical findings as described in the doctor-patient dialogue.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model addresses most of the key points from the true answer, such as the duration of the ear pain, the examination findings, the CT scan results, and the provisional diagnosis. It, however, does not mention the patient's age or the specific treatments mentioned in the true answer (e.g., ticarcillin clavulanate potassium, Ciprofloxacin via PICC line).\n",
      "\n",
      "- Naturalness: 4.2  \n",
      "  The response is generally fluent and human-like, with a clear and structured summary of the situation. However, it has a somewhat mechanical tone that slightly detracts from sounding fully conversational or natural in a patient retelling context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0 - The model's response is highly coherent, as it logically follows the dialogue between the doctor and patient. The repetition of the dialogue indicates alignment with the context provided.\n",
      "- Completeness: 3.0 - While the model's response effectively mirrors the interaction between the doctor and patient, it does not address additional details found in the True Answer, such as the patient's demographic information, the year Li treatment began, or the contextual commentary regarding improvement trends shown in the original summary.\n",
      "- Naturalness: 4.0 - The response closely resembles the tone and structure of human dialogue, utilizing natural language patterning. However, the repetition of the entire conversation instead of providing a synthesized summary reduces natural conversational flow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It effectively summarizes the conversation between the doctor and the patient, capturing the key elements discussed. It maintains coherence with the original dialogue, but it slightly lacks in addressing the context about the hyperkalemia history and the detailed list of medical history described in the input.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The model's response sufficiently covers the main points of the question, including the Rottweiler's age, medical history, pre-operative blood work results, and the doctor's recommendations. However, it omits certain details such as the extreme specificity of the previous treatments and diagnostic tests (e.g., detailed descriptions of past surgeries and the exact range of hyperkalemia), which are present in the True Answer.\n",
      "\n",
      "- Naturalness: 4.8\n",
      "\n",
      "The model's response sounds fluent and human-like, capturing the essence of a case summary with clear and concise language. There is a natural flow to the information presented, which appropriately mirrors the style of medical case notes, but it could slightly benefit from more varied sentence structures to enhance naturalness further.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It correctly summarizes key points such as the timing of the presentation, test results, and treatment. However, it omits some contextual details like the patient’s occupation and the conditions under which the injury occurred, which are present in the conversation and provide important background context.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  While the model's response includes significant medical details from the conversation, it omits several aspects of the scenario, such as the patient's occupation, the specific circumstances of the burn incident, and the lack of the protective waist belt. These details contribute to a fuller understanding of the case but are not included in the summary.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response is fluent and human-like, presenting information clearly and professionally, just as a human medical professional would likely summarize the situation. The language and structure used are appropriate for a medical context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response aligns well logically with the context provided in the input. It captures the sequence of events and details mentioned in the conversation. \n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response is largely complete in terms of capturing the critical points about the patient's condition and medical history. However, it omits specific percentile information for growth metrics, additional laboratory details, and some specifics from the genetic screening, which would give a more complete picture.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The response is fluent and structured in a manner that is typical of clinical notes. It is concise and the style is appropriate for a medical summary, though it lacks a transitional phrase that might be used in natural conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.0\n",
      "\n",
      "The model’s response is partially coherent with the input. While it maintains consistency in some parts (e.g., diagnosis and treatment plan), it introduces an unwarranted addition of chest pain and shortness of breath at the beginning, which was not mentioned in the original dialogue. This inconsistency affects the coherence.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The response is mostly complete, adequately covering the important medical history, diagnosis, and surgical plan details provided in the input. However, the addition of symptoms not present in the original context slightly detracts from the completeness concerning the specific input details.\n",
      "\n",
      "- Naturalness: 4.0\n",
      "\n",
      "The model's response is mostly fluent and sounds human-like, with an appropriate dialogue flow between the doctor and the patient. The conversational style is maintained properly throughout the response. However, the duplication of certain phrases and structurally similar sentences affect the overall naturalness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0  \n",
      "  The model's response merely repeats the conversation verbatim, without transforming it into a structured clinical note as requested. It fails to provide the required concise summary and structure indicative of a clinical note format.\n",
      "\n",
      "- Completeness: 1.0  \n",
      "  The response does not sufficiently answer the instruction to summarize the conversation into a clinical note. Rather than summarizing, it reproduces the conversation in its entirety without extracting key information or providing relevant clinical insights.\n",
      "\n",
      "- Naturalness: 2.5  \n",
      "  The response sounds fluent, but it's essentially a copy of the original dialogue. While the language is grammatically correct and easy to follow, the lack of transformation into a summary or structured note detracts from its effectiveness and natural appropriation of a summary task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.8\n",
      "- Naturalness: 4.7\n",
      "\n",
      "**Explanation:**\n",
      "- **Coherence (4.5)**: The model's response is highly coherent with the input context. It accurately reflects the patient's medical history, the description of the nodular formation, and the planned course of action. The primary minor gap pertains to the geographical reference and institutional detail present in the true answer but absent in the model's response, such as the specific location and institution.\n",
      "\n",
      "- **Completeness (4.8)**: The model's response is very complete. It includes all critical aspects of the patient's history, the description of the mass, the medications taken, and the intended biopsy. It omits only minor non-clinical details like the patient's age and some specific institutional or equipment references, which do not significantly alter the core clinical narrative.\n",
      "\n",
      "- **Naturalness (4.7)**: The model's response is mostly natural, using clear, fluent, and professional language suitable for a clinical note. The vocabulary and structure are appropriate for a medical setting, mimicking human-written clinical documentation effectively. A slight deduction is made as some portions of the phrasing could sound more nuanced or tailored, but it's a solid performance overall.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response aligns well with the context provided in the input. It accurately captures the main details of the doctor-patient conversation and correctly identifies the diagnosis of cutaneous blastomycosis. However, the detail surrounding the patient's enjoyment of the outdoors and the information about her travel history is missing, which is a minor context misalignment.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response covers most of the critical points in the conversation, including the patient's medical history, the results of the biopsy, and the diagnosis. However, it misses some nuanced details like the patient's geographical background and the description of her nasal mass progression, which are present in the true answer.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The model's response is fluent and human-like, using appropriate medical terminology and maintaining a clear and concise summary style that closely resembles how a healthcare professional would potentially communicate a summary to a colleague or in documentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0  \n",
      "  The model's response logically aligns with the dialogue provided in the input. It follows the chronological progression of events as discussed by the doctor and patient.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  While the model's response covers most of the key events described in the dialogue, it lacks specific details such as the baby's initial good muscle tone, the gender, the mother's gestational diabetes, and the specific time updates for the baby’s condition.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The model's response is fluent and maintains a human-like tone throughout the summary. It clearly conveys the situation without awkward phrasing, though slightly more narrative elements or specific emotional tone might enhance the naturalness further.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0\n",
      "- Naturalness: 4.5\n",
      "\n",
      "Explanation:\n",
      "\n",
      "- **Coherence (4.5)**: The model's response aligns well with the context provided. It summarizes the main points accurately and logically follows the progression of events as reported in the dialogue. However, there is a slight incoherence in the statement \"unfortunately patient passed away,\" which seems out of context as the doctor's statement directly to the patient doesn't logically allow for the mention of the patient's passing in the summary.\n",
      "\n",
      "- **Completeness (4.0)**: The model covers most of the critical information presented in the input, including the patient's conditions, lab results, and progression of symptoms. However, it misses some specific details from the true answer, such as the treatment regimen (azathioprine and prednisolone), the use of insulin and anticoagulant drugs, and the timeline of events (e.g., hospitalization date). These omissions slightly reduce the completeness of the response.\n",
      "\n",
      "- **Naturalness (4.5)**: The response is generally fluent and easy to read. The language used is clear and concise, resembling human-written text. The ending where it states, \"unfortunately patient passed away,\" might need a smoother integration considering the narrative, but overall, it maintains a human-like tone throughout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 1.0\n",
      "- Completeness: 1.0\n",
      "- Naturalness: 2.0\n",
      "\n",
      "**Explanation**:\n",
      "- **Coherence**: The model's response is not coherent as it repeats the dialogue between the doctor and patient verbatim and does not address a specific question or provide further insight relevant to the context. It outputs a duplicated conversation rather than constructing a new, coherent response aligned with the actual situation or question.\n",
      "  \n",
      "- **Completeness**: The model's response is completely incomplete because it does not actually answer a posed question or provide any additional summary or new information; it merely replicates the prior conversation without contributing any new content or answers.\n",
      "\n",
      "- **Naturalness**: While the output retains some aspects of human-like dialogue since it repeats verbatim the given text, it lacks an appropriate response or a question's answer, affecting its fluency and purpose when assessed against natural conversation standards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response is coherent and logically aligns with the context provided in the input. It consistently reflects the sequence of events and information shared during the conversation between the doctor and the patient.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  - The model's response covers most of the details provided in the conversation and the true answer. However, it does not mention the patient's age or education level, which are included in the true answer.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  - The response is fluent and reads mostly like a human-like continuation or summary. However, it is somewhat robotic in listing details without much variation or emphasis that a human might naturally include.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5 \n",
      "  - The model's response logically aligns well with the context provided in the input. It maintains a structured and organized format similar to clinical notes, adequately capturing the sequence of events and details discussed between the doctor and patient. However, the detail about the dermatology consultation is not included, which slightly affects coherence.\n",
      "\n",
      "- Completeness: 4.0 \n",
      "  - The model's response includes most key elements of the conversation, such as the medical condition, symptoms, treatments, and follow-up instructions. However, it misses some minor specifics like the exact ethnicity and age of the child, the reason why amoxicillin/clavulanic acid was used instead of cloxacillin, and the medication details, which were in the true answer.\n",
      "\n",
      "- Naturalness: 5.0 \n",
      "  - The model's response is fluently written and mimics the style of a clinical note, maintaining a natural and professional tone. It reads smoothly and conveys information clearly, fitting the context of a medical consultation summary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0  \n",
      "  The model's response logically aligns with the input context. It accurately summarizes the patient's belief of insect infestation, his medical history, the treatment prescribed, and the outcome. \n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The model's response sufficiently answers the question, covering most of the key aspects discussed in the dialogue. However, it does not mention the patient's age or that there was no other psychopathology revealed, which is noted in the true answer.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response sounds fluent and human-like and is written in a format appropriate for a clinical note. However, the phrasing slightly lacks the nuanced detail a human expert might include in a clinical note.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response effectively captures most of the relevant information from the input dialogue. It logically aligns with the context provided, maintaining consistent details about the patient's condition, medical history, and the follow-up plan, although there are minor omissions and duplications.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  While the model includes several key details from the conversation, such as the patient's history, symptoms, and follow-up instructions, it fails to incorporate some specific aspects mentioned in the true answer, like the patient's age, nulliparity, and the underlying reasons for discontinuing hormonal management due to endometrial cancer risks.\n",
      "\n",
      "- Naturalness: 4.0  \n",
      "  The model's response is structured clearly and is easy to understand. It is written in a clinical note format which may not sound entirely natural compared to a conversation, but it remains appropriate and fluent for a medical context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model accurately reflects the sequence and details of the conversation between the doctor and the patient. Most details in the interactions, such as the treatment history and the condition of the stricture, were captured well. One potential inconsistency is the age of the patient mentioned in the model's response, which should have been 51 instead of 70.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The model's response is comprehensive and captures the essential aspects of the true answer, including treatment details and outcomes. However, it misses specific details such as the patient's age accurately and the type II diabetes condition, even though these were included in the true answer.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The response is fluent and well-structured, resembling a well-written medical summary. It provides information in a clear and professional manner, appropriate to a clinical context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model’s response is coherent and aligns well with the context provided in the input. It captures the major details of the conversation between doctor and patient, such as medical history, symptoms, and treatment plan. However, it includes repetitions and restructures the narrative into a monologue, which slightly affects its coherence in the multi-turn dialogue context.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response covers the significant details about the patient’s condition, medical history, and upcoming treatment. It provides a thorough overview of the patient’s symptoms, conditions, and medications. However, it omits the details regarding the scan results, differential diagnoses, and the histopathological examination plan, leaving out some elements present in the true answer.\n",
      "\n",
      "- Naturalness: 3.5  \n",
      "  The model’s response is fluent and structurally coherent but lacks some natural conversational elements due to its summary-like, monologue form. In the context of a patient-doctor interaction, it appears less dynamic and more factual, losing some of the human-like interaction present in the dialogue format of the input and true answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It accurately reflects the conversation between the doctor and the patient, ensuring that the key details from the context are maintained.\n",
      "  \n",
      "- Completeness: 4.8  \n",
      "  The model's response sufficiently answers the main aspects of the situation described in the input, covering the diagnosis, the medications, and the upcoming procedures. While the model captures the critical elements, it doesn't mention the hospital name or the patient's age, which were in the true answer.\n",
      "  \n",
      "- Naturalness: 4.7  \n",
      "  The model response is fluent and human-like, mimicking the style of a patient summarizing their own medical situation to another person. It maintains a high degree of naturalness, though its structure is slightly more formal than typical patient speech.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  The model's response is mostly coherent with the context provided, capturing key points such as the patient's history of chronic alcoholic pancreatitis, symptoms, and various examinations. However, there are minor discrepancies and omissions, such as incorrect mention of \"night sweats\" which the patient denied having in the input context.\n",
      "  \n",
      "- Completeness: 4.5  \n",
      "  The response is largely complete, as it includes most critical information like symptoms, medical history, laboratory and sonographic results, and the plan for transfer to a tertiary care center. The only slight deficiency in completeness is the omission of some specific numeric values, such as precise laboratory findings and blood pressure measures.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The clinical note format of the model's response is appropriately structured in a professional and human-like manner, fitting the expected style for medical documentation. It reads coherently and fluently for its intended purpose, though it could include slightly more variability or expressive detail to fully mirror human writing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response accurately aligns with the context provided, retaining relevant details such as BMI, emotional eating, family history, and physical activity. However, there is a minor inconsistency regarding the patient’s age as mentioned in the summary (\"13-year-old female\") versus the input context (\"9-year-old girl\").\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response captures most of the critical details necessary to summarize the case despite minor omissions such as specific details related to school physical education, screen time averages, water intake, and the precise birth weight and menarche status. However, it effectively communicates the essential elements of the patient’s health profile and management plan.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response is fluent and presented in a structured, human-like manner that is easily comprehensible. The language and tone used fit well with professional medical reporting, though occasional minor redundancies slightly impact its perfect naturalness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  The model's response aligns well with the context, mentioning key elements such as the patient's conditions and the discussion about nursing intensity and reimbursement. However, it doesn't capture the nuanced comparison between two cases as hinted in the true answer.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The response covers most of the essential information provided in the dialogue, such as the patient's conditions and the need for appropriate reimbursement and nursing intensity. It misses a deeper exploration of how current DRG systems compare cases and some finer details present in the true answer.\n",
      "\n",
      "- Naturalness: 3.0  \n",
      "  The response is factual and straightforward but lacks a human-like fluency. It reads more like a series of notes than a naturally flowing summary or conversation, which detracts from the naturalness of the communication.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       4.183333\n",
      "Completeness    3.830000\n",
      "Naturalness     4.303333\n",
      "BLEURT          0.589861\n",
      "BERTScore_F1    0.651869\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summarization\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "summarization = df[df['task'] == 'summarization']\n",
    "results = []\n",
    "\n",
    "for _, row in summarization.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is logically aligned with the context provided. It addresses the symptoms and the situation, suggesting that they could be related to the diabetes and poorly controlled blood sugar levels. However, the response could be slightly more specific in connecting each symptom to potential causes or concerns related to diabetes.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response sufficiently answers the query by explaining the likely connection between the symptoms and diabetes, and recommending actions such as consulting a healthcare provider for a comprehensive evaluation and management plan. However, it does not provide a detailed treatment plan or specific guidance on managing the current symptoms, which was part of the request.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is written in fluent and human-like language. It uses appropriate medical terminology while remaining accessible to a layperson. There are no awkward phrases or unnatural expressions, and the response feels like it could be given by a human healthcare professional.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context by mentioning the importance of keeping the foreskin clean to avoid infections. It acknowledges the practice of shaking after urination and suggests a more general cleaning routine, which is coherent with maintaining hygiene.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response provides a general recommendation about hygiene but lacks some specific details mentioned in the true answer, such as the role of smegma and specific conditions like diabetes that may increase infection risk. It doesn't explicitly state that pulling back the foreskin every time is unnecessary.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response is fluent and human-like, with clear and natural language. It communicates the hygiene advice in an approachable manner, although slightly more elaboration could enhance its naturalness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0  \n",
      "  The model's response does not align well with the context. It suggests the presence of an oral torus, which is a bony growth usually associated with the alveolar ridge, not the tongue or its described location. This recommendation does not fit the symptoms described in the input.\n",
      "\n",
      "- Completeness: 1.5  \n",
      "  The response fails to sufficiently answer the question as it misidentifies the condition entirely and does not consider the possibility of common tongue-related conditions such as inflamed taste buds, herpes, canker sores, or infections, as mentioned in the true answer.\n",
      "\n",
      "- Naturalness: 4.0  \n",
      "  Despite its inaccuracies, the response is fluently written and sounds human-like. The language used is natural, clear, and easy to understand.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model’s response is coherent and logically aligns with the context provided in the input. It correctly recognizes key aspects such as the baby's heartbeat, cephalic presentation, low AFI, and the general guidelines around inducing labor.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response provides a general stance on inducing labor at 38 weeks, referencing professional guidelines and conditions under which induction might be considered. However, it lacks mention of specific criteria for induction and does not address the importance of monitoring through CTG as outlined in the true answer.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The model's response is fluent and human-like, making effective use of language that is reassuring and informative. It sounds like advice one might receive from a healthcare professional, striking a balance between acknowledging the user's situation and suggesting consultation with a healthcare provider for personalized advice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  *Evaluation*: The model's response logically aligns with the symptoms of flashes or lightning in the eye and provides an explanation related to diabetic retinopathy, which is a plausible cause considering the user's medical conditions (diabetes).\n",
      "\n",
      "- Completeness: 3.0  \n",
      "  *Evaluation*: While the response is well-detailed in terms of diabetic retinopathy, it does not fully address other potential causes of flashes at the user's age, such as posterior vitreous detachment, as highlighted in the true answer. It lacks a comprehensive exploration of all possible explanations and subsequent actions or treatments directly relating to the user's situation.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  *Evaluation*: The language used in the response is fluent, clear, and human-like. It conveys the information in a manner that is easily understandable to a layperson while using some technical terms appropriately for the context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "   The model's response logically aligns with the context provided in the input. It addresses the key issues of depression and heart pain, acknowledges the coping mechanism of masturbation, and suggests practical steps such as seeking help from a mental health professional. However, a minor deduction is noted for not directly addressing the familial and romantic relationship pressures mentioned, which could also be contributing to the depressive symptoms.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "   The response sufficiently answers the question by recommending consultation with a mental health professional and addressing concerns about masturbation and heart pain. However, it could enhance completeness by more precisely addressing the multifaceted sources of stress and depression such as family pressures and romantic failures, which were highlighted in the input.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "   The response is fluent and human-like, showing empathy and providing understandable guidance. The language is professional yet accessible, which suits the context of a doctor-patient interaction well. Some terms could be softened for a more conversational tone, but generally, the response is natural and appropriate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response logically aligns with the context by suggesting a consultation with a gynecologist for further evaluation, which is appropriate given the symptoms described. However, it doesn't explicitly address the question about hormonal imbalance.\n",
      "  \n",
      "- Completeness: 3.0\n",
      "  - The response suggests seeing a gynecologist, which is a good first step, but it lacks depth in directly addressing whether the issue could be due to hormonal imbalance. The true answer provides more specific medical advice and considerations that the model's response misses.\n",
      "  \n",
      "- Naturalness: 5.0\n",
      "  - The response is fluent and sounds human-like. The language used is clear, professional, and reflects how a healthcare professional might communicate with a patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 2.0  \n",
      "  The model's response is somewhat coherent, as it recognizes that a dermatologist would be the appropriate specialist to address hair loss concerns. However, it fails to address or align with the more detailed context provided in the user's input, such as the relationships between medication, hair loss, and other health concerns raised.\n",
      "\n",
      "- Completeness: 1.0  \n",
      "  The response does not sufficiently answer the question asked by the user. The input includes detailed medical history and specific questions about medication impacts, potential solutions, and the effectiveness of treatments, all of which are ignored in the response. \n",
      "\n",
      "- Naturalness: 3.0  \n",
      "  The response is somewhat fluent and sounds like it could be part of a human conversation. However, it is very brief and lacks engagement with the user's detailed concerns, which affects how natural it feels in the context of a more substantive interaction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response generally aligns logically with the context of the input, addressing concerns about natural remedies and providing information about CAT scans. However, it lacks some critical points regarding the necessity of surgery for a 2-cm kidney stone, which could slightly affect coherence with the true medical situation.\n",
      "  \n",
      "- Completeness: 3.5\n",
      "  - The model addresses several aspects of the user's query, such as natural remedies, potential interactions with medications, and the nature of CT scans, but it fails to adequately address the necessity of surgery for a stone of this size and the type of anesthesia typically used, as highlighted in the true answer.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "  - The response is fluent and sounds human-like. It presents information in a reassuring and friendly manner, which is appropriate for a healthcare discussion. It reads like advice from a caring provider, but the slight lack of critical details affects its overall thoroughness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context by explaining the significance of the test results and defining \"hypoplastic right cavernosal artery\" in terms related to ED. It connects the clinical findings to the symptom of erectile dysfunction reasonably well.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response adequately explains the meaning of the doppler test's findings and suggests discussing results with a urologist, which is a reasonable next step. However, it lacks inquiries about additional factors or medical history that might affect ED, as present in the reference answer.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response is fluent, clear, and reads naturally as if a human provided the explanation. It uses appropriate medical terminology while remaining accessible and understandable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  The model's response aligns with the context provided, mentioning both the benefits and potential risks of the ketogenic diet, but it includes a few incoherent points, such as stating weight loss as a negative effect.\n",
      "  \n",
      "- Completeness: 3.5  \n",
      "  The response addresses the long-term concerns but lacks depth in explaining how ketosis could affect a diabetic or normal person. It briefly mentions increased risk for certain diseases without sufficient elaboration.\n",
      "  \n",
      "- Naturalness: 4.0  \n",
      "  The response is mostly fluent and human-like but occasionally includes awkward phrasing, such as \"self keto diet experience\" and an unclear context regarding \"weight loss\" as a negative effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response is mostly coherent as it provides information related to Argyreia speciosa and its traditional use for conditions such as gonorrhea and syphilis. However, it does not provide the specific details on the traditional preparation techniques mentioned in the input, which detracts slightly from coherence.\n",
      "\n",
      "- Completeness: 3.0\n",
      "\n",
      "The response does not fully address the request for traditional preparation techniques specific to Argyreia speciosa for treating gonorrhea and syphilis. While it mentions the plant's general traditional use, it focuses more on the necessity of consulting healthcare for these conditions, rather than detailing the traditional methods.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The response is fluently written and sounds human-like. It effectively conveys the information in a clear and natural manner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It identifies possible causes of both the cough and the gastrointestinal symptoms, and it recognizes the impact of poor air quality, which is relevant given the user's location. However, it doesn't consider the specific detail of previously prescribed medications.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  While the model suggests seeing specialists and considers poor air quality, it doesn't provide a direct treatment plan or address the ineffectiveness of the current prescriptions, which the user specifically mentions. It misses the opportunity to suggest alternative medications or further clarification on the given medications.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The model's response is fluent and structured in a professional tone that mirrors how a human doctor might communicate. It is clear and empathetic, with a natural flow to the advice provided. Slightly more personalization or details in the response could enhance its human-like quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It acknowledges the individual's concerns about their immune system and suggests self-isolation as a precaution, which is a reasonable interpretation.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response provides a complete answer concerning the need for precautions such as self-isolation and hygiene practices. However, it does not clarify that Hashimoto's disease specifically may not increase the risk of viral infections, which is mentioned in the true answer.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent and natural, resembling a human-like communication style effectively. It uses appropriate language and structure that is easy to understand.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It addresses stage IV endometrial cancer with lung metastasis and mentions the standard treatment possibilities, which are coherent with an advanced cancer stage context. However, it misses specific considerations like the exclusion of surgery or particular drugs, which slightly affects coherence with the fine-grained details of the true answer.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The model provides a general overview of treatment options, including surgery, chemotherapy, and radiation therapy. However, it lacks depth in the specific treatment recommendations for stage IV cancer as described in the true answer. It doesn't mention factors that might influence chemotherapy choices, such as the patient's age or comorbid conditions.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The model's response is fluent and sounds human-like. It is clear and professionally presented, similar to the way a healthcare provider might explain treatment options to a patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.5\n",
      "\n",
      "The model's response is mostly coherent with the input provided. It logically discusses cubital tunnel syndrome as a potential diagnosis that aligns with the symptoms described and recognizes the possibility that the issue may not be stemming from the neck, given the normal MRI results. \n",
      "\n",
      "In terms of completeness, the model provides a reasonable answer by suggesting physical therapy or surgery as potential treatment options and recommends seeking a second opinion, which closely mirrors typical medical advice. However, it misses some additional possible causes and interventions mentioned in the true answer, such as lifestyle changes and further tests that could be conducted.\n",
      "\n",
      "The response sounds natural and fluent, closely resembling human-like communication. It uses appropriate medical terminology, explains the condition clearly, and offers practical advice in a manner that could easily be understood by a layperson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5 \n",
      "    - The model's response logically aligns with the context provided. It addresses the concern about excessive foot sweating and odor by suggesting common causes and advising consulting a healthcare provider, which is coherent with the input's context.\n",
      "\n",
      "- Completeness: 4.0\n",
      "    - While the model's response addresses the main concern and provides advice including consulting healthcare professionals and trying over-the-counter products, it does not mention some specific underlying causes like those listed in the true answer, such as diabetes or other medical conditions that could also be relevant. However, it does cover a reasonable scope of possible causes and recommendations.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "    - The response sounds fluent and human-like, offering structured advice and recommendations in a conversational tone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.5  \n",
      "  The model's response is coherent in terms of providing potential causes for the symptoms described, such as varicocele and epididymitis. It aligns logically with the patient's context of having a dull ache and an enlarged vein. However, it introduces multiple potential issues without clearly prioritizing or contextualizing them based on the patient’s specific symptoms.\n",
      "\n",
      "- Completeness: 3.0  \n",
      "  While the response offers several possible explanations for the symptoms and suggests seeing a urologist, it does not address all aspects of the patient's situation. Specifically, it does not inquire further into associated symptoms or lifestyle factors that could better inform a recommendation, nor does it explain why a hernia, as mentioned in the true answer, might be considered.\n",
      "\n",
      "- Naturalness: 4.0  \n",
      "  The response is generally fluent and human-like, using clear and understandable language. However, there is some repetition towards the end regarding seeing a urologist and a history of testicular cancer, which detracts from the natural flow of the response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context of the input, as it correctly addresses the user's query about air travel during pregnancy with considerations for thyroid issues.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  While the model provides several helpful tips for comfortable air travel, it does not address all essential considerations, such as ensuring it's not a high-risk pregnancy and traveling with necessary medical documents and a fit-to-fly certificate, which are mentioned in the true answer. The model also lacks advice on specific pregnancy-related precautions beyond the general travel tips.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The response is fluent and human-like, structured well with clear, numbered tips. It reads naturally, although it might benefit from a slightly more empathetic tone given the context of a pregnancy inquiry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent and logically aligns with the context of a brain hemorrhage and the situation described. It appropriately discusses the seriousness of the condition, common treatments, and the importance of medical monitoring, which is relevant to the details provided in the input.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The model's response provides a general overview of what a brain hemorrhage involves and the potential treatments. However, it lacks specific details present in the true answer, such as the location of the bleed and potential long-term effects, which would have offered a more comprehensive response. Also, it does not address the uncertainty of recovery or mention possible complications from co-morbid conditions.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response is fluent and sounds human-like. It uses appropriate medical terminology and constructs sentences in a way that feels natural and empathetic. The language employed is professional yet clear, which is fitting for a doctor's response to a concerned family member.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model provides a coherent explanation supporting morning exercise, aligning with common advice about the benefits of morning workouts. However, it doesn't fully align with the detailed physiological considerations mentioned in the true answer.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The model gives a sufficient answer focusing on morning exercise benefits but misses discussing the body's core temperature and hormone levels, which are relevant to a person with diabetes as outlined in the true answer. It also briefly mentions avoiding exercise close to bedtime but doesn't specifically address the option of working out before dinner.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is very fluent and human-like. It clearly conveys reasons for exercising in the morning with a conversational tone, making it sound natural and engaging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.0\n",
      "\n",
      "The response logically aligns with the context to a certain degree by suggesting consulting a doctor, which is a typical recommendation for medical concerns. However, it lacks specific reference to the user's symptoms or the previous consultation with a doctor, which diminishes its alignment with the context provided.\n",
      "\n",
      "- Completeness: 1.0\n",
      "\n",
      "The model's response does not sufficiently answer the specific question asked. The user was seeking information on the cause of the ringing noise and how long it might take to resolve, but the model only provides a generic suggestion without addressing these specific concerns.\n",
      "\n",
      "- Naturalness: 4.0\n",
      "\n",
      "The response is fluently written and sounds human-like. The suggestion to consult a doctor is articulated in a typical conversational manner, though it is somewhat short and lacks depth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "\n",
      "The model's response is coherent and logically aligns with the context provided. It correctly addresses the management of a burst sebaceous cyst with dressing and antibiotics. However, the suggestion to consult a dermatologist misses the specific context regarding the user's reluctance to undergo surgery, which is slightly less aligned with the user's specific concerns.\n",
      "\n",
      "- Completeness: 3.0\n",
      "\n",
      "The response partially answers the question by discussing the dressing and the potential need for surgical removal. However, it lacks detailed information on further steps, such as continuing antibiotics or addressing potential complications like infection or diabetes. It misses offering specific care instructions or prescribing medication, unlike the true answer.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "\n",
      "The response sounds fluent and human-like, maintaining a professional tone suitable for medical advice. It offers clear and concise guidance, making it sound natural for a medical professional's response, although it could provide more empathetic support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- **Coherence: 4.5**  \n",
      "  The model's response logically aligns with the context provided in the input. It addresses the main question about insulin treatment selection and additional related questions, providing explanations for each point raised in the input. However, there are minor discrepancies such as differing emphases on the use of insulin pumps.\n",
      "\n",
      "- **Completeness: 4.0**  \n",
      "  The model sufficiently answers most questions raised in the input but lacks the depth in certain areas compared to the true answer. For instance, it does not mention economic considerations or the current real-world usage of inhaled insulin from the perspective of practice, and it oversimplifies when saying islet cells are transplanted into the liver. It also misses detailed specifics like the necessity of subcutaneous insulin for inhaled insulin.\n",
      "\n",
      "- **Naturalness: 4.8**  \n",
      "  The response is fluent and human-like, with professional and approachable language appropriate for a medical context. It provides clear, informative answers without overly technical language, though small improvements could be made for more nuanced understanding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response logically aligns with the context by addressing potential causes of dry skin in a diabetic patient and suggesting that there might be a link between diabetes and the skin issue. However, it lacks a direct connection to the cat scratch incident, which could be relevant to the problem.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  - The response provides a comprehensive view of how diabetes could be causing the dry skin, but it misses the connection to the skin issue mentioned with the cat scratch, which might be an infection or allergic reaction that needs attention.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  - The response is fluent, professional, and maintains a human-like tone, suggesting actionable steps such as consulting a doctor for further treatment options.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context, as it addresses the query about vacuolated neutrophils and mentions potential conditions other than infection. However, it does not mention the timeframe context of the patient's symptoms not worsening for a week.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response partly answers the question by providing other potential causes for high vacuolated neutrophils and suggests further investigation. Nonetheless, it lacks specific examples of infections where the patient's symptoms might not visibly worsen for a week and does not address whether it's possible for an infection to not show symptoms immediately.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The response is fluent and structured in a human-like manner. It communicates the information clearly and professionally, appropriate for a medical context, though the addition of more examples could improve the perception of expertise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.5  \n",
      "  The response generally aligns with the context provided but introduces the concept of muscle damage, which is not directly related to M2-PK in the context of cancer screening. It partially addresses the concern about cancer but lacks clarity regarding the wider implications of M2-PK levels in relation to other conditions.\n",
      "\n",
      "- Completeness: 3.0  \n",
      "  The response touches on the fact that elevated M2-PK levels don't necessarily indicate cancer by mentioning other possible non-cancerous reasons for elevation. However, it doesn't provide the same level of detail or suggestions for further testing and differential diagnosis as the true answer does, such as fecal tests or colonoscopy.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The response is fluent and sounds human-like, using a conversational tone that is easy to understand. It reassures the user in a manner similar to a face-to-face interaction, which contributes to its naturalness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response is logically aligned with the context provided. It correctly identifies symptoms associated with insufficient thyroid hormone production after partial thyroid removal despite normal blood test results, which are mentioned in the input.\n",
      "\n",
      "- Completeness: 3.0\n",
      "  - The model partially answers the question by discussing the general role of the thyroid gland and the potential need for hormone replacement. However, it does not consider other possible causes of symptoms like vitamin D or calcium deficiency, as mentioned in the true answer. It also doesn’t provide specific further testing recommendations.\n",
      "\n",
      "- Naturalness: 4.8\n",
      "  - The response is fluent and human-like. The language is clear and easy to understand, with a tone that would be expected in a general informational setting. The model uses phrases like \"I'm not a doctor, but I can offer some general information,\" which adds to the naturalness of the response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  The model's response logically aligns with the context provided by acknowledging the issue with the medications and offers suggestions that are coherent, such as increasing the dosage, switching medication, or seeking a second opinion. However, it did not directly address the specific question about why the patient is taking Olanzapine when dealing with anxiety only.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response provides several suggestions that are generally helpful, such as increasing dosage, switching medications, and combining with other therapies. However, it lacks specificity about the appropriateness of Olanzapine for anxiety, which the true answer highlights as unnecessary. This lack of advice on Olanzapine makes the response less complete.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The model's response is fluent, empathetic, and human-like. It uses polite language and structured suggestions, making it sound professional and natural. A slight deduction is given for not directly addressing some specific nuances raised in the input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response aligns well with the context provided. It acknowledges the user's problem and provides relevant general advice regarding potential causes for hair loss, matching the likelihood discussed in the true answer.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response provides a good overview of potential causes and suggests seeing a healthcare professional for a more comprehensive assessment. However, it lacks specific details and actionable steps given in the true answer, such as specific diagnoses or treatments and lifestyle adjustments.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response is fluent and human-like, maintaining a professional tone that is typical in medical advice, which is consistent with how a healthcare professional might communicate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       4.133333\n",
      "Completeness    3.283333\n",
      "Naturalness     4.570000\n",
      "BLEURT          0.632200\n",
      "BERTScore_F1    0.464410\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# generation\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "generation = df[df['task'] == 'generation']\n",
    "results = []\n",
    "\n",
    "for _, row in generation.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column BLEURT not found in DataFrame. Skipping normalization.\n",
      "Warning: Column BERTScore_F1 not found in DataFrame. Skipping normalization.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Coherence', 'Completeness', 'Naturalness', 'BLEURT', 'BERTScore_F1'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m normalize_scores(evaluation_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEURT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m normalize_scores(evaluation_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore_F1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m average_scores \u001b[38;5;241m=\u001b[39m evaluation_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleteness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaturalness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEURT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore_F1\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m평균 점수:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Coherence', 'Completeness', 'Naturalness', 'BLEURT', 'BERTScore_F1'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# daily_diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "daily = df[df['task'] == 'daily_diets']\n",
    "results = []\n",
    "\n",
    "for i, row in daily.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_outpu_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Evaluation {i+1} result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       3.223577\n",
      "Completeness    2.880081\n",
      "Naturalness     3.008130\n",
      "BLEURT          0.488236\n",
      "BERTScore_F1    0.627131\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# alternative_diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "alternative = df[df['task'] == 'alternative_diet']\n",
    "results = []\n",
    "\n",
    "for _, row in alternative.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Evaluation {i+1} result:\\n{evaluation}\")\n",
    "            \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>cook_time</th>\n",
       "      <th>servings</th>\n",
       "      <th>steps</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition_facts</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark</td>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark: Dive into ...</td>\n",
       "      <td>10 min</td>\n",
       "      <td>4 hr</td>\n",
       "      <td>6 Servings</td>\n",
       "      <td>['Cover a freezer-safe tray with parchment pap...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...</td>\n",
       "      <td>{'Servings': '6 Servings', 'Serving Size': '1 ...</td>\n",
       "      <td>[{'label': 'Plain Nonfat Greek yogurt', 'us_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple-Pumpkin Spice Oatmeal Cookies</td>\n",
       "      <td>Description not found</td>\n",
       "      <td>10 min</td>\n",
       "      <td>25 min</td>\n",
       "      <td>14 Servings</td>\n",
       "      <td>['Preheat the oven to 350 degrees F. Line two ...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...</td>\n",
       "      <td>{'Servings': '14 Servings', 'Serving Size': '1...</td>\n",
       "      <td>[{'label': 'old-fashioned rolled oats', 'us_me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0   Raspberry Swirl Frozen Yogurt Bark   \n",
       "1  Maple-Pumpkin Spice Oatmeal Cookies   \n",
       "\n",
       "                                         description prep_time cook_time  \\\n",
       "0  Raspberry Swirl Frozen Yogurt Bark: Dive into ...    10 min      4 hr   \n",
       "1                              Description not found    10 min    25 min   \n",
       "\n",
       "      servings                                              steps  \\\n",
       "0   6 Servings  ['Cover a freezer-safe tray with parchment pap...   \n",
       "1  14 Servings  ['Preheat the oven to 350 degrees F. Line two ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...   \n",
       "1  ['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...   \n",
       "\n",
       "                                     nutrition_facts  \\\n",
       "0  {'Servings': '6 Servings', 'Serving Size': '1 ...   \n",
       "1  {'Servings': '14 Servings', 'Serving Size': '1...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [{'label': 'Plain Nonfat Greek yogurt', 'us_me...  \n",
       "1  [{'label': 'old-fashioned rolled oats', 'us_me...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfh = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data2_daily_diets/diabetes_food_hub_new_nutri_facts.csv\")\n",
    "dfh.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:00<00:00, 22063.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for Each Row ===\n",
      "Row Index: 661\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 662\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 663\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 664\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 665\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 666\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 667\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 668\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 669\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 670\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 671\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 672\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 673\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 674\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 675\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 676\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 677\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 678\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 679\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 680\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 681\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 682\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 683\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 684\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 685\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 686\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 687\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 688\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 689\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 690\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 691\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 692\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 693\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 694\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 695\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 696\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 697\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 698\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 699\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 700\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 701\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 702\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 703\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 704\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 705\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 706\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 707\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 708\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 709\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 710\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 711\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 712\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 713\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 714\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 715\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 716\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 717\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 718\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 719\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 720\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 721\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 722\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 723\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 724\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 725\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 726\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 727\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 728\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 729\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 730\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 731\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 732\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 733\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 734\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 735\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 736\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 737\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 738\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 739\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 740\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 741\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 742\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 743\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 744\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 745\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 746\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 747\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 748\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 749\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 750\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 751\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 752\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 753\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 754\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 755\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 756\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 757\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 758\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 759\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 760\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 761\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 762\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 763\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 764\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 765\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 766\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 767\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 768\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 769\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 770\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 771\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 772\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 773\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 774\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 775\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 776\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 777\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 778\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 779\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 780\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 781\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 782\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 783\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 784\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 785\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 786\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 787\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 788\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 789\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 790\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 791\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 792\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 793\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 794\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 795\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 796\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 797\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 798\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 799\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 800\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 801\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 802\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 803\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 804\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 805\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 806\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 807\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 808\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 809\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 810\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 811\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 812\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 813\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 814\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 815\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 816\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 817\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 818\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 819\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 820\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 821\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 822\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 823\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 824\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 825\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 826\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 827\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 828\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 829\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 830\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 831\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 832\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 833\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 834\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 835\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 836\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 837\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 838\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 839\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 840\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 841\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 842\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 843\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 844\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 845\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 846\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 847\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 848\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 849\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 850\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 851\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 852\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 853\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 854\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 855\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 856\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 857\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 858\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 859\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 860\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 861\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 862\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 863\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 864\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 865\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 866\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 867\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 868\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 869\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 870\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 871\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 872\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 873\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 874\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 875\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 876\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 877\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 878\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 879\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 880\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 881\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 882\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 883\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 884\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 885\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 886\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 887\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 888\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 889\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 890\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 891\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 892\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 893\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 894\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 895\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 896\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 897\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 898\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 899\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 900\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 901\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 902\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 903\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 904\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 905\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 906\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 907\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 908\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 909\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 910\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 911\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 912\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 913\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 914\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 915\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 916\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 917\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 918\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 919\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 920\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 921\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 922\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 923\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 924\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 925\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 926\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 927\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 928\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 929\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 930\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 931\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 932\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 933\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 934\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "=== Overall Averages ===\n",
      "Output Average Nutri-Score: None\n",
      "Model Output Average Nutri-Score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# daily diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return all(key in data for key in ['Breakfast', 'Lunch', 'Dinner'])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # 과일/채소 비율을 100g 기준으로 변환\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data) if isinstance(data, (int, float, str)) else default\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # 전체 무게 계산\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g 기준으로 성분 정규화\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_meal_nutri_score(meal_data, dfh):\n",
    "    meal_scores = {}\n",
    "\n",
    "    for meal, title in meal_data.items():\n",
    "        matched_row = find_most_similar_row(title, dfh)\n",
    "        if matched_row is None:\n",
    "            continue\n",
    "\n",
    "        nutrition_facts = matched_row['nutrition_facts']\n",
    "        ingredients = matched_row['ingredients']\n",
    "        score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "\n",
    "        if score is None:\n",
    "            print(f\"Warning: Nutri-Score calculation failed for meal '{meal}' with title '{title}'.\")\n",
    "            grade = \"N/A\"\n",
    "        else:\n",
    "            grade = get_nutri_score_grade(score)\n",
    "\n",
    "        meal_scores[meal] = {'score': score, 'grade': grade}\n",
    "\n",
    "    return meal_scores\n",
    "\n",
    "def calculate_scores_with_comparison(df, dfh):\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        output_scores = {}\n",
    "        model_scores = {}\n",
    "        if is_valid_meal_structure(row.get('output', '')):\n",
    "            output_data = json.loads(row['output'])\n",
    "            output_scores = calculate_meal_nutri_score(output_data, dfh)\n",
    "        if is_valid_meal_structure(row.get('model_output_512', '')):\n",
    "            model_data = json.loads(row['model_output_512'])\n",
    "            model_scores = calculate_meal_nutri_score(model_data, dfh)\n",
    "        results.append({'row_index': idx, 'output_scores': output_scores, 'model_scores': model_scores})\n",
    "    return results\n",
    "\n",
    "def calculate_average_scores(results):\n",
    "    \"\"\"\n",
    "    Calculate the average Nutri-Scores for outputs and model outputs.\n",
    "    \"\"\"\n",
    "    output_total_score = 0\n",
    "    model_total_score = 0\n",
    "    output_count = 0\n",
    "    model_count = 0\n",
    "\n",
    "    for result in results:\n",
    "        # Extract output scores\n",
    "        for meal, score_data in result['output_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                output_total_score += score_data['score']\n",
    "                output_count += 1\n",
    "\n",
    "        # Extract model scores\n",
    "        for meal, score_data in result['model_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                model_total_score += score_data['score']\n",
    "                model_count += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    output_avg = output_total_score / output_count if output_count > 0 else None\n",
    "    model_avg = model_total_score / model_count if model_count > 0 else None\n",
    "\n",
    "    return output_avg, model_avg\n",
    "\n",
    "\n",
    "# 'daily_diets' task Nutri-Score calculation\n",
    "filtered_df = df[df['task'] == 'daily_diets']\n",
    "results = calculate_scores_with_comparison(filtered_df, dfh)\n",
    "\n",
    "# Calculate overall averages\n",
    "output_avg, model_avg = calculate_average_scores(results)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Results for Each Row ===\")\n",
    "for result in results:\n",
    "    print(f\"Row Index: {result['row_index']}\")\n",
    "    print(f\"Output Scores: {result['output_scores']}\")\n",
    "    print(f\"Model Output Scores: {result['model_scores']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Overall Averages ===\")\n",
    "print(f\"Output Average Nutri-Score: {output_avg}\")\n",
    "print(f\"Model Output Average Nutri-Score: {model_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 158/246 [03:34<01:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error identifying fruits and vegetables: unterminated string literal (detected at line 1) (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 207/246 [04:37<00:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Total weight is zero. Skipping calculation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [05:27<00:00,  1.33s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 216\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Execution\u001b[39;00m\n\u001b[1;32m    215\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malternative_diet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_scores_with_comparison_no_meals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[32], line 207\u001b[0m, in \u001b[0;36mcalculate_scores_with_comparison_no_meals\u001b[0;34m(df, dfh)\u001b[0m\n\u001b[1;32m    200\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    201\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_index\u001b[39m\u001b[38;5;124m'\u001b[39m: idx,\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         })\n\u001b[1;32m    206\u001b[0m final_output_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(output_scores_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(output_scores_list) \u001b[38;5;28;01mif\u001b[39;00m output_scores_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m final_model_output_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_scores_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_output_scores_list) \u001b[38;5;28;01mif\u001b[39;00m model_output_scores_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Average Nutri-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_output_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Output Average Nutri-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_output_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# alternative diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return isinstance(data, dict)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # 과일/채소 비율을 100g 기준으로 변환\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # 전체 무게 계산\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g 기준으로 성분 정규화\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_scores_with_comparison_no_meals(df, dfh):\n",
    "    results = []\n",
    "    output_scores_list = []\n",
    "    model_output_scores_list = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            output_text = row.get('output', '')\n",
    "            if output_text:\n",
    "                matched_row = find_most_similar_row(output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    output_scores_list.append(output_score)\n",
    "                else:\n",
    "                    output_score = None\n",
    "\n",
    "            model_output_text = row.get('model_output_512', '')\n",
    "            if model_output_text:\n",
    "                matched_row = find_most_similar_row(model_output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    model_output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    model_output_scores_list.append(model_output_score)\n",
    "                else:\n",
    "                    model_output_score = None\n",
    "\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': output_score,\n",
    "                'model_output_score': model_output_score\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': None,\n",
    "                'model_output_score': None\n",
    "            })\n",
    "\n",
    "    final_output_avg = sum(output_scores_list) / len(output_scores_list) if output_scores_list else None\n",
    "    final_model_output_avg = sum(model_output_scores_list) / len(model_output_scores_list) if model_output_scores_list else None\n",
    "\n",
    "    print(f\"Output Average Nutri-Score: {final_output_avg}\")\n",
    "    print(f\"Model Output Average Nutri-Score: {final_model_output_avg}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Execution\n",
    "filtered_df = df[df['task'] == 'alternative_diet']\n",
    "results = calculate_scores_with_comparison_no_meals(filtered_df, dfh)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
