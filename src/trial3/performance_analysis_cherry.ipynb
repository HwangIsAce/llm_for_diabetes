{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# \n",
    "file_paths = [\n",
    "    \"/data/jaesung/llm_for_diabetes/src/trial3/model_response/model_output_cherry.jsonl\",\n",
    "    # \"/data/jaesung/llm_for_diabetes/src/trial/CoT_collection/model_response/test_1.jsonl\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "# medqa\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa1 = df[df['task'] == 'qa1']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa1)\n",
    "\n",
    "for _, row in qa1.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# medmcqa\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa2 = df[df['task'] == 'qa2']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa2)\n",
    "\n",
    "for _, row in qa2.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# pubmedqa\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "qa3 = df[df['task'] == 'qa3']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(qa3)\n",
    "\n",
    "for _, row in qa3.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Accuracy: 0.3667\n"
     ]
    }
   ],
   "source": [
    "# bionli\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_answer_correctness(true_answer, model_answer):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_answer와 model_answer가 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated medical answer.\n",
    "    Compare the following two answers and determine if they mean the same thing.\n",
    "\n",
    "    **Correct Answer**: {true_answer}\n",
    "    **Model Output**: {model_answer}\n",
    "\n",
    "    If the model output correctly conveys the same answer as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "nli = df[df['task'] == 'nli']\n",
    "\n",
    "# 정답 판별 수행\n",
    "correct_count = 0\n",
    "total_count = len(nli)\n",
    "\n",
    "for _, row in nli.iterrows():\n",
    "    true_answer = row['output'].strip()\n",
    "    model_answer = row['model_output_128'].strip()\n",
    "    is_correct = check_answer_correctness(true_answer, model_answer)\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "\n",
    "# Accuracy 계산\n",
    "accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"GPT-based Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-based Total Precision: 0.2000\n",
      "GPT-based Total Recall: 0.2000\n",
      "GPT-based Total F1-Score: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# ddi\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드 (API 키 설정)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def check_relation_correctness(true_relation, model_relation):\n",
    "    \"\"\"\n",
    "    GPT-3.5-turbo를 사용하여 true_relation과 model_relation이 같은 의미인지 판별\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the correctness of an AI-generated relationship extraction model.\n",
    "    Compare the following two sets of extracted relationships and determine if they contain the same meaning.\n",
    "\n",
    "    **Correct Relationships**: {true_relation}\n",
    "    **Model Predicted Relationships**: {model_relation}\n",
    "\n",
    "    If the model output correctly conveys the same relationships as the correct answer, respond only with \"YES\".\n",
    "    If the model output is incorrect or has a different meaning, respond only with \"NO\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return result == \"YES\"  # GPT 응답이 YES면 정답 처리\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT evaluation: {e}\")\n",
    "        return False  # 오류 발생 시 오답 처리\n",
    "\n",
    "# 데이터 로드 및 필터링\n",
    "re_df = df[df['task'] == 're']\n",
    "\n",
    "# 정답 판별 수행 및 메트릭 계산\n",
    "total_true_positive = 0\n",
    "total_false_positive = 0\n",
    "total_false_negative = 0\n",
    "\n",
    "for _, row in re_df.iterrows():\n",
    "    true_relations = set(row['output'].split(', '))\n",
    "    model_relations = set(row['model_output_128'].split(', '))\n",
    "\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    # 모델 예측과 정답을 비교하여 TP, FP, FN 계산\n",
    "    for relation in model_relations:\n",
    "        if any(check_relation_correctness(relation, correct_rel) for correct_rel in true_relations):\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "\n",
    "    for relation in true_relations:\n",
    "        if not any(check_relation_correctness(relation, predicted_rel) for predicted_rel in model_relations):\n",
    "            false_negative += 1\n",
    "\n",
    "    # 누적 합산\n",
    "    total_true_positive += true_positive\n",
    "    total_false_positive += false_positive\n",
    "    total_false_negative += false_negative\n",
    "\n",
    "# Precision, Recall, F1 계산\n",
    "precision = total_true_positive / (total_true_positive + total_false_positive) if (total_true_positive + total_false_positive) > 0 else 0\n",
    "recall = total_true_positive / (total_true_positive + total_false_negative) if (total_true_positive + total_false_negative) > 0 else 0\n",
    "f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"GPT-based Total Precision: {precision:.4f}\")\n",
    "print(f\"GPT-based Total Recall: {recall:.4f}\")\n",
    "print(f\"GPT-based Total F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_168992/1096887098.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
      "/tmp/ipykernel_168992/1096887098.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(all_precisions) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_precisions), \u001b[38;5;28msum\u001b[39m(all_recalls) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_recalls), \u001b[38;5;28msum\u001b[39m(all_f1s) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_f1s)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Precision, Recall, F1-score 계산\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m precision, recall, f1 \u001b[38;5;241m=\u001b[39m calculate_scores(ie[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m], ie[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_output_128\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6297\u001b[0m ):\n\u001b[1;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:191\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(data)\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:245\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    242\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "# chemdner\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임 예시 (df_ie 가 주어진 데이터프레임)\n",
    "ie = df[df['task'] == 'ie']\n",
    "\n",
    "ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n",
    "\n",
    "# Precision, Recall, F1-score 계산 함수\n",
    "def calculate_scores(y_true, y_pred):\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "    \n",
    "    for true_vals, pred_vals in zip(y_true, y_pred):\n",
    "        true_set = set(true_vals) if isinstance(true_vals, list) else set()\n",
    "        pred_set = set(pred_vals) if isinstance(pred_vals, list) else set()\n",
    "\n",
    "        TP = len(true_set & pred_set)  # True Positives (정답과 예측이 일치하는 것)\n",
    "        FP = len(pred_set - true_set)  # False Positives (예측했지만 정답이 아닌 것)\n",
    "        FN = len(true_set - pred_set)  # False Negatives (정답이지만 예측하지 못한 것)\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    return sum(all_precisions) / len(all_precisions), sum(all_recalls) / len(all_recalls), sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "# Precision, Recall, F1-score 계산\n",
    "precision, recall, f1 = calculate_scores(ie[\"output\"], ie[\"model_output_128\"].str.split('Answer: ').str[1])\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Precision: 0.0476, Recall: 0.3333, F1: 0.0833\n",
      "[2/30] Precision: 0.5000, Recall: 0.5000, F1: 0.5000\n",
      "[3/30] Precision: 1.0000, Recall: 0.3333, F1: 0.5000\n",
      "[4/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[5/30] Precision: 4.0000, Recall: 0.3333, F1: 0.6154\n",
      "[6/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[7/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[8/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[9/30] Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "[10/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[11/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[12/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[13/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[14/30] Precision: 9.0000, Recall: 0.7500, F1: 1.3846\n",
      "[15/30] Precision: 3.0000, Recall: 1.0000, F1: 1.5000\n",
      "[16/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[17/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[18/30] Precision: 2.0000, Recall: 1.0000, F1: 1.3333\n",
      "[19/30] Precision: 1.0000, Recall: 0.5000, F1: 0.6667\n",
      "[20/30] Precision: 2.0000, Recall: 0.4000, F1: 0.6667\n",
      "[21/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[22/30] Precision: 1.0000, Recall: 0.2500, F1: 0.4000\n",
      "[23/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[24/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[25/30] Precision: 1.0000, Recall: 0.1000, F1: 0.1818\n",
      "[26/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[27/30] Precision: 1.0000, Recall: 0.2500, F1: 0.4000\n",
      "[28/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[29/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[30/30] Precision: 3.0000, Recall: 0.5000, F1: 0.8571\n",
      "\n",
      "=== Final Evaluation Scores ===\n",
      "Precision: 0.9683\n",
      "Recall: 0.2417\n",
      "F1-score: 0.3252\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ✅ OpenAI API Key 설정\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# ✅ GPT-3.5를 활용하여 정답과 예측을 비교하는 함수\n",
    "def gpt_judge(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    GPT-3.5를 사용하여 예측값과 정답값이 의미적으로 일치하는지 판단.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in named entity recognition evaluation.\n",
    "    Compare the gold standard answer with the predicted answer and determine whether they match.\n",
    "\n",
    "    Gold Standard: {true_text}\n",
    "    Model Prediction: {pred_text}\n",
    "\n",
    "    Output \"Yes\" if both values are identical, including cases where both are empty (None, \"\", [], or {{}}). Output \"No\" otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return 1 if answer.lower() == \"yes\" else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] OpenAI API call failed: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# ✅ 데이터프레임에서 \"ie\" 태스크 선택\n",
    "ie = df[df['task'] == 'ie'].copy()\n",
    "\n",
    "# ✅ 전처리: 소문자로 변환 후 리스트로 변환\n",
    "ie[\"output\"] = ie[\"output\"].str.lower().str.split(\", \")\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].str.lower().str.split(\", \")\n",
    "\n",
    "# ✅ GPT-3.5를 이용한 Precision, Recall, F1-score 계산\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "for idx, (true_vals, pred_vals) in enumerate(zip(ie[\"output\"], ie[\"model_output_128\"])):\n",
    "    true_set = set(true_vals) if isinstance(true_vals, list) else set()\n",
    "    pred_set = set(pred_vals) if isinstance(pred_vals, list) else set()\n",
    "\n",
    "    TP = sum(gpt_judge(t, p) for t in true_set for p in pred_set)  # GPT로 TP 판별\n",
    "    FP = len(pred_set) - TP  # False Positives\n",
    "    FN = len(true_set) - TP  # False Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    # ✅ 진행상황 출력\n",
    "    print(f\"[{idx+1}/{len(ie)}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    time.sleep(1)  # OpenAI API Rate Limit 방지\n",
    "\n",
    "# ✅ 최종 성능 계산\n",
    "final_precision = sum(all_precisions) / len(all_precisions)\n",
    "final_recall = sum(all_recalls) / len(all_recalls)\n",
    "final_f1 = sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(\"\\n=== Final Evaluation Scores ===\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1-score: {final_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] Precision: 0.0476, Recall: 0.3333, F1: 0.0833\n",
      "[2/30] Precision: 0.5000, Recall: 0.5000, F1: 0.5000\n",
      "[3/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[4/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[5/30] Precision: 0.1333, Recall: 0.1667, F1: 0.1481\n",
      "[6/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[7/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[8/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[9/30] Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "[10/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[11/30] Precision: 0.0625, Recall: 0.3333, F1: 0.1053\n",
      "[12/30] Precision: 0.0455, Recall: 0.2500, F1: 0.0769\n",
      "[13/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[14/30] Precision: 0.0625, Recall: 0.0833, F1: 0.0714\n",
      "[15/30] Precision: 0.0909, Recall: 0.6667, F1: 0.1600\n",
      "[16/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[17/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[18/30] Precision: 0.0667, Recall: 0.5000, F1: 0.1176\n",
      "[19/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[20/30] Precision: 0.1818, Recall: 0.4000, F1: 0.2500\n",
      "[21/30] Precision: 0.1111, Recall: 0.3333, F1: 0.1667\n",
      "[22/30] Precision: 0.0588, Recall: 0.1667, F1: 0.0870\n",
      "[23/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[24/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[25/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[26/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[27/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[28/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[29/30] Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "[30/30] Precision: 0.1176, Recall: 0.3333, F1: 0.1739\n",
      "\n",
      "=== Final Evaluation Scores ===\n",
      "Precision: 0.0659\n",
      "Recall: 0.1689\n",
      "F1-score: 0.0869\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ✅ OpenAI API Key 설정\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# ✅ GPT-4o를 활용하여 정답과 예측을 비교하는 함수\n",
    "def gpt_judge(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    GPT-4o를 사용하여 예측값과 정답값이 의미적으로 일치하는지 판단.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in named entity recognition evaluation.\n",
    "    Compare the gold standard entity with the predicted entity.\n",
    "    \n",
    "    Gold Standard Entity: \"{true_text}\"\n",
    "    Model Predicted Entity: \"{pred_text}\"\n",
    "    \n",
    "    If both entities refer to the same concept, output \"Yes\".\n",
    "    If they are different, output \"No\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=5,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return 1 if answer.lower() == \"yes\" else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] OpenAI API call failed: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# ✅ 데이터프레임에서 \"ie\" 태스크 선택\n",
    "ie = df[df['task'] == 'ie'].copy()\n",
    "\n",
    "# ✅ 전처리 함수: 다양한 형태를 리스트로 변환\n",
    "def normalize_to_list(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed_value = json.loads(value)  # JSON 문자열 파싱 시도\n",
    "            if isinstance(parsed_value, list):\n",
    "                return [v.strip().lower() for v in parsed_value]\n",
    "            elif isinstance(parsed_value, dict):\n",
    "                return [v.strip().lower() for v in parsed_value.values()]\n",
    "            else:\n",
    "                return [str(parsed_value).strip().lower()]\n",
    "        except json.JSONDecodeError:\n",
    "            return [v.strip().lower() for v in value.replace(\"\\n\", \",\").split(\",\")]\n",
    "    elif isinstance(value, dict):\n",
    "        return [v.strip().lower() for v in value.values()]\n",
    "    elif isinstance(value, list):\n",
    "        return [v.strip().lower() for v in value]\n",
    "    else:\n",
    "        return [str(value).strip().lower()]\n",
    "\n",
    "# ✅ 데이터 정리\n",
    "ie[\"output\"] = ie[\"output\"].apply(normalize_to_list)\n",
    "ie[\"model_output_128\"] = ie[\"model_output_128\"].apply(normalize_to_list)\n",
    "\n",
    "# ✅ GPT-4o를 이용한 Precision, Recall, F1-score 계산\n",
    "all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "for idx, (true_vals, pred_vals) in enumerate(zip(ie[\"output\"], ie[\"model_output_128\"])):\n",
    "    true_set = set(true_vals)\n",
    "    pred_set = set(pred_vals)\n",
    "\n",
    "    # ✅ GPT-4o로 TP(정답 매칭) 판별\n",
    "    TP = sum(1 for t in true_set if any(gpt_judge(t, p) for p in pred_set))\n",
    "    FP = len(pred_set) - TP  # False Positives (잘못된 예측)\n",
    "    FN = len(true_set) - TP  # False Negatives (누락된 정답)\n",
    "\n",
    "    # ✅ 정밀도, 재현율, F1-score 계산\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    all_precisions.append(precision)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1s.append(f1)\n",
    "\n",
    "    print(f\"[{idx+1}/{len(ie)}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# ✅ 최종 성능 출력\n",
    "print(\"\\n=== Final Evaluation Scores ===\")\n",
    "print(f\"Precision: {sum(all_precisions) / len(all_precisions):.4f}\")\n",
    "print(f\"Recall: {sum(all_recalls) / len(all_recalls):.4f}\")\n",
    "print(f\"F1-score: {sum(all_f1s) / len(all_f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent with the context. The exchange between the patient and the doctor follows a logical sequence of questions and answers that align with the medical situation described, although it misses a few specific details provided in the context.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response covers most of the critical aspects related to the patient's symptoms, medical history, and treatment, but omits the specific mention of the patient's clinical improvement timeline and some details from the histopathological examination that are present in the true answer.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The dialogue produced by the model flows naturally and resembles a real conversation between a patient and a doctor, but could slightly improve by diversifying patient responses to ensure more natural sounding interaction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0  \n",
      "  The model's response aligns logically with the context provided. It accurately captures the sequence of events in the conversation and presents them in a clear, structured format.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The model's response sufficiently covers the key points from the conversation, including the patient's condition, treatment history, and outcomes. However, it lacks specific details such as the patient's age and the division of the iloperidone dose as mentioned in the true answer, which slightly affects the completeness.\n",
      "\n",
      "- Naturalness: 4.0  \n",
      "  The model's response is fluent and systematically organized, which is typical for clinical notes. However, it feels more like a structured report than a natural conversation, which might lack some elements of human-like expression expected in a natural setting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0  \n",
      "The model's response logically aligns with the context provided in the input. It accurately recounts pertinent details from the dialogue and organizes them cohesively in a clinical note style format. The information flow matches the progression of the initial conversation between the doctor and the patient.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "The model's response covers a majority of the key information necessary to convey the context of the patient's medical evaluation. It includes the relevant family medical history, details about the child's birth and early life, and the doctor's conclusions and proposed actions. However, while it captures the essence of the input, it slightly lacks the specificity of some symptoms detail, e.g., not quoting exact numbers of diarrhea episodes or vomiting frequency which was provided in the dialogue.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "The response is fluent and reads like a professional clinical note which is appropriate for the setting. It maintains a consistent clinical tone typical of documentation in medical practice. However, this style, while highly appropriate and clear, might come across as slightly robotic, which impacts the conversational naturalness slightly. Nevertheless, for the purpose of a clinical note, it's highly effective.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0. The model's response logically aligns with the context provided, following the medical conversation accurately.\n",
      "- Completeness: 4.5. The model’s response captures most of the key details from the true answer, such as the medical history, tumor details, treatment, and results. It omits some specific numerical details related to the medical treatment, but it generally conveys the situation well.\n",
      "- Naturalness: 5.0. The dialogue between the doctor and the patient flows naturally and sounds fluent and human-like, maintaining a consistent and realistic conversational tone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent and aligns well with the context provided in the input. The key points from the doctor-patient conversation are accurately reflected in the clinical note. However, there might be slight room for improvement in ensuring every nuance from the input is maintained, for example, mentioning coronary artery bypass graft surgery more explicitly.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's summary is mostly complete, capturing the critical aspects of the patient's case, including the main complaint, history, examination findings, treatments, and outcomes. However, it omits specific details such as the cardiac arrest that followed coronary artery bypass graft surgery and the three-piece flexible silicone lens with polyimide haptics.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The model's response is natural and reads fluently like a typical clinical note written by a healthcare professional. It concisely conveys the summarized information in a structured and professional manner appropriate for a clinical context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent and logically aligns with the provided context, summarizing the patient's diagnosis, symptoms, treatment, and follow-up plan effectively. However, it does not mention the patient being Assyrian and having moved to Sweden, which were included in the true answer.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model provides a comprehensive summary of the patient's condition and treatment, but misses some details included in the true answer, such as specific examination findings (e.g., perioral movements, chorea, and dystonic posturing), the fact that the patient moved to Sweden, and detailed psychometric evaluation results. \n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent and human-like, effectively written in a clinical note format. However, it might sound slightly too structured compared to real human interaction, which typically has more variability in tone and structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It captures all relevant details from the interaction between the doctor and the patient, including medical history, treatment regimen, response to treatment, side effects, biopsy results, and recommendations.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The model's response is mostly complete, capturing almost all significant aspects of the patient's conversation with the doctor. It includes the patient's medical history, treatment details, response to treatment, and biopsy results. However, it could improve by mentioning the specific Fitzpatrick skin phototype (3) of the patient, as present in the true answer.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The model's response is fluently structured as a clinical note. It effectively organizes the patient's information in a manner consistent with medical documentation, maintaining a human-like clarity and precision expected in clinical settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response logically aligns with the input context, capturing key medical details discussed between the doctor and Mrs. Smith. Most important details such as medical history, current treatments, symptoms, and examination findings are present. However, there is a minor inconsistency with the patient's age.\n",
      "\n",
      "- Completeness: 4.0\n",
      "  - The response captures the essential points needed to summarize Mrs. Smith's medical situation effectively. However, it lacks some details included in the true answer, such as her past smoking history and chest X-ray findings, as well as not mentioning the minor discrepancy in her age.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "  - The model's response is fluent and human-like. It presents the clinical note in a professional and clear manner, appropriately matching the standard format used in clinical settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is coherent as it accurately reflects the information and events from the dialogue. However, it could be slightly improved by mentioning that the patient is not diabetic or immunosuppressed, aligning with the input where the patient explicitly states this.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The model's response is mostly complete and provides a comprehensive summary of the patient's condition, examination findings, diagnosis, treatment, and follow-up plan. A minor omission is the detail about the patient not being diabetic or immunosuppressed, which could be relevant to the medical context.\n",
      "\n",
      "- Naturalness: 4.0  \n",
      "  The response is clear and systematically organized, typical of a clinical note. While it effectively communicates medical information, it does not fully capture the conversational tone from the dialogue, which would make it sound more human-like and natural.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "  - The model’s response is fully aligned with the information provided in the input dialogue. It follows a logical sequence and accurately represents the patient history discussed between the doctor and the patient.\n",
      "\n",
      "- Completeness: 4.5\n",
      "  - The model's response covers almost all the critical points discussed in the dialogue, providing a comprehensive summary of the patient’s medical history and treatment. However, it misses out on the specific detail of the patient being a 55-year-old Middle-Eastern female, which appears in the true answer.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "  - The response is fluent and reads in a clear, human-like manner. It employs a professional tone appropriate for summarizing medical information and uses vocabulary that is typical in a clinical setting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.8  \n",
      "  The model’s response is coherent with the context provided. It accurately summarizes the pertinent details of the conversation such as the medical history, test results, adjustments in insulin dosage, and follow-up plans. Minor specific details such as the surgery location being at Cornell University Veterinary Specialists are omitted.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The response contains the majority of the critical information from the conversation, including medical history, pre-operative blood work results, and dosage adjustments. However, it lacks some specifics like the location of the service and some diagnostic tests, which prevents it from being completely thorough.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The model's response is fluent and well-structured, reflecting a natural and human-like summary. It employs appropriate medical terminology and maintains a professional tone consistent with a veterinary discussion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It effectively summarizes the key points of the dialogue between the doctor and the patient, including details of the injury, treatment, and the progress of the patient. However, it slightly lacks additional context given in the input, such as the patient's detailed work environment and use of a waist support belt, which would add to the coherence.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The model provides a sufficiently complete answer, covering the main aspects of the case, such as the nature of the injury, the medical examination, treatment, and follow-up plans. However, it misses some specifics found in the 'True Answer', like the diabetic status and exact work conditions, which would have made it more thorough.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The response is fluent and human-like, reflecting the tone and style typical of clinical notes. It reads as a coherent narrative, maintaining a professional tone suitable for medical documentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response details align well with the context provided in the conversation. It logically follows the chronology and includes details discussed between the doctor and patient.\n",
      "  \n",
      "- Completeness: 4.0\n",
      "  - The model covers most key aspects mentioned in the conversation. However, it misses some detailed values and qualifiers that are present in the true answer, like specific percentile information for measurements and some precise data from the genetic screening results.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "  - The response is generally fluent and human-like, though it has a more structured and list-like format. It maintains a clear, concise summary style appropriate for medical information sharing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model’s response is coherent with the input context. It captures the critical details of the patient’s medical condition and the preparations made for the surgery. However, it doesn't mention certain specifics such as the patient's exact age, weight, and height which were mentioned in the true answer.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The response covers most of the important points such as the patient's medical history, diagnosis, and procedural preparations. Yet, it misses some specific numeric details like the patient's age, weight, and certain procedural steps before anesthesia, as provided in the true answer.\n",
      "\n",
      "- Naturalness: 4.7\n",
      "\n",
      "The model's response is fluent, human-like, and resembles a professional clinical note, which is appropriate for the context. It effectively presents the information clearly and logically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response aligns well with the input context. It accurately summarizes the conversation and synthesizes the key elements of the dialogue between the doctor and the patient. One minor issue is that it doesn't specify the location (Tokushima University Hospital) or the patient's specific demographics (e.g., age, nationality), which are present in the true answer.\n",
      "  \n",
      "- Completeness: 4.5\n",
      "  - The response is comprehensive in summarizing the patient's symptoms, diagnosis, treatment plan, and evaluation methods. However, it doesn't mention some specifics such as the patient's age, nationality, and exact medicine dosage mentioned in the true answer. Despite these omissions, the response still adequately answers the question of documenting the doctor's interaction with the patient regarding BMS.\n",
      "  \n",
      "- Naturalness: 4.0\n",
      "  - The response is structured more like a clinical note rather than conversational dialogue, which is appropriate given the task but may slightly reduce its fluency for someone expecting a narrative format. This format is concise and typical in medical documentation, yet slightly impersonal and less human-like in tone compared to a conversational style.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "\n",
      "The model's response logically aligns with the context provided. It accurately summarizes key pieces of information from the input data, including the patient's medical history, medications, the nature of the nodular formation, imaging done, and the planned biopsy. All the details in the model's response are corroborated by the input dialogue.\n",
      "\n",
      "- Completeness: 5.0\n",
      "\n",
      "The model sufficiently answers the implied question, which is to summarize the patient's condition and plan of action for a clinical note. It covers all major aspects mentioned in the input, including symptoms, medical history, medications, the diagnosis, and the treatment plan. While it does not specify the patient's age or the timing of the visit (as in the true answer), these details are not critical for the typical scope of a clinical note.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "\n",
      "The model's response is structured and clear but resembles more of a clinical note format rather than a conversational summary. While this is appropriate for the context, it slightly reduces the naturalness as it is not conversational. However, considering the purpose of producing a clinical note, the structure is suitable and retains the necessary clarity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "  - The model's response is coherent with the input context. It logically organizes and summarizes the conversation between the doctor and the patient in a clear and structured format as a clinical note.\n",
      "\n",
      "- Completeness: 4.5\n",
      "  - The model's response captures the critical aspects of the conversation, including the patient's history, symptoms, evaluation, diagnosis, treatment, and follow-up. However, it doesn't mention specific details such as the patient's love for the outdoors or her geographic history, which could be relevant for a complete clinical picture.\n",
      "\n",
      "- Naturalness: 4.0\n",
      "  - The model's response is clear, fluent, and resembles a typical clinical note written by a healthcare professional. However, the format is more structured than conversational, which makes it less human-like in tone compared to natural dialogue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response logically aligns with the context provided, with only minor omissions (e.g., the specific age and gender of the baby, and the mother's gestational diabetes mellitus).\n",
      "  \n",
      "- Completeness: 4.0\n",
      "  - The model's response captures the majority of relevant details from the dialogue. However, it misses the mention of the gestational diabetes mellitus of the mother, which is present in the true answer. While the absence of this detail slightly affects the depth of the background understanding, the main sequence of events and their logical progression are adequately covered.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "  - The response is fluent and sounds human-like. It presents the information in a clear and well-structured manner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "\n",
      "The model's response is highly coherent as it logically aligns with the context provided in the input. It accurately summarizes the medical conditions, symptoms, and laboratory results discussed between the doctor and the patient.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The model's response sufficiently answers the question by incorporating the key elements of the patient's medical status and developments. However, it does not provide specific treatment details or particular historical context, such as past treatments with azathioprine and prednisolone, which appear in the True Answer.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The model's response maintains a fluent and human-like tone, especially given the clinical context. It presents information in a structured manner typical of clinical notes, which aligns well with the expected format for such documentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.8  \n",
      "  The model's response is highly coherent with the context provided. It accurately captures the sequence of the conversation and the medical details, reflecting a logical alignment with the input.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The response covers almost all key elements and specifics mentioned in the input. It correctly includes information like symptoms, comorbidities, test results, and treatments. However, it slightly lacks some details from the True Answer, such as exact numerical ranges or specific details about the histopathological assessment which were briefly referred but not expanded upon.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response maintains a natural, human-like dialogue flow and uses language that is appropriate for a clinical setting. It replicates conversational exchanges between a doctor and a patient effectively, though it occasionally sounds a bit too structured or formal compared to casual conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response aligns well with the context provided in the input. It accurately summarizes the patient's history, medical history, and treatments. However, there are a few minor details missing, such as the patient's educational background, which affects the overall coherence slightly. \n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "While the model's response covers most of the essential information provided in the input, it omits specific details like the educational background of the patient, that he was married with children, and the specifics of the rehabilitation therapies (like hyperbaric oxygen therapy and acupuncture). These omissions reduce the completeness of the response slightly.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "\n",
      "The response is structured in a clear and concise manner, resembling a medical summary or report, which is appropriate for the context. It uses professional medical language and maintains fluency, although it might not sound as conversational as a typical human interaction might be. This slightly affects the naturalness score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response aligns well with the input context. It includes relevant details such as the history of the swollen finger, comorbid conditions, examination findings, X-ray results, treatment given, and instructions for follow-up. However, it lacks the specific detail about the unavailability of cloxacillin, which was mentioned in the true answer. This slightly affects coherence with the true context presented.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The model's response covers most of the key points from the input, including history, examination findings, diagnosis, treatment, and follow-up instructions. However, it omits certain specific details like the specific antibiotic used during admission (amoxicillin/clavulanic acid) and the absence of details about surgical drainage not being warranted. These aspects are important for a comprehensive clinical note.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "The model's response is fluent and sounds human-like. It uses appropriate medical terminology and follows a coherent structure typical of clinical notes. The language is clear and flows naturally, making it easy to read and understand.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "- Completeness: 4.5\n",
      "- Naturalness: 4.8\n",
      "\n",
      "**Explanation**:\n",
      "- **Coherence (5.0)**: The model's response is logically aligned with the provided conversation. It accurately captures the dialogue between the doctor and the patient, including the patient's beliefs and treatment details, indicating a high level of coherence.\n",
      "  \n",
      "- **Completeness (4.5)**: The model's response sufficiently answers the question by providing a detailed clinical note that reflects most of the pertinent details discussed in the conversation. The only minor omission is the detail regarding the patient's wife's anger at the explanation about the absence of insects, which is present in the true answer. \n",
      "\n",
      "- **Naturalness (4.8)**: The model's response is fluent and presented in a professional and human-like manner typical for clinical notes. It maintains a consistent and natural tone throughout, aligning well with the expectations of medical documentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model’s response is largely coherent with the conversation provided. It captures key details such as the patient's medical history, surgical procedures, and post-operative complications. However, some elements, like the exact medical context or nuances of treatment decisions, could be more clearly detailed to perfectly align with the conversation.\n",
      "\n",
      "- Completeness: 4.0\n",
      "  - The model adequately summarizes the key elements of the patient's situation, including her medical history, surgeries, and post-operative symptoms. However, it omits some detailed medical history aspects and rationale for treatment decisions provided in the true answer, such as the risk factors for endometrial cancer and specific treatments for rheumatoid arthritis, which would complete the context.\n",
      "\n",
      "- Naturalness: 4.8\n",
      "  - The response is well-structured and reads fluently like a typical clinical note. The language used is appropriate for medical reporting, maintaining a professional tone. It effectively mirrors the natural, concise style of medical documentation, with the clarity of the note being generally strong. The inclusion of placeholders for patient details adds a minor unnatural element that can be improved with specific information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It accurately summarizes the patient's medical history and treatments as discussed in the conversation between the doctor and the patient. However, it lacks some specific details present in the true answer, like the patient's age and diabetes condition.\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The model's response sufficiently answers the question by capturing the key points of the patient's medical condition and treatment history. However, it omits specific details such as the frequency and composition of the treatments (e.g., the concentration and volume of mitomycin-C gel), which were present in the true answer.\n",
      "\n",
      "- Naturalness: 4.5\n",
      "\n",
      "The model's response is fluent and human-like, effectively presenting the clinical summary in a structured and coherent manner. It reads like a clinical note, which is appropriate for the context. Still, there is a slightly clinical and formal tone typical of medical documentation, which may feel less conversational compared to a typical human response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response aligns well with the input context. It accurately reflects the details discussed in the patient-doctor conversation, maintaining logical consistency throughout the summary.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response sufficiently answers the question and covers most important details from the input. However, it lacks certain specifics, such as the patient's age and the discovery date of the optic nerve head mass, which were present in the true answer.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The response is fluent and reads like a clinical note written by a human. It is clear and concise, using appropriate medical terminology without being overly technical or incoherent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "\n",
      "The model's response is logically aligned with the context provided in the input. The clinical note structure naturally follows the discussion between the doctor and the patient, accurately detailing the patient's primary complaint, past diagnoses, and medication history. The coherence could be improved slightly by matching more specific details, such as the setting (e.g., Kindai University Hospital) and explicit patient demographics (e.g., age).\n",
      "\n",
      "- Completeness: 4.0\n",
      "\n",
      "The model's response captures most of the key information necessary to summarize the clinical scenario, including the patient's complaint, diagnosis, medication history, and planned diagnostic tests. However, it lacks specific details present in the true answer, like the patient's age and the specific setting (which may or may not have been shared directly in this text but might be deemed critical in a complete clinical note), which could be essential for medical documentation.\n",
      "\n",
      "- Naturalness: 4.8\n",
      "\n",
      "The language used in the model's response is fluent and human-like, closely resembling how a medical professional might structure a clinical note. It follows a typical format for documentation in medical settings, making it sound professional and natural, very much in line with expected human-generated text in a clinical context. Further improvements could involve minor refinements to enhance the natural flow further.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response maintains logical consistency with the context provided. The dialogue flows well and matches the progression of questions and answers seen in the input and the true answer. There are no significant logical discrepancies.\n",
      "\n",
      "- Completeness: 4.8\n",
      "  - The model's response covers the details essential to the patient's medical history, current symptoms, and diagnostic findings as outlined in the true answer. However, it slightly generalizes some of the specifics, such as missing the details of the laboratory results like precise numerical values.\n",
      "\n",
      "- Naturalness: 4.7\n",
      "  - The model's response is fluent and reads in a human-like conversational manner. It mirrors the realistic interaction between a doctor and patient effectively, with a few minor deviations from typical conversational norms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0 - The model's response aligns logically with the context provided in the input, maintaining the flow of the dialogue between the doctor and patient.\n",
      "- Completeness: 4.5 - The model covers most of the important details discussed in the input and includes additional clinical notes that summarize the situation well. However, there are some minor details omitted or less detailed compared to the true answer, such as specific BMI values, additional background on neuropsychiatric evaluation, and screen time details.\n",
      "- Naturalness: 4.5 - The response is fluent and mimics a natural conversation between a doctor and a patient reasonably well. However, the inclusion of clinical notes in the middle of the dialogue, while informative, slightly disrupts the natural conversation flow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "\n",
      "The model's response logically aligns with the context provided in the input. It accurately captures the key discussions between the doctor and the patient about nursing intensity and DRG systems. However, there are minor details that appear to be altered, such as the patient's gender.\n",
      "\n",
      "- Completeness: 4.5\n",
      "\n",
      "The model's response sufficiently answers the question in the input by summarizing the doctor's explanations and the patient's understanding. It captures most of the important points discussed, such as the importance of nursing intensity weights and the doctor's commitment to providing the best care. However, it lacks details about the comparison of cases with different nursing intensities, as mentioned in the true answer.\n",
      "\n",
      "- Naturalness: 4.7\n",
      "\n",
      "The model's response is fluent and human-like. It presents the information in a coherent manner, and the flow of information is natural for a clinical note or summary format. The dialogue is transformed into a concise clinical note without losing clarity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       4.670000\n",
      "Completeness    4.293333\n",
      "Naturalness     4.680000\n",
      "BLEURT          0.539646\n",
      "BERTScore_F1    0.463108\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summarization\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "summarization = df[df['task'] == 'summarization']\n",
    "results = []\n",
    "\n",
    "for _, row in summarization.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.8  \n",
      "  The model’s response logically aligns with the context provided. It addresses the concerns about high blood sugar, acknowledges the sedentary lifestyle, mentions symptoms, and considers the medical history. The response maintains a coherent structure throughout.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The model’s response is quite comprehensive, addressing both the medication and lifestyle factors, and suggests consultations with a dietitian and healthcare provider for further management. It covers most of the concerns but does not delve into specific treatment adjustments or new medications, which could have enhanced completeness.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "  The response sounds fluent and human-like, maintaining an empathetic and professional tone appropriate for a medical response. The language used is clear and understandable, and it reflects a natural conversational style typically used by healthcare providers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0  \n",
      "  The model's response logically aligns with the context and question provided. It addresses the concern about foreskin hygiene and provides a clear insight into when and why actions like wiping or shaking after urination are sufficient.\n",
      "\n",
      "- Completeness: 4.5  \n",
      "  The response sufficiently answers the question by explaining that it is not necessary to pull back the foreskin and wipe or dry after every urination. It mentions the importance of general hygiene practices and provides a possible action if symptoms occur. However, it could have been slightly more complete by mentioning smegma and specific conditions like diabetes that could increase the risk of infections.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The model's response is fluent and sounds human-like. The language used is clear, courteous, and appropriate for the context, mimicking a typical professional response from a healthcare provider.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 3.5  \n",
      "  The model's response logically aligns with the context provided in the input, addressing symptoms such as painful bumps, burning sensation, and swelling. However, it focuses specifically on a tonsillar abscess as the potential diagnosis, which doesn't directly match the location of the bumps described (back of the tongue versus tonsil area).\n",
      "\n",
      "- Completeness: 3.0  \n",
      "  While the response does provide a potential explanation for the symptoms, it offers only one possible condition (tonsillar abscess) without considering other medical possibilities discussed in the true answer such as herpes, canker sores, or oral thrush. Thus, it doesn't sufficiently cover the range of potential causes for the symptoms.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The model's response is fluent and human-like, using appropriate medical terminology and explaining the situation clearly. It suggests consulting a healthcare provider and describes possible diagnostic and treatment procedures in a natural and professional manner. However, it might benefit from a slightly more conversational tone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response aligns well with the input context. It logically follows from the details provided, such as the baby's presentation and the AFI measurement, offering advice related to these elements.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response sufficiently addresses the question about labor induction by discussing general guidelines and potential medical conditions that might warrant induction. However, it lacks specific recommendations concerning the AFI level and doesn't mention other monitoring like CTG, which the true answer addresses.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent and human-like, providing reassurance and guidance in a clear and empathetic manner, similar to how a healthcare provider might communicate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It addresses the symptoms in relation to the user's medical conditions such as diabetes, blood pressure, and thyroid issues, which could impact eye health. However, it does not mention posterior vitreous detachment, which is a significant factor at the user's age, as noted in the true answer.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response is thorough in discussing possible causes related to the user's existing conditions and advises seeing an ophthalmologist. It suggests maintaining medication and a healthy lifestyle, but it doesn't specifically mention posterior vitreous detachment, which is highly relevant given the user's age.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent and sounds very much like a human doctor would when providing medical advice. The language used is clear and professionally reassuring, which is appropriate for the context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response aligns quite well with the input and maintains logical consistency throughout. The response appropriately addresses the issues of depression, heart pain, and concerns about masturbation, offering a sympathetic and structured reply.\n",
      "  \n",
      "- Completeness: 4.0\n",
      "  - The response sufficiently answers the question by offering a range of suggestions to address the main concerns presented. It provides insights into the depression and related symptoms along with constructive alternatives to cope with stress, although it could more directly address the specific emotional trigger of failed love mentioned in the input.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "  - The response reads very fluently and human-like, with an empathetic and supportive tone. The language is clear and sounds like it could come from a compassionate human advisor or therapist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response logically aligns with the context provided, acknowledging that hormonal imbalance could be a cause but also mentions other factors that can influence vaginal discharge. It appropriately suggests consultation with a healthcare provider for a proper diagnosis, which is recommended in this context.\n",
      "  \n",
      "- Completeness: 3.5  \n",
      "  - While the model addresses that hormonal imbalance is a possible cause and suggests seeing a healthcare provider, it lacks specificity and depth in addressing an infection as a likely cause, which is a key element in the true answer. It provides a general suggestion for seeking medical advice without detailing specific concerns such as infection or diabetes screening that the true answer covers.\n",
      "  \n",
      "- Naturalness: 4.8  \n",
      "  - The response is fluent and human-like, offering a polite and professional tone. It reads clearly and would sound natural in a medical advice context, maintaining a respectful manner while advising the user to seek further help.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "The model's response aligns well with the context provided in the input. It addresses the user's concerns about hair loss, acne, the impact of past treatments like Marvelon, and the effects of current treatments such as Tetracycline.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "The response is quite complete, offering insights into potential courses of action and the effects of the mentioned medications and treatments. However, it does not provide a concrete recommendation or guidance concerning whether or not to resume Marvelon, specific alternative anti-androgen medications, or the long-term effects of Spironolactone as asked in the query.\n",
      "\n",
      "- Naturalness: 4.7  \n",
      "The language used in the response is fluent and human-like, making it easy to understand. It reads like a professional advice article, which is appropriate for the context of providing medical insights and recommendations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0\n",
      "\n",
      "    The model's response logically aligns with the context provided, acknowledging the patient's concern regarding the kidney stone, stent, and possible use of natural remedies. It also provides relevant information about the procedure for the CAT scan and advice on handling symptoms. However, it lacks specific details about the necessity of general anesthesia for surgery that were mentioned in the true answer.\n",
      "\n",
      "- Completeness: 3.5\n",
      "\n",
      "    The response addresses several aspects of the user's questions, offering advice to consult a doctor about natural remedies and explaining the typical process for a CAT scan. However, it lacks a complete response regarding the definite need for surgery due to the stone's size and type of anesthesia required, which is crucial information in this context. Also, it does not directly address the question about whether kidney stones can come out from behind.\n",
      "\n",
      "- Naturalness: 5.0\n",
      "\n",
      "    The model's response is fluent and human-like, with polite and empathetic language. It sounds natural and supportive, which is appropriate for addressing the user's health concerns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided, discussing the implications of a hypoplastic artery and its association with erectile dysfunction, as mentioned in the input. However, it doesn't address the discrepancy between the urologist's conclusion and the test results, which slightly affects coherence.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model provides a clear explanation of what \"hypoplastic right cavernosal artery\" means and how it relates to erectile dysfunction, which directly answers the user's primary question. However, it doesn't explore potential next steps, such as additional questions about medical history or lifestyle factors, which are present in the true answer.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent and human-like. It uses appropriate medical terminology in a way that is comprehensible to a non-specialist, sounding natural and conversational.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5 - The model's response aligns well with the context provided in the input by discussing the potential negative effects of a long-term ketogenic diet and acknowledging short-term benefits, similar to the true answer.\n",
      "- Completeness: 4.7 - The model thoroughly answers the question by addressing multiple potential negative effects of a long-term ketogenic diet, though it could include more on ketosis and other risks mentioned in the true answer (like ketoacidosis).\n",
      "- Naturalness: 4.8 - The response is fluent and human-like, with clear and well-structured language. It offers detailed explanations and reads naturally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response aligns with the context provided in the input and addresses the potential traditional use of Argyreia speciosa in treating gonorrhea and syphilis. It also logically introduces the necessity of conventional antibiotics for these bacterial infections.\n",
      "\n",
      "- Completeness: 3.0  \n",
      "  - While the response provides general information about Argyreia speciosa, it lacks the specificity found in the True Answer regarding traditional preparation techniques for treating gonorrhea and syphilis. It mentions consulting a qualified herbalist instead of detailing preparation techniques like heating leaves or using seed decoctions.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  - The model's response is fluent, clear, and sounds human-like. It uses natural-sounding language, giving a professional and empathetic tone appropriate for addressing a health-related inquiry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is logically aligned with the context, addressing both the respiratory and gastrointestinal symptoms and linking them to potential causes such as respiratory infections and gastrointestinal irritation. It also considers the impact of poor air quality on respiratory issues, showing a good understanding of the context.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  While the model identifies the potential issues and suggests consulting a specialist, it lacks specific recommendations or treatment options compared to the true answer, which offers clear medication suggestions and dietary advice. The model provides a general guideline but misses the actionable steps that could directly assist the user.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent and human-like, with a professional tone appropriate for a medical consultation. The language used is clear and easy to understand, resembling how a healthcare provider might communicate information to a patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "  - The model's response logically aligns with the context provided in the input. It acknowledges the individual's concern about being more vulnerable to COVID-19 due to a weakened immune system and provides advice on self-isolation. However, the model's assertion that autoimmune conditions specifically increase risk is slightly misleading when compared to the true answer.\n",
      "  \n",
      "- Completeness: 4.0\n",
      "  - The response addresses the main question regarding self-isolation by advising extra precautions and offers general advice on staying safe from COVID-19. However, it lacks detailed clarity on why autoimmune conditions may not necessarily increase risk, which the true answer provides. The model does not discuss handwashing or social distancing, which would have offered a more comprehensive response.\n",
      "  \n",
      "- Naturalness: 5.0\n",
      "  - The response is fluent, human-like, and clear. It provides practical advice and conveys empathy and understanding, making it sound natural and well-structured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context of a stage 4 endometrium cancer with lung metastasis, and it acknowledges the absence of recurrence at the surgical site. It focuses on appropriate treatment options for managing the lung metastases.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response covers several primary treatment options, including chemotherapy, targeted therapy, palliative care, and clinical trials, providing a broad overview. However, it lacks a detailed consideration of patient-specific factors such as age or associated medical conditions, which could guide the choice of treatment. \n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent, well-structured, and resembles the tone and style of a human medical professional, offering empathy and clear advice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "    - The model's response is coherent as it aligns with the symptoms and context provided by the user, acknowledging the possibility of ulnar neuropathy and offering an explanation of the condition. It provides logical steps for addressing the symptoms, such as physical therapy and potential surgery.\n",
      "\n",
      "- Completeness: 4.0\n",
      "    - The model's response addresses the user's questions about possible treatments like physical therapy and surgery, as well as potential long-term effects. However, it could be improved by mentioning additional factors and tests for a comprehensive evaluation, which the true answer provides (e.g., checking for B12 deficiency, diabetes, hypothyroidism, etc.).\n",
      "\n",
      "- Naturalness: 4.5\n",
      "    - The response is very natural and fluent, with a human-like empathetic tone. It uses clear and concise language that makes it easy to understand, resembling a conversation with a healthcare professional.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "The model's response logically aligns with the context provided in the input. It addresses possible causes for the symptoms mentioned, such as hyperhidrosis and infections, which are reasonable assumptions based on the complaint of excessive sweating and odor.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "The model's response sufficiently answers the question about potential causes and suggests seeking professional evaluation. However, it doesn't explore lifestyle or home remedies like the true answer does, giving a slightly less comprehensive answer.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "The response sounds fluent and human-like, using clear and professional language appropriate for a health-related query, showing excellent naturalness in expression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.0  \n",
      "  The model's response is coherent with the context provided, in that it addresses the symptoms described by the user (dull ache and enlarged vein) and suggests varicocele as a possible explanation. However, it does not consider the possibility of a hernia as mentioned in the true answer, which could be another plausible diagnosis.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The model provides a thorough explanation of varicocele, a common condition associated with the symptoms described. It also suggests consulting a healthcare provider, which is appropriate advice. However, it does not explore alternative diagnoses like hernia or ask for additional information that might be necessary for a more accurate assessment, as the true answer does.\n",
      "\n",
      "- Naturalness: 4.5  \n",
      "  The model's response is very natural and human-like, offering a fluent explanation and advice. It reads like a response from a knowledgeable medical professional, providing appropriate detail and guidance in a conversational yet informative manner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response is logically aligned with the input, addressing concerns about traveling during pregnancy with a history of ectopic pregnancy and thyroid issues. It offers relevant advice and considerations.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  - The model's response sufficiently addresses the question with detailed precautions and considerations. However, it could improve by including specific advice such as drinking water or wearing compression stockings during the flight, as noted in the true answer. Additionally, it could mention obtaining a fit to fly certificate for air travel approval.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  - The response is fluent and human-like, with a friendly and empathetic tone. It reads like advice from a healthcare professional, employing a conversational style that aligns well with a human-approach advice context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided in the input. It correctly identifies that a brain hemorrhage is a serious condition and mentions relevant information about monitoring and treatment. However, it incorrectly refers to a brain hemorrhage as a \"stroke,\" which isn't always accurate, as not all strokes are hemorrhagic.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response provides a general overview of the situation, mentioning the severity, potential treatment, and the need for professional medical guidance. However, it lacks specific details that could directly relate to the case, such as the specific area affected, potential long-term effects, and the severity of the condition, which the true answer highlighted.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is very fluent and human-like, employing a compassionate tone suitable for the context. It reads naturally, provides reassurance, and invites further questions, which adds to its realistic quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the input context. It discusses the effects of exercising at different times of the day, the impact on diabetes, and offers advice based on the medication regimen. However, it does not address the aspect of body temperature as highlighted in the true answer.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response provides a well-rounded answer regarding the potential timings for exercise, considering the effects on sleep and blood sugar levels. Still, it misses some underlying physiological details about body temperature mentioned in the true answer and does not discuss the importance of potentially working out on an empty stomach or the benefits of late afternoon exercise.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent, well-structured, and sounds human-like. It uses clear and natural language, offering practical and medically sound advice. However, to align even more closely with typical human interaction, it could incorporate a more engaging or empathetic tone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided. It explains potential reasons for experiencing a ringing noise (tinnitus) following a cold and ties it to Eustachian tube dysfunction, which is coherent with the symptoms described.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response sufficiently answers why there is a ringing noise, and it mentions that it might take time to clear up. However, it could go further by asking for more details about the patient's condition to better tailor a response, as the true answer does.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response sounds fluent and human-like. It provides a clear explanation and reads much like what might be expected from a communication with a healthcare provider, though it slightly lacks the conversational inquiries present in the true answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5 - The model's response logically aligns with the context provided. It correctly identifies the situation and the potential need for further medical intervention, consistent with the input.\n",
      "  \n",
      "- Completeness: 3.0 - The response lacks specific details and recommendations that the true answer provides, such as specific medications, further antibiotic needs, and general wound care advice suited for diabetes or hypertension considerations. It informs only about consulting a professional which is a good step but lacks thorough actions.\n",
      "\n",
      "- Naturalness: 4.8 - The response is fluent and human-like. It uses professional language appropriate for a healthcare scenario, contributing to its naturalness and readability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5\n",
      "- Completeness: 4.0 \n",
      "- Naturalness: 4.8\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "- **Coherence (4.5):** The model's response is coherent and logically aligns with the input questions, providing a structured answer that addresses various aspects of insulin treatment and related medical procedures. However, certain specific details, like the preference for insulin pumps as the best method or lack of personal experience with inhaled insulin, are not explicitly mentioned, causing a slight disconnect with the true answer.\n",
      "\n",
      "- **Completeness (4.0):** The response is largely complete, addressing most of the questions asked. It systematically covers different insulin delivery methods, learning curves, effects of repeated injections, use of ports, inhaled insulin, and transplant procedures. However, the model misses some nuance found in the true answer, such as discussing the cost and patient education for pumps, and the specifics on when entire pancreas transplants are considered over islet cell transplants.\n",
      "\n",
      "- **Naturalness (4.8):** The language used in the model's response is fluent and human-like, providing clear and understandable information. The style is professional yet accessible, making it suitable for a patient inquiry. Minor improvements could include a more personalized tone or some expressions of empathy typical in patient-doctor communication.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model’s response is coherent and logically aligns with the context provided. It acknowledges the user’s concerns about dry skin and offers a plausible explanation relating to diabetes, which fits within the context of the user being a diabetes patient.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The response provides a sufficient answer by addressing the possibility of diabetes-related dry skin and suggests practical steps, such as using a gentle moisturizer. However, it does not address the specific issue of the cat scratch and the red patch effectively, which might have been an important aspect of the user's query.  \n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent and human-like, utilizing empathetic language that is characteristic of a natural conversation. The advice flows well and maintains a professional tone consistent with how a healthcare provider would respond to a patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context. It addresses the possibility of having a high vacuolated neutrophil result with an infection that doesn't show worsening symptoms immediately, and mentions other conditions that could cause such a result.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  The model's response provides a sufficient answer, listing examples of infections that might not worsen symptoms quickly and explaining other conditions indicated by high vacuolated neutrophils. However, it misses the detail about interpreting these results in conjunction with other blood parameters, as emphasized in the true answer.\n",
      "\n",
      "- Naturalness: 4.8  \n",
      "  The response is fluent and sounds human-like, effectively conveying the intended information in a clear and professional manner suitable for a medical context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response logically aligns with the context provided, discussing elevated M2-PK levels and the need for further consultation with a healthcare provider. However, it might have benefitted from acknowledging the specific implication of a 150 level in more detail.\n",
      "  \n",
      "- Completeness: 3.5  \n",
      "  The response adequately answers the question by explaining the need for further testing and a healthcare provider's involvement. However, it lacks the depth present in the true answer, such as mentioning non-cancerous conditions that can cause elevated M2-PK levels and suggesting specific next steps like a fecal test or other screenings.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The response is fluent and sounds human-like. It is polite, reassuring, and professional in tone, similar to how a healthcare provider might respond to a concerned patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  The model's response is largely coherent and logically aligns with the context provided in the input. It correctly identifies concerns regarding thyroid function and relates these to the symptoms mentioned by the user. However, it could have broadened the consideration to include other potential causes of the symptoms, as mentioned in the true answer.\n",
      "\n",
      "- Completeness: 3.5  \n",
      "  The response partially answers the question by linking symptoms to possible thyroid dysfunction after gland removal. However, it misses discussing other potential causes of the symptoms such as calcium or vitamin D deficiency or other health issues that were suggested in the true answer. Thus, it lacks a comprehensive exploration of the user's situation.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  The model's response is fluent and sounds human-like. It uses empathetic and clear language, making the explanation easily understandable and reassuring for the user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 4.5  \n",
      "  - The model's response logically aligns with the context provided. It recognizes the user's ongoing struggle with anxiety and the need for professional evaluation and possible adjustment of current treatments. However, it does not inquire specifically about the use of Olanzapine for anxiety, which is a point brought up in the True Answer.\n",
      "\n",
      "- Completeness: 4.0  \n",
      "  - The response provides substantial advice on the next steps, including consulting with a psychiatrist and considering therapy and lifestyle changes. However, it lacks specific guidance on the medications mentioned, such as questioning the use of Olanzapine or suggesting alternative medications, which is something highlighted in the True Answer.\n",
      "\n",
      "- Naturalness: 5.0  \n",
      "  - The response is fluent and human-like, employing empathetic and professional language that is appropriate for a healthcare setting. The advice is communicated clearly and concisely.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "- Coherence: 5.0\n",
      "  - The model's response is logically aligned with the context provided by recognizing the issue of hair loss and suggesting an appropriate course of action—consulting a healthcare professional. It acknowledges various potential causes of hair loss, which is consistent with the subject matter.\n",
      "\n",
      "- Completeness: 3.5\n",
      "  - While the model's response covers the general causes of hair loss and suggests consulting a healthcare provider, it lacks specific actionable advice or over-the-counter treatment options that were present in the true answer, such as specific tests or products. \n",
      "\n",
      "- Naturalness: 5.0\n",
      "  - The model's response is fluent and sounds human-like. It maintains a professional tone appropriate for a healthcare context and uses naturally flowing language.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       4.476667\n",
      "Completeness    3.823333\n",
      "Naturalness     4.856667\n",
      "BLEURT          0.657181\n",
      "BERTScore_F1    0.558139\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# generation\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "generation = df[df['task'] == 'generation']\n",
    "results = []\n",
    "\n",
    "for _, row in generation.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    print(f\"Evaluation result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column BLEURT not found in DataFrame. Skipping normalization.\n",
      "Warning: Column BERTScore_F1 not found in DataFrame. Skipping normalization.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Coherence', 'Completeness', 'Naturalness', 'BLEURT', 'BERTScore_F1'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m normalize_scores(evaluation_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEURT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m evaluation_df \u001b[38;5;241m=\u001b[39m normalize_scores(evaluation_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore_F1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m average_scores \u001b[38;5;241m=\u001b[39m evaluation_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleteness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaturalness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEURT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore_F1\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m평균 점수:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Coherence', 'Completeness', 'Naturalness', 'BLEURT', 'BERTScore_F1'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# daily_diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "daily = df[df['task'] == 'daily_diets']\n",
    "results = []\n",
    "\n",
    "for i, row in daily.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_outpu_1024'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Evaluation {i+1} result:\\n{evaluation}\")\n",
    "    \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 점수:\n",
      "Coherence       3.223577\n",
      "Completeness    2.880081\n",
      "Naturalness     3.008130\n",
      "BLEURT          0.488236\n",
      "BERTScore_F1    0.627131\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# alternative_diets\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# BLEURT 모델 로드\n",
    "bleurt_model_name = \"Elron/bleurt-large-512\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bleurt_model_name)\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(bleurt_model_name)\n",
    "bleurt_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleurt_model = bleurt_model.to(device)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 8192\n",
    "\n",
    "# GPT-4 평가 함수\n",
    "def evaluate_with_gpt4(input_text, model_output, true_output):\n",
    "    input_text = str(input_text)[:MAX_CONTEXT_LENGTH]\n",
    "    model_output = str(model_output)[:MAX_CONTEXT_LENGTH]\n",
    "    true_output = str(true_output)[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with evaluating the quality of a QA model's responses based on the following metrics:\n",
    "    1. **Coherence**: Does the model's response logically align with the context provided in the input?\n",
    "    2. **Completeness**: Does the model's response sufficiently answer the question in the input?\n",
    "    3. **Naturalness**: Does the model's response sound fluent and human-like?\n",
    "\n",
    "    **Input**:\n",
    "    {input_text}\n",
    "\n",
    "    **Model's Response**:\n",
    "    {model_output}\n",
    "\n",
    "    **True Answer**:\n",
    "    {true_output}\n",
    "\n",
    "    Please rate each metric on a scale from 1 to 5. \n",
    "    Example response format:\n",
    "    - Coherence: X.X\n",
    "    - Completeness: X.X\n",
    "    - Naturalness: X.X\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for Summarization models.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error with GPT-4 API:\", e)\n",
    "        return None\n",
    "\n",
    "# GPT-4 점수 추출\n",
    "def extract_scores(evaluation):\n",
    "    if evaluation is None:\n",
    "        return {\"Coherence\": 0.0, \"Completeness\": 0.0, \"Naturalness\": 0.0}\n",
    "    coherence = re.search(r\"Coherence: (\\d\\.\\d)\", evaluation)\n",
    "    completeness = re.search(r\"Completeness: (\\d\\.\\d)\", evaluation)\n",
    "    naturalness = re.search(r\"Naturalness: (\\d\\.\\d)\", evaluation)\n",
    "    \n",
    "    return {\n",
    "        \"Coherence\": float(coherence.group(1)) if coherence else 0.0,\n",
    "        \"Completeness\": float(completeness.group(1)) if completeness else 0.0,\n",
    "        \"Naturalness\": float(naturalness.group(1)) if naturalness else 0.0\n",
    "    }\n",
    "\n",
    "# BLEURT 점수 계산\n",
    "def calculate_bleurt(y_true, y_pred):\n",
    "    inputs = tokenizer(y_pred, y_true, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = bleurt_model(**inputs).logits\n",
    "\n",
    "    if scores.numel() == 1:\n",
    "        return float(scores.squeeze().item())  \n",
    "    return [float(score) for score in scores.squeeze().tolist()]  \n",
    "\n",
    "# BLEURT 및 BERTScore 계산\n",
    "def calculate_bleurt_and_bertscore(y_true, y_pred):\n",
    "    bleurt_score_value = calculate_bleurt(y_true, y_pred)\n",
    "    _, _, bert_f1 = bert_score(y_pred, y_true, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_f1_avg = sum(bert_f1) / len(bert_f1) if len(bert_f1) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"BLEURT\": bleurt_score_value if isinstance(bleurt_score_value, float) else sum(bleurt_score_value) / len(bleurt_score_value),\n",
    "        \"BERTScore_F1\": bert_f1_avg\n",
    "    }\n",
    "\n",
    "# 점수 정규화 함수\n",
    "def normalize_scores(df, column):\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column {column} not found in DataFrame. Skipping normalization.\")\n",
    "        return df\n",
    "    df[column] = df[column].apply(lambda x: float(x) if isinstance(x, torch.Tensor) else x)\n",
    "    min_val, max_val = df[column].min(), df[column].max()\n",
    "    df[column] = df[column].apply(lambda x: (x - min_val) / (max_val - min_val) if max_val > min_val else 0.5)\n",
    "    return df\n",
    "\n",
    "alternative = df[df['task'] == 'alternative_diet']\n",
    "results = []\n",
    "\n",
    "for _, row in alternative.iterrows():\n",
    "    input_text = str(row['input'])\n",
    "    model_output = str(row['model_output'])\n",
    "    true_output = str(row['output'])\n",
    "    \n",
    "    evaluation = evaluate_with_gpt4(input_text, model_output, true_output)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Evaluation {i+1} result:\\n{evaluation}\")\n",
    "            \n",
    "    scores = extract_scores(evaluation)\n",
    "    metric_scores = calculate_bleurt_and_bertscore([true_output], [model_output])\n",
    "\n",
    "    results.append({\n",
    "        \"input\": input_text,\n",
    "        \"model_output\": model_output,\n",
    "        \"true_output\": true_output,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"Coherence\": scores[\"Coherence\"],\n",
    "        \"Completeness\": scores[\"Completeness\"],\n",
    "        \"Naturalness\": scores[\"Naturalness\"],\n",
    "        \"BLEURT\": metric_scores.get(\"BLEURT\", 0.0),  # 기본값 설정\n",
    "        \"BERTScore_F1\": metric_scores.get(\"BERTScore_F1\", 0.0)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "\n",
    "# 'BLEURT' 컬럼이 존재하는지 확인 후 정규화 수행\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BLEURT\")\n",
    "evaluation_df = normalize_scores(evaluation_df, \"BERTScore_F1\")\n",
    "\n",
    "average_scores = evaluation_df[[\"Coherence\", \"Completeness\", \"Naturalness\", \"BLEURT\", \"BERTScore_F1\"]].mean()\n",
    "print(\"평균 점수:\")\n",
    "print(average_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>cook_time</th>\n",
       "      <th>servings</th>\n",
       "      <th>steps</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition_facts</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark</td>\n",
       "      <td>Raspberry Swirl Frozen Yogurt Bark: Dive into ...</td>\n",
       "      <td>10 min</td>\n",
       "      <td>4 hr</td>\n",
       "      <td>6 Servings</td>\n",
       "      <td>['Cover a freezer-safe tray with parchment pap...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...</td>\n",
       "      <td>{'Servings': '6 Servings', 'Serving Size': '1 ...</td>\n",
       "      <td>[{'label': 'Plain Nonfat Greek yogurt', 'us_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple-Pumpkin Spice Oatmeal Cookies</td>\n",
       "      <td>Description not found</td>\n",
       "      <td>10 min</td>\n",
       "      <td>25 min</td>\n",
       "      <td>14 Servings</td>\n",
       "      <td>['Preheat the oven to 350 degrees F. Line two ...</td>\n",
       "      <td>['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...</td>\n",
       "      <td>{'Servings': '14 Servings', 'Serving Size': '1...</td>\n",
       "      <td>[{'label': 'old-fashioned rolled oats', 'us_me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0   Raspberry Swirl Frozen Yogurt Bark   \n",
       "1  Maple-Pumpkin Spice Oatmeal Cookies   \n",
       "\n",
       "                                         description prep_time cook_time  \\\n",
       "0  Raspberry Swirl Frozen Yogurt Bark: Dive into ...    10 min      4 hr   \n",
       "1                              Description not found    10 min    25 min   \n",
       "\n",
       "      servings                                              steps  \\\n",
       "0   6 Servings  ['Cover a freezer-safe tray with parchment pap...   \n",
       "1  14 Servings  ['Preheat the oven to 350 degrees F. Line two ...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Kid Friendly', 'Vegetarian', 'Dessert', 'Sna...   \n",
       "1  ['Kid Friendly', 'Vegetarian', 'Snacks', 'Glut...   \n",
       "\n",
       "                                     nutrition_facts  \\\n",
       "0  {'Servings': '6 Servings', 'Serving Size': '1 ...   \n",
       "1  {'Servings': '14 Servings', 'Serving Size': '1...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [{'label': 'Plain Nonfat Greek yogurt', 'us_me...  \n",
       "1  [{'label': 'old-fashioned rolled oats', 'us_me...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfh = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data2_daily_diets/diabetes_food_hub_new_nutri_facts.csv\")\n",
    "dfh.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:00<00:00, 22063.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for Each Row ===\n",
      "Row Index: 661\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 662\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 663\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 664\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 665\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 666\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 667\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 668\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 669\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 670\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 671\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 672\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 673\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 674\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 675\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 676\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 677\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 678\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 679\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 680\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 681\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 682\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 683\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 684\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 685\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 686\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 687\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 688\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 689\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 690\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 691\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 692\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 693\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 694\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 695\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 696\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 697\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 698\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 699\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 700\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 701\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 702\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 703\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 704\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 705\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 706\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 707\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 708\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 709\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 710\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 711\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 712\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 713\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 714\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 715\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 716\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 717\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 718\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 719\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 720\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 721\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 722\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 723\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 724\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 725\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 726\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 727\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 728\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 729\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 730\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 731\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 732\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 733\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 734\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 735\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 736\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 737\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 738\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 739\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 740\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 741\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 742\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 743\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 744\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 745\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 746\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 747\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 748\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 749\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 750\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 751\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 752\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 753\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 754\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 755\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 756\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 757\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 758\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 759\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 760\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 761\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 762\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 763\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 764\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 765\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 766\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 767\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 768\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 769\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 770\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 771\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 772\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 773\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 774\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 775\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 776\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 777\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 778\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 779\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 780\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 781\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 782\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 783\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 784\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 785\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 786\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 787\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 788\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 789\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 790\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 791\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 792\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 793\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 794\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 795\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 796\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 797\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 798\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 799\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 800\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 801\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 802\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 803\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 804\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 805\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 806\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 807\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 808\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 809\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 810\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 811\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 812\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 813\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 814\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 815\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 816\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 817\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 818\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 819\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 820\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 821\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 822\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 823\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 824\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 825\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 826\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 827\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 828\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 829\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 830\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 831\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 832\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 833\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 834\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 835\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 836\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 837\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 838\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 839\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 840\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 841\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 842\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 843\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 844\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 845\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 846\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 847\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 848\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 849\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 850\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 851\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 852\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 853\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 854\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 855\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 856\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 857\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 858\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 859\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 860\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 861\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 862\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 863\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 864\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 865\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 866\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 867\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 868\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 869\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 870\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 871\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 872\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 873\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 874\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 875\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 876\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 877\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 878\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 879\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 880\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 881\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 882\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 883\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 884\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 885\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 886\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 887\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 888\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 889\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 890\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 891\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 892\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 893\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 894\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 895\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 896\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 897\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 898\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 899\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 900\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 901\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 902\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 903\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 904\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 905\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 906\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 907\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 908\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 909\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 910\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 911\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 912\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 913\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 914\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 915\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 916\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 917\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 918\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 919\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 920\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 921\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 922\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 923\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 924\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 925\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 926\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 927\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 928\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 929\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 930\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 931\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 932\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 933\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "Row Index: 934\n",
      "Output Scores: {}\n",
      "Model Output Scores: {}\n",
      "\n",
      "=== Overall Averages ===\n",
      "Output Average Nutri-Score: None\n",
      "Model Output Average Nutri-Score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# daily diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return all(key in data for key in ['Breakfast', 'Lunch', 'Dinner'])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # 과일/채소 비율을 100g 기준으로 변환\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data) if isinstance(data, (int, float, str)) else default\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # 전체 무게 계산\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g 기준으로 성분 정규화\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_meal_nutri_score(meal_data, dfh):\n",
    "    meal_scores = {}\n",
    "\n",
    "    for meal, title in meal_data.items():\n",
    "        matched_row = find_most_similar_row(title, dfh)\n",
    "        if matched_row is None:\n",
    "            continue\n",
    "\n",
    "        nutrition_facts = matched_row['nutrition_facts']\n",
    "        ingredients = matched_row['ingredients']\n",
    "        score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "\n",
    "        if score is None:\n",
    "            print(f\"Warning: Nutri-Score calculation failed for meal '{meal}' with title '{title}'.\")\n",
    "            grade = \"N/A\"\n",
    "        else:\n",
    "            grade = get_nutri_score_grade(score)\n",
    "\n",
    "        meal_scores[meal] = {'score': score, 'grade': grade}\n",
    "\n",
    "    return meal_scores\n",
    "\n",
    "def calculate_scores_with_comparison(df, dfh):\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        output_scores = {}\n",
    "        model_scores = {}\n",
    "        if is_valid_meal_structure(row.get('output', '')):\n",
    "            output_data = json.loads(row['output'])\n",
    "            output_scores = calculate_meal_nutri_score(output_data, dfh)\n",
    "        if is_valid_meal_structure(row.get('model_output_512', '')):\n",
    "            model_data = json.loads(row['model_output_512'])\n",
    "            model_scores = calculate_meal_nutri_score(model_data, dfh)\n",
    "        results.append({'row_index': idx, 'output_scores': output_scores, 'model_scores': model_scores})\n",
    "    return results\n",
    "\n",
    "def calculate_average_scores(results):\n",
    "    \"\"\"\n",
    "    Calculate the average Nutri-Scores for outputs and model outputs.\n",
    "    \"\"\"\n",
    "    output_total_score = 0\n",
    "    model_total_score = 0\n",
    "    output_count = 0\n",
    "    model_count = 0\n",
    "\n",
    "    for result in results:\n",
    "        # Extract output scores\n",
    "        for meal, score_data in result['output_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                output_total_score += score_data['score']\n",
    "                output_count += 1\n",
    "\n",
    "        # Extract model scores\n",
    "        for meal, score_data in result['model_scores'].items():\n",
    "            if score_data['score'] is not None:\n",
    "                model_total_score += score_data['score']\n",
    "                model_count += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    output_avg = output_total_score / output_count if output_count > 0 else None\n",
    "    model_avg = model_total_score / model_count if model_count > 0 else None\n",
    "\n",
    "    return output_avg, model_avg\n",
    "\n",
    "\n",
    "# 'daily_diets' task Nutri-Score calculation\n",
    "filtered_df = df[df['task'] == 'daily_diets']\n",
    "results = calculate_scores_with_comparison(filtered_df, dfh)\n",
    "\n",
    "# Calculate overall averages\n",
    "output_avg, model_avg = calculate_average_scores(results)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Results for Each Row ===\")\n",
    "for result in results:\n",
    "    print(f\"Row Index: {result['row_index']}\")\n",
    "    print(f\"Output Scores: {result['output_scores']}\")\n",
    "    print(f\"Model Output Scores: {result['model_scores']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Overall Averages ===\")\n",
    "print(f\"Output Average Nutri-Score: {output_avg}\")\n",
    "print(f\"Model Output Average Nutri-Score: {model_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 158/246 [03:34<01:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error identifying fruits and vegetables: unterminated string literal (detected at line 1) (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 207/246 [04:37<00:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Total weight is zero. Skipping calculation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [05:27<00:00,  1.33s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 216\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Execution\u001b[39;00m\n\u001b[1;32m    215\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malternative_diet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_scores_with_comparison_no_meals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[32], line 207\u001b[0m, in \u001b[0;36mcalculate_scores_with_comparison_no_meals\u001b[0;34m(df, dfh)\u001b[0m\n\u001b[1;32m    200\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    201\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_index\u001b[39m\u001b[38;5;124m'\u001b[39m: idx,\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         })\n\u001b[1;32m    206\u001b[0m final_output_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(output_scores_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(output_scores_list) \u001b[38;5;28;01mif\u001b[39;00m output_scores_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m final_model_output_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_scores_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_output_scores_list) \u001b[38;5;28;01mif\u001b[39;00m model_output_scores_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Average Nutri-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_output_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Output Average Nutri-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_output_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# alternative diet - nutri score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_numeric_value: {e}, value: {value}\")\n",
    "    return 0.0\n",
    "\n",
    "def is_valid_meal_structure(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return isinstance(data, dict)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return False\n",
    "\n",
    "def find_most_similar_row(title, dfh):\n",
    "    try:\n",
    "        dfh['title'] = dfh['title'].fillna('')  # Handle NaN values\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(dfh['title'])\n",
    "        input_vector = vectorizer.transform([title])\n",
    "        similarities = cosine_similarity(input_vector, tfidf_matrix)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        return dfh.iloc[most_similar_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in find_most_similar_row: {e}, title: {title}\")\n",
    "        return None\n",
    "\n",
    "def identify_fruit_veg(ingredients_list):\n",
    "    try:\n",
    "        prompt = f\"Identify which items in the following ingredient list are fruits or vegetables:\\n\\n{ingredients_list}\\n\\nReturn only the names of items that are fruits or vegetables in a Python list format.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant identifying fruits and vegetables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0\n",
    "        )\n",
    "        fruits_vegetables = response['choices'][0]['message']['content']\n",
    "        return ast.literal_eval(fruits_vegetables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying fruits and vegetables: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_fruit_veg_points(ingredients, total_weight):\n",
    "    try:\n",
    "        ingredients_list = ast.literal_eval(ingredients)\n",
    "        fruit_veg_labels = identify_fruit_veg(ingredients_list)\n",
    "\n",
    "        fruit_veg_weight = 0\n",
    "        for ingredient in ingredients_list:\n",
    "            label = ingredient.get('label', '')\n",
    "            weight = extract_numeric_value(ingredient.get('metric_measure', 0))\n",
    "            if label in fruit_veg_labels:\n",
    "                fruit_veg_weight += weight\n",
    "\n",
    "        # 과일/채소 비율을 100g 기준으로 변환\n",
    "        fruit_veg_ratio = (fruit_veg_weight / total_weight) * 100 if total_weight > 0 else 0\n",
    "\n",
    "        if fruit_veg_ratio > 80:\n",
    "            return 5\n",
    "        elif fruit_veg_ratio > 60:\n",
    "            return 2\n",
    "        elif fruit_veg_ratio > 40:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating fruit_veg_points: {e}\")\n",
    "        return 0\n",
    "\n",
    "def extract_nested_value(data, keys, default=0):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict):\n",
    "                data = data.get(key, {})\n",
    "            else:\n",
    "                return default\n",
    "        return extract_numeric_value(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_nested_value: {e}, keys: {keys}, data: {data}\")\n",
    "        return default\n",
    "\n",
    "def calculate_nutri_score(nutrition_facts, ingredients):\n",
    "    try:\n",
    "        if isinstance(nutrition_facts, str):\n",
    "            nutrition_facts = ast.literal_eval(nutrition_facts)\n",
    "\n",
    "        # 전체 무게 계산\n",
    "        total_weight = sum(\n",
    "            extract_numeric_value(ingredient.get('metric_measure', 0)) \n",
    "            for ingredient in ast.literal_eval(ingredients)\n",
    "        )\n",
    "        if total_weight == 0:\n",
    "            print(\"Warning: Total weight is zero. Skipping calculation.\")\n",
    "            return None\n",
    "\n",
    "        # 100g 기준으로 성분 정규화\n",
    "        energy = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Calories']) / total_weight * 100\n",
    "        saturated_fat = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Fat', 'Amount']) / total_weight * 100\n",
    "        sugar = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Total Sugars']) / total_weight * 100\n",
    "        sodium = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Sodium']) / total_weight * 100\n",
    "        fiber = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Total Carbohydrates', 'Dietary Fiber']) / total_weight * 100\n",
    "        protein = extract_nested_value(nutrition_facts, ['Amount per Serving', 'Protein']) / total_weight * 100\n",
    "\n",
    "        # Unfavorable points calculation\n",
    "        energy_points = min(energy / 80, 800)\n",
    "        saturated_fat_points = min(saturated_fat / 1, 10)\n",
    "        sugar_points = min(sugar / 4.5, 45)\n",
    "        sodium_points = min(sodium / 90, 900)\n",
    "\n",
    "        unfavorable_points = energy_points + saturated_fat_points + sugar_points + sodium_points\n",
    "\n",
    "        # Favorable points calculation\n",
    "        fiber_points = min(fiber / 0.7, 3.5)\n",
    "        protein_points = min(protein / 1.6, 8.0)\n",
    "        fruit_veg_points = calculate_fruit_veg_points(ingredients, total_weight)\n",
    "\n",
    "        favorable_points = fiber_points + protein_points + fruit_veg_points\n",
    "\n",
    "        # Final Nutri-Score calculation\n",
    "        total_score = unfavorable_points - favorable_points\n",
    "        return total_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_nutri_score: {e}, nutrition_facts: {nutrition_facts}\")\n",
    "        return None\n",
    "\n",
    "def get_nutri_score_grade(score):\n",
    "    if score <= -1:\n",
    "        return \"A\"\n",
    "    elif score <= 2:\n",
    "        return \"B\"\n",
    "    elif score <= 10:\n",
    "        return \"C\"\n",
    "    elif score <= 18:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"E\"\n",
    "\n",
    "def calculate_scores_with_comparison_no_meals(df, dfh):\n",
    "    results = []\n",
    "    output_scores_list = []\n",
    "    model_output_scores_list = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            output_text = row.get('output', '')\n",
    "            if output_text:\n",
    "                matched_row = find_most_similar_row(output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    output_scores_list.append(output_score)\n",
    "                else:\n",
    "                    output_score = None\n",
    "\n",
    "            model_output_text = row.get('model_output_512', '')\n",
    "            if model_output_text:\n",
    "                matched_row = find_most_similar_row(model_output_text, dfh)\n",
    "                if matched_row is not None:\n",
    "                    nutrition_facts = matched_row['nutrition_facts']\n",
    "                    ingredients = matched_row['ingredients']\n",
    "                    model_output_score = calculate_nutri_score(nutrition_facts, ingredients)\n",
    "                    model_output_scores_list.append(model_output_score)\n",
    "                else:\n",
    "                    model_output_score = None\n",
    "\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': output_score,\n",
    "                'model_output_score': model_output_score\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            results.append({\n",
    "                'row_index': idx,\n",
    "                'output_score': None,\n",
    "                'model_output_score': None\n",
    "            })\n",
    "\n",
    "    final_output_avg = sum(output_scores_list) / len(output_scores_list) if output_scores_list else None\n",
    "    final_model_output_avg = sum(model_output_scores_list) / len(model_output_scores_list) if model_output_scores_list else None\n",
    "\n",
    "    print(f\"Output Average Nutri-Score: {final_output_avg}\")\n",
    "    print(f\"Model Output Average Nutri-Score: {final_model_output_avg}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Execution\n",
    "filtered_df = df[df['task'] == 'alternative_diet']\n",
    "results = calculate_scores_with_comparison_no_meals(filtered_df, dfh)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
