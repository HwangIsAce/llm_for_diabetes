{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571df5ac7c6e4435822df335db38fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff68e16f120470f88105acc466eb87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69e2ee6c1a24404ad7075341fd69715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1245b6583bba48f4bbf9b2069841aa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 3471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cherry = load_dataset(\"passionMan/dataset_cherry\")\n",
    "cherry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b09c806444e93bda7f5d96ac47df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af26999b4744055979d64ccc8017438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345eda50df174b2ea276b05eaeabf615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 30, # 50\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/model_cherry\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 3,471 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 327\n",
      " \"-____-\"     Number of trainable parameters = 9,437,184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 53:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.932600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce021eab8714bba8d7384ac41be8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3036503dcee14c3da49de2b8defce9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/575k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753d8ee0141440c6a4f4480cacef1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/240] Sample processed in 0.90s, ETA: 3.58 min\n",
      "[2/240] Sample processed in 0.39s, ETA: 2.55 min\n",
      "[3/240] Sample processed in 0.32s, ETA: 2.12 min\n",
      "[4/240] Sample processed in 0.39s, ETA: 1.96 min\n",
      "[5/240] Sample processed in 0.53s, ETA: 1.98 min\n",
      "[6/240] Sample processed in 0.45s, ETA: 1.93 min\n",
      "[7/240] Sample processed in 0.36s, ETA: 1.85 min\n",
      "[8/240] Sample processed in 0.64s, ETA: 1.92 min\n",
      "[9/240] Sample processed in 0.55s, ETA: 1.93 min\n",
      "[10/240] Sample processed in 0.28s, ETA: 1.84 min\n",
      "[11/240] Sample processed in 0.33s, ETA: 1.78 min\n",
      "[12/240] Sample processed in 4.48s, ETA: 3.04 min\n",
      "[13/240] Sample processed in 0.53s, ETA: 2.95 min\n",
      "[14/240] Sample processed in 0.54s, ETA: 2.87 min\n",
      "[15/240] Sample processed in 0.56s, ETA: 2.81 min\n",
      "[16/240] Sample processed in 0.32s, ETA: 2.70 min\n",
      "[17/240] Sample processed in 0.36s, ETA: 2.61 min\n",
      "[18/240] Sample processed in 0.38s, ETA: 2.53 min\n",
      "[19/240] Sample processed in 0.28s, ETA: 2.44 min\n",
      "[20/240] Sample processed in 0.25s, ETA: 2.35 min\n",
      "[21/240] Sample processed in 0.40s, ETA: 2.30 min\n",
      "[22/240] Sample processed in 0.42s, ETA: 2.26 min\n",
      "[23/240] Sample processed in 0.58s, ETA: 2.24 min\n",
      "[24/240] Sample processed in 0.25s, ETA: 2.17 min\n",
      "[25/240] Sample processed in 0.36s, ETA: 2.13 min\n",
      "[26/240] Sample processed in 0.28s, ETA: 2.08 min\n",
      "[27/240] Sample processed in 0.32s, ETA: 2.03 min\n",
      "[28/240] Sample processed in 0.32s, ETA: 1.99 min\n",
      "[29/240] Sample processed in 0.94s, ETA: 2.03 min\n",
      "[30/240] Sample processed in 0.58s, ETA: 2.02 min\n",
      "[31/240] Sample processed in 0.33s, ETA: 1.98 min\n",
      "[32/240] Sample processed in 0.44s, ETA: 1.96 min\n",
      "[33/240] Sample processed in 0.35s, ETA: 1.92 min\n",
      "[34/240] Sample processed in 0.49s, ETA: 1.91 min\n",
      "[35/240] Sample processed in 1.36s, ETA: 1.98 min\n",
      "[36/240] Sample processed in 0.39s, ETA: 1.95 min\n",
      "[37/240] Sample processed in 0.45s, ETA: 1.93 min\n",
      "[38/240] Sample processed in 0.32s, ETA: 1.90 min\n",
      "[39/240] Sample processed in 0.39s, ETA: 1.87 min\n",
      "[40/240] Sample processed in 0.39s, ETA: 1.85 min\n",
      "[41/240] Sample processed in 0.35s, ETA: 1.82 min\n",
      "[42/240] Sample processed in 0.25s, ETA: 1.79 min\n",
      "[43/240] Sample processed in 0.36s, ETA: 1.77 min\n",
      "[44/240] Sample processed in 0.77s, ETA: 1.78 min\n",
      "[45/240] Sample processed in 0.32s, ETA: 1.75 min\n",
      "[46/240] Sample processed in 0.56s, ETA: 1.74 min\n",
      "[47/240] Sample processed in 0.63s, ETA: 1.74 min\n",
      "[48/240] Sample processed in 0.35s, ETA: 1.72 min\n",
      "[49/240] Sample processed in 0.32s, ETA: 1.70 min\n",
      "[50/240] Sample processed in 0.52s, ETA: 1.69 min\n",
      "[51/240] Sample processed in 0.77s, ETA: 1.69 min\n",
      "[52/240] Sample processed in 0.76s, ETA: 1.70 min\n",
      "[53/240] Sample processed in 0.25s, ETA: 1.67 min\n",
      "[54/240] Sample processed in 0.53s, ETA: 1.66 min\n",
      "[55/240] Sample processed in 0.45s, ETA: 1.65 min\n",
      "[56/240] Sample processed in 0.25s, ETA: 1.62 min\n",
      "[57/240] Sample processed in 0.36s, ETA: 1.61 min\n",
      "[58/240] Sample processed in 0.28s, ETA: 1.58 min\n",
      "[59/240] Sample processed in 0.28s, ETA: 1.56 min\n",
      "[60/240] Sample processed in 0.32s, ETA: 1.54 min\n",
      "[61/240] Sample processed in 0.13s, ETA: 1.52 min\n",
      "[62/240] Sample processed in 0.17s, ETA: 1.49 min\n",
      "[63/240] Sample processed in 0.11s, ETA: 1.47 min\n",
      "[64/240] Sample processed in 0.13s, ETA: 1.44 min\n",
      "[65/240] Sample processed in 0.15s, ETA: 1.42 min\n",
      "[66/240] Sample processed in 0.15s, ETA: 1.39 min\n",
      "[67/240] Sample processed in 0.12s, ETA: 1.37 min\n",
      "[68/240] Sample processed in 0.12s, ETA: 1.35 min\n",
      "[69/240] Sample processed in 0.15s, ETA: 1.33 min\n",
      "[70/240] Sample processed in 0.15s, ETA: 1.31 min\n",
      "[71/240] Sample processed in 0.15s, ETA: 1.29 min\n",
      "[72/240] Sample processed in 0.17s, ETA: 1.27 min\n",
      "[73/240] Sample processed in 0.11s, ETA: 1.25 min\n",
      "[74/240] Sample processed in 0.11s, ETA: 1.23 min\n",
      "[75/240] Sample processed in 0.13s, ETA: 1.21 min\n",
      "[76/240] Sample processed in 0.11s, ETA: 1.19 min\n",
      "[77/240] Sample processed in 0.17s, ETA: 1.17 min\n",
      "[78/240] Sample processed in 0.16s, ETA: 1.16 min\n",
      "[79/240] Sample processed in 0.16s, ETA: 1.14 min\n",
      "[80/240] Sample processed in 0.14s, ETA: 1.12 min\n",
      "[81/240] Sample processed in 0.13s, ETA: 1.11 min\n",
      "[82/240] Sample processed in 0.14s, ETA: 1.09 min\n",
      "[83/240] Sample processed in 0.15s, ETA: 1.07 min\n",
      "[84/240] Sample processed in 0.12s, ETA: 1.06 min\n",
      "[85/240] Sample processed in 0.17s, ETA: 1.04 min\n",
      "[86/240] Sample processed in 0.14s, ETA: 1.03 min\n",
      "[87/240] Sample processed in 0.17s, ETA: 1.02 min\n",
      "[88/240] Sample processed in 0.13s, ETA: 1.00 min\n",
      "[89/240] Sample processed in 0.17s, ETA: 0.99 min\n",
      "[90/240] Sample processed in 0.15s, ETA: 0.98 min\n",
      "[91/240] Sample processed in 0.10s, ETA: 0.96 min\n",
      "[92/240] Sample processed in 0.18s, ETA: 0.95 min\n",
      "[93/240] Sample processed in 0.17s, ETA: 0.94 min\n",
      "[94/240] Sample processed in 0.18s, ETA: 0.93 min\n",
      "[95/240] Sample processed in 0.17s, ETA: 0.91 min\n",
      "[96/240] Sample processed in 0.18s, ETA: 0.90 min\n",
      "[97/240] Sample processed in 0.18s, ETA: 0.89 min\n",
      "[98/240] Sample processed in 0.18s, ETA: 0.88 min\n",
      "[99/240] Sample processed in 0.17s, ETA: 0.87 min\n",
      "[100/240] Sample processed in 0.18s, ETA: 0.86 min\n",
      "[101/240] Sample processed in 0.11s, ETA: 0.85 min\n",
      "[102/240] Sample processed in 0.17s, ETA: 0.84 min\n",
      "[103/240] Sample processed in 0.18s, ETA: 0.83 min\n",
      "[104/240] Sample processed in 0.11s, ETA: 0.82 min\n",
      "[105/240] Sample processed in 0.11s, ETA: 0.80 min\n",
      "[106/240] Sample processed in 0.17s, ETA: 0.79 min\n",
      "[107/240] Sample processed in 0.18s, ETA: 0.78 min\n",
      "[108/240] Sample processed in 0.18s, ETA: 0.77 min\n",
      "[109/240] Sample processed in 0.18s, ETA: 0.77 min\n",
      "[110/240] Sample processed in 0.18s, ETA: 0.76 min\n",
      "[111/240] Sample processed in 0.18s, ETA: 0.75 min\n",
      "[112/240] Sample processed in 0.18s, ETA: 0.74 min\n",
      "[113/240] Sample processed in 0.11s, ETA: 0.73 min\n",
      "[114/240] Sample processed in 0.18s, ETA: 0.72 min\n",
      "[115/240] Sample processed in 0.18s, ETA: 0.71 min\n",
      "[116/240] Sample processed in 0.18s, ETA: 0.70 min\n",
      "[117/240] Sample processed in 0.11s, ETA: 0.69 min\n",
      "[118/240] Sample processed in 0.11s, ETA: 0.68 min\n",
      "[119/240] Sample processed in 0.18s, ETA: 0.67 min\n",
      "[120/240] Sample processed in 0.18s, ETA: 0.67 min\n",
      "[121/240] Sample processed in 3.97s, ETA: 0.72 min\n",
      "[122/240] Sample processed in 0.66s, ETA: 0.72 min\n",
      "[123/240] Sample processed in 4.47s, ETA: 0.78 min\n",
      "[124/240] Sample processed in 4.47s, ETA: 0.83 min\n",
      "[125/240] Sample processed in 4.50s, ETA: 0.89 min\n",
      "[126/240] Sample processed in 4.46s, ETA: 0.94 min\n",
      "[127/240] Sample processed in 0.69s, ETA: 0.94 min\n",
      "[128/240] Sample processed in 4.16s, ETA: 0.98 min\n",
      "[129/240] Sample processed in 0.87s, ETA: 0.98 min\n",
      "[130/240] Sample processed in 4.26s, ETA: 1.02 min\n",
      "[131/240] Sample processed in 2.66s, ETA: 1.04 min\n",
      "[132/240] Sample processed in 4.49s, ETA: 1.09 min\n",
      "[133/240] Sample processed in 0.83s, ETA: 1.08 min\n",
      "[134/240] Sample processed in 4.26s, ETA: 1.12 min\n",
      "[135/240] Sample processed in 4.49s, ETA: 1.16 min\n",
      "[136/240] Sample processed in 4.52s, ETA: 1.19 min\n",
      "[137/240] Sample processed in 3.80s, ETA: 1.22 min\n",
      "[138/240] Sample processed in 4.51s, ETA: 1.26 min\n",
      "[139/240] Sample processed in 4.53s, ETA: 1.29 min\n",
      "[140/240] Sample processed in 4.55s, ETA: 1.32 min\n",
      "[141/240] Sample processed in 4.53s, ETA: 1.35 min\n",
      "[142/240] Sample processed in 4.54s, ETA: 1.38 min\n",
      "[143/240] Sample processed in 3.52s, ETA: 1.40 min\n",
      "[144/240] Sample processed in 4.52s, ETA: 1.42 min\n",
      "[145/240] Sample processed in 4.52s, ETA: 1.45 min\n",
      "[146/240] Sample processed in 2.01s, ETA: 1.45 min\n",
      "[147/240] Sample processed in 4.46s, ETA: 1.47 min\n",
      "[148/240] Sample processed in 4.53s, ETA: 1.49 min\n",
      "[149/240] Sample processed in 2.29s, ETA: 1.49 min\n",
      "[150/240] Sample processed in 4.53s, ETA: 1.51 min\n",
      "[151/240] Sample processed in 0.18s, ETA: 1.48 min\n",
      "[152/240] Sample processed in 0.19s, ETA: 1.46 min\n",
      "[153/240] Sample processed in 0.18s, ETA: 1.43 min\n",
      "[154/240] Sample processed in 0.19s, ETA: 1.41 min\n",
      "[155/240] Sample processed in 0.19s, ETA: 1.38 min\n",
      "[156/240] Sample processed in 0.18s, ETA: 1.36 min\n",
      "[157/240] Sample processed in 0.18s, ETA: 1.34 min\n",
      "[158/240] Sample processed in 0.22s, ETA: 1.32 min\n",
      "[159/240] Sample processed in 0.19s, ETA: 1.29 min\n",
      "[160/240] Sample processed in 0.18s, ETA: 1.27 min\n",
      "[161/240] Sample processed in 0.19s, ETA: 1.25 min\n",
      "[162/240] Sample processed in 0.18s, ETA: 1.23 min\n",
      "[163/240] Sample processed in 0.18s, ETA: 1.20 min\n",
      "[164/240] Sample processed in 0.18s, ETA: 1.18 min\n",
      "[165/240] Sample processed in 0.22s, ETA: 1.16 min\n",
      "[166/240] Sample processed in 0.18s, ETA: 1.14 min\n",
      "[167/240] Sample processed in 0.18s, ETA: 1.12 min\n",
      "[168/240] Sample processed in 0.18s, ETA: 1.10 min\n",
      "[169/240] Sample processed in 0.18s, ETA: 1.08 min\n",
      "[170/240] Sample processed in 0.18s, ETA: 1.06 min\n",
      "[171/240] Sample processed in 0.18s, ETA: 1.04 min\n",
      "[172/240] Sample processed in 0.18s, ETA: 1.02 min\n",
      "[173/240] Sample processed in 0.18s, ETA: 1.00 min\n",
      "[174/240] Sample processed in 0.18s, ETA: 0.98 min\n",
      "[175/240] Sample processed in 0.18s, ETA: 0.96 min\n",
      "[176/240] Sample processed in 0.18s, ETA: 0.94 min\n",
      "[177/240] Sample processed in 0.18s, ETA: 0.92 min\n",
      "[178/240] Sample processed in 0.18s, ETA: 0.90 min\n",
      "[179/240] Sample processed in 0.18s, ETA: 0.89 min\n",
      "[180/240] Sample processed in 0.18s, ETA: 0.87 min\n",
      "[181/240] Sample processed in 22.81s, ETA: 0.97 min\n",
      "[182/240] Sample processed in 7.15s, ETA: 0.99 min\n",
      "[183/240] Sample processed in 11.49s, ETA: 1.03 min\n",
      "[184/240] Sample processed in 22.73s, ETA: 1.12 min\n",
      "[185/240] Sample processed in 9.74s, ETA: 1.14 min\n",
      "[186/240] Sample processed in 7.75s, ETA: 1.15 min\n",
      "[187/240] Sample processed in 7.85s, ETA: 1.16 min\n",
      "[188/240] Sample processed in 8.80s, ETA: 1.17 min\n",
      "[189/240] Sample processed in 7.54s, ETA: 1.18 min\n",
      "[190/240] Sample processed in 15.22s, ETA: 1.22 min\n",
      "[191/240] Sample processed in 6.91s, ETA: 1.21 min\n",
      "[192/240] Sample processed in 5.80s, ETA: 1.21 min\n",
      "[193/240] Sample processed in 7.97s, ETA: 1.21 min\n",
      "[194/240] Sample processed in 8.62s, ETA: 1.21 min\n",
      "[195/240] Sample processed in 6.65s, ETA: 1.20 min\n",
      "[196/240] Sample processed in 7.75s, ETA: 1.20 min\n",
      "[197/240] Sample processed in 7.39s, ETA: 1.19 min\n",
      "[198/240] Sample processed in 9.23s, ETA: 1.19 min\n",
      "[199/240] Sample processed in 10.29s, ETA: 1.19 min\n",
      "[200/240] Sample processed in 13.32s, ETA: 1.20 min\n",
      "[201/240] Sample processed in 7.52s, ETA: 1.19 min\n",
      "[202/240] Sample processed in 5.36s, ETA: 1.17 min\n",
      "[203/240] Sample processed in 6.18s, ETA: 1.15 min\n",
      "[204/240] Sample processed in 7.91s, ETA: 1.14 min\n",
      "[205/240] Sample processed in 5.21s, ETA: 1.12 min\n",
      "[206/240] Sample processed in 8.65s, ETA: 1.11 min\n",
      "[207/240] Sample processed in 7.64s, ETA: 1.09 min\n",
      "[208/240] Sample processed in 17.14s, ETA: 1.09 min\n",
      "[209/240] Sample processed in 20.37s, ETA: 1.11 min\n",
      "[210/240] Sample processed in 5.51s, ETA: 1.08 min\n",
      "[211/240] Sample processed in 9.83s, ETA: 1.06 min\n",
      "[212/240] Sample processed in 6.21s, ETA: 1.03 min\n",
      "[213/240] Sample processed in 7.56s, ETA: 1.01 min\n",
      "[214/240] Sample processed in 6.36s, ETA: 0.98 min\n",
      "[215/240] Sample processed in 6.91s, ETA: 0.95 min\n",
      "[216/240] Sample processed in 12.42s, ETA: 0.93 min\n",
      "[217/240] Sample processed in 4.19s, ETA: 0.89 min\n",
      "[218/240] Sample processed in 19.91s, ETA: 0.88 min\n",
      "[219/240] Sample processed in 10.82s, ETA: 0.86 min\n",
      "[220/240] Sample processed in 4.85s, ETA: 0.82 min\n",
      "[221/240] Sample processed in 8.49s, ETA: 0.79 min\n",
      "[222/240] Sample processed in 7.88s, ETA: 0.75 min\n",
      "[223/240] Sample processed in 4.54s, ETA: 0.71 min\n",
      "[224/240] Sample processed in 5.02s, ETA: 0.68 min\n",
      "[225/240] Sample processed in 11.98s, ETA: 0.64 min\n",
      "[226/240] Sample processed in 12.73s, ETA: 0.61 min\n",
      "[227/240] Sample processed in 4.00s, ETA: 0.57 min\n",
      "[228/240] Sample processed in 6.45s, ETA: 0.53 min\n",
      "[229/240] Sample processed in 14.16s, ETA: 0.49 min\n",
      "[230/240] Sample processed in 6.17s, ETA: 0.45 min\n",
      "[231/240] Sample processed in 7.24s, ETA: 0.41 min\n",
      "[232/240] Sample processed in 6.52s, ETA: 0.37 min\n",
      "[233/240] Sample processed in 5.55s, ETA: 0.32 min\n",
      "[234/240] Sample processed in 25.78s, ETA: 0.29 min\n",
      "[235/240] Sample processed in 5.93s, ETA: 0.24 min\n",
      "[236/240] Sample processed in 4.20s, ETA: 0.19 min\n",
      "[237/240] Sample processed in 3.63s, ETA: 0.14 min\n",
      "[238/240] Sample processed in 6.05s, ETA: 0.10 min\n",
      "[239/240] Sample processed in 6.03s, ETA: 0.05 min\n",
      "[240/240] Sample processed in 4.19s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 11.64 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### ✅ Hugging Face에서 데이터 로드\n",
    "dataset_name = \"passionMan/test_dataset4\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split 로드\n",
    "\n",
    "### ✅ JSONL 저장 함수 (평가 결과 저장용)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### ✅ 모델 응답 생성 함수\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # ✅ 모델의 최대 입력 길이 가져오기 (보통 4096 또는 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # ✅ 입력 토큰 길이 확인\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # 🔥 입력이 너무 길면 최대 입력 길이에 맞게 자름\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # ✅ 생성 수행 (max_new_tokens을 적용)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # ✅ 생성 길이 적용\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ✅ 데이터 경로 설정 (결과 저장용)\n",
    "output_json_path = \"/data/jaesung/llm_for_diabetes/src/trial/model_response/model_output_cherry.jsonl\"\n",
    "\n",
    "# ✅ Task별 데이터 그룹화 (각 태스크별 0~29번 샘플 선택)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# ✅ 성능 평가할 데이터 생성 (각 태스크별 30개만 추출)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:30])  # 최대 30개 선택\n",
    "\n",
    "# ✅ 성능 평가 시작\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # ✅ 생성할 토큰 길이 설정 (생성 토큰 수 조절)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"re\"}  # 생성 32\n",
    "    medium_context_tasks = {\"ie\"} # 생성 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # 생성 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 128  # ✅ 생성 길이 128\n",
    "    elif task in medium_context_tasks:\n",
    "        max_new_tokens = 128\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 1024  # ✅ 생성 길이 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # 기본값\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial3/outputs/model_cherry/checkpoint-327\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetes\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "For a diabetes-friendly diet, it's important to focus on whole, unprocessed foods that are low in added sugars, salt, and unhealthy fats. Here are some key recommendations to help manage diabetes:\n",
      "\n",
      "1. **Vegetables**: Emphasize non-starchy vegetables like leafy greens, broccoli, bell peppers, and cucumbers. They are low in carbs and high in fiber, which helps regulate blood sugar levels.\n",
      "\n",
      "2. **Protein**: Include lean protein sources like poultry, fish, tofu, and legumes. Protein helps slow down the absorption of carbohydrates and keeps you full for longer.\n",
      "\n",
      "3. **Whole Grains\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Please recommend a diet for diabetes\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Context:\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite. Dice, chop, and stir—that’s all you need to create broccoli and apple salad. This healthy recipe is low in sodium and high in flavor. Caution: this recipe contains walnuts. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Veggie Rich', 'Kid Friendly', 'Salads', 'Vegetarian', 'Sides', 'Quick & Easy'])\n",
      "Chicken Nuggets with Honey Mustard Dipping Sauce: This recipe is not only delicious, but it’s also kidney-friendly. Try a variety of dipping sauces, such as barbecue sauce, curry sauce, fruit spreads, or low-sodium salad dressing. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "\n",
      "### Response: \n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad is perfect for diabetic patients as it combines lean protein from chicken with fiber-rich apples and whole grains. The salad is low in carbohydrates and high in fiber, which helps regulate blood sugar levels. To make it even more kidney-friendly, you can substitute the chicken with tofu or tempeh. The apple crunch topping adds natural sweetness without the need for added sugars. This salad is a great option for a quick and satisfying meal or snack that is also low in sodium and calories.\n",
      "\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a great option for diabetic patients as it\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# 2️⃣ 음식 데이터 로드 & FAISS 인덱스 로드\n",
    "df = pd.read_csv(\"processed_food_data.csv\")  # 음식 데이터 로드\n",
    "index = faiss.read_index(\"food_faiss.index\")  # FAISS 인덱스 로드\n",
    "embedding_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # 한국어 임베딩 모델\n",
    "\n",
    "# 3️⃣ 음식 검색 함수 (FAISS 사용)\n",
    "def search_food(query, top_k=3):\n",
    "    \"\"\"질문을 벡터화하여 FAISS에서 유사한 음식 검색\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # 검색된 음식 정보 추출\n",
    "    results = df.iloc[indices[0]][[\"title\", \"description\", \"tags\"]]\n",
    "    \n",
    "    # 검색된 음식 정보를 프롬프트용 문자열로 변환\n",
    "    search_context = \"\\n\".join([f\"{row['title']}: {row['description']} (tags: {row['tags']})\" for _, row in results.iterrows()])\n",
    "    return search_context\n",
    "\n",
    "# 4️⃣ 사용자 입력 및 검색 수행\n",
    "query = \"Please recommend a diet for diabetic patients.\"\n",
    "search_results = search_food(query)\n",
    "\n",
    "# 5️⃣ LLM 프롬프트 생성\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_text = alpaca_prompt.format(\n",
    "    \"Please recommend a diet for diabetic patients.\",\n",
    "    query,\n",
    "    search_results\n",
    ")\n",
    "\n",
    "# 6️⃣ LLM을 사용하여 검색된 정보 기반으로 답변 생성\n",
    "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
