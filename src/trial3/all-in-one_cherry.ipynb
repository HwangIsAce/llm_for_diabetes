{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571df5ac7c6e4435822df335db38fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff68e16f120470f88105acc466eb87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69e2ee6c1a24404ad7075341fd69715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1245b6583bba48f4bbf9b2069841aa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 3471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cherry = load_dataset(\"passionMan/dataset_cherry\")\n",
    "cherry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b09c806444e93bda7f5d96ac47df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af26999b4744055979d64ccc8017438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345eda50df174b2ea276b05eaeabf615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/3471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 30, # 50\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/model_cherry\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 3,471 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 327\n",
      " \"-____-\"     Number of trainable parameters = 9,437,184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 53:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.932600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce021eab8714bba8d7384ac41be8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3036503dcee14c3da49de2b8defce9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/575k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753d8ee0141440c6a4f4480cacef1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/240] Sample processed in 0.90s, ETA: 3.58 min\n",
      "[2/240] Sample processed in 0.39s, ETA: 2.55 min\n",
      "[3/240] Sample processed in 0.32s, ETA: 2.12 min\n",
      "[4/240] Sample processed in 0.39s, ETA: 1.96 min\n",
      "[5/240] Sample processed in 0.53s, ETA: 1.98 min\n",
      "[6/240] Sample processed in 0.45s, ETA: 1.93 min\n",
      "[7/240] Sample processed in 0.36s, ETA: 1.85 min\n",
      "[8/240] Sample processed in 0.64s, ETA: 1.92 min\n",
      "[9/240] Sample processed in 0.55s, ETA: 1.93 min\n",
      "[10/240] Sample processed in 0.28s, ETA: 1.84 min\n",
      "[11/240] Sample processed in 0.33s, ETA: 1.78 min\n",
      "[12/240] Sample processed in 4.48s, ETA: 3.04 min\n",
      "[13/240] Sample processed in 0.53s, ETA: 2.95 min\n",
      "[14/240] Sample processed in 0.54s, ETA: 2.87 min\n",
      "[15/240] Sample processed in 0.56s, ETA: 2.81 min\n",
      "[16/240] Sample processed in 0.32s, ETA: 2.70 min\n",
      "[17/240] Sample processed in 0.36s, ETA: 2.61 min\n",
      "[18/240] Sample processed in 0.38s, ETA: 2.53 min\n",
      "[19/240] Sample processed in 0.28s, ETA: 2.44 min\n",
      "[20/240] Sample processed in 0.25s, ETA: 2.35 min\n",
      "[21/240] Sample processed in 0.40s, ETA: 2.30 min\n",
      "[22/240] Sample processed in 0.42s, ETA: 2.26 min\n",
      "[23/240] Sample processed in 0.58s, ETA: 2.24 min\n",
      "[24/240] Sample processed in 0.25s, ETA: 2.17 min\n",
      "[25/240] Sample processed in 0.36s, ETA: 2.13 min\n",
      "[26/240] Sample processed in 0.28s, ETA: 2.08 min\n",
      "[27/240] Sample processed in 0.32s, ETA: 2.03 min\n",
      "[28/240] Sample processed in 0.32s, ETA: 1.99 min\n",
      "[29/240] Sample processed in 0.94s, ETA: 2.03 min\n",
      "[30/240] Sample processed in 0.58s, ETA: 2.02 min\n",
      "[31/240] Sample processed in 0.33s, ETA: 1.98 min\n",
      "[32/240] Sample processed in 0.44s, ETA: 1.96 min\n",
      "[33/240] Sample processed in 0.35s, ETA: 1.92 min\n",
      "[34/240] Sample processed in 0.49s, ETA: 1.91 min\n",
      "[35/240] Sample processed in 1.36s, ETA: 1.98 min\n",
      "[36/240] Sample processed in 0.39s, ETA: 1.95 min\n",
      "[37/240] Sample processed in 0.45s, ETA: 1.93 min\n",
      "[38/240] Sample processed in 0.32s, ETA: 1.90 min\n",
      "[39/240] Sample processed in 0.39s, ETA: 1.87 min\n",
      "[40/240] Sample processed in 0.39s, ETA: 1.85 min\n",
      "[41/240] Sample processed in 0.35s, ETA: 1.82 min\n",
      "[42/240] Sample processed in 0.25s, ETA: 1.79 min\n",
      "[43/240] Sample processed in 0.36s, ETA: 1.77 min\n",
      "[44/240] Sample processed in 0.77s, ETA: 1.78 min\n",
      "[45/240] Sample processed in 0.32s, ETA: 1.75 min\n",
      "[46/240] Sample processed in 0.56s, ETA: 1.74 min\n",
      "[47/240] Sample processed in 0.63s, ETA: 1.74 min\n",
      "[48/240] Sample processed in 0.35s, ETA: 1.72 min\n",
      "[49/240] Sample processed in 0.32s, ETA: 1.70 min\n",
      "[50/240] Sample processed in 0.52s, ETA: 1.69 min\n",
      "[51/240] Sample processed in 0.77s, ETA: 1.69 min\n",
      "[52/240] Sample processed in 0.76s, ETA: 1.70 min\n",
      "[53/240] Sample processed in 0.25s, ETA: 1.67 min\n",
      "[54/240] Sample processed in 0.53s, ETA: 1.66 min\n",
      "[55/240] Sample processed in 0.45s, ETA: 1.65 min\n",
      "[56/240] Sample processed in 0.25s, ETA: 1.62 min\n",
      "[57/240] Sample processed in 0.36s, ETA: 1.61 min\n",
      "[58/240] Sample processed in 0.28s, ETA: 1.58 min\n",
      "[59/240] Sample processed in 0.28s, ETA: 1.56 min\n",
      "[60/240] Sample processed in 0.32s, ETA: 1.54 min\n",
      "[61/240] Sample processed in 0.13s, ETA: 1.52 min\n",
      "[62/240] Sample processed in 0.17s, ETA: 1.49 min\n",
      "[63/240] Sample processed in 0.11s, ETA: 1.47 min\n",
      "[64/240] Sample processed in 0.13s, ETA: 1.44 min\n",
      "[65/240] Sample processed in 0.15s, ETA: 1.42 min\n",
      "[66/240] Sample processed in 0.15s, ETA: 1.39 min\n",
      "[67/240] Sample processed in 0.12s, ETA: 1.37 min\n",
      "[68/240] Sample processed in 0.12s, ETA: 1.35 min\n",
      "[69/240] Sample processed in 0.15s, ETA: 1.33 min\n",
      "[70/240] Sample processed in 0.15s, ETA: 1.31 min\n",
      "[71/240] Sample processed in 0.15s, ETA: 1.29 min\n",
      "[72/240] Sample processed in 0.17s, ETA: 1.27 min\n",
      "[73/240] Sample processed in 0.11s, ETA: 1.25 min\n",
      "[74/240] Sample processed in 0.11s, ETA: 1.23 min\n",
      "[75/240] Sample processed in 0.13s, ETA: 1.21 min\n",
      "[76/240] Sample processed in 0.11s, ETA: 1.19 min\n",
      "[77/240] Sample processed in 0.17s, ETA: 1.17 min\n",
      "[78/240] Sample processed in 0.16s, ETA: 1.16 min\n",
      "[79/240] Sample processed in 0.16s, ETA: 1.14 min\n",
      "[80/240] Sample processed in 0.14s, ETA: 1.12 min\n",
      "[81/240] Sample processed in 0.13s, ETA: 1.11 min\n",
      "[82/240] Sample processed in 0.14s, ETA: 1.09 min\n",
      "[83/240] Sample processed in 0.15s, ETA: 1.07 min\n",
      "[84/240] Sample processed in 0.12s, ETA: 1.06 min\n",
      "[85/240] Sample processed in 0.17s, ETA: 1.04 min\n",
      "[86/240] Sample processed in 0.14s, ETA: 1.03 min\n",
      "[87/240] Sample processed in 0.17s, ETA: 1.02 min\n",
      "[88/240] Sample processed in 0.13s, ETA: 1.00 min\n",
      "[89/240] Sample processed in 0.17s, ETA: 0.99 min\n",
      "[90/240] Sample processed in 0.15s, ETA: 0.98 min\n",
      "[91/240] Sample processed in 0.10s, ETA: 0.96 min\n",
      "[92/240] Sample processed in 0.18s, ETA: 0.95 min\n",
      "[93/240] Sample processed in 0.17s, ETA: 0.94 min\n",
      "[94/240] Sample processed in 0.18s, ETA: 0.93 min\n",
      "[95/240] Sample processed in 0.17s, ETA: 0.91 min\n",
      "[96/240] Sample processed in 0.18s, ETA: 0.90 min\n",
      "[97/240] Sample processed in 0.18s, ETA: 0.89 min\n",
      "[98/240] Sample processed in 0.18s, ETA: 0.88 min\n",
      "[99/240] Sample processed in 0.17s, ETA: 0.87 min\n",
      "[100/240] Sample processed in 0.18s, ETA: 0.86 min\n",
      "[101/240] Sample processed in 0.11s, ETA: 0.85 min\n",
      "[102/240] Sample processed in 0.17s, ETA: 0.84 min\n",
      "[103/240] Sample processed in 0.18s, ETA: 0.83 min\n",
      "[104/240] Sample processed in 0.11s, ETA: 0.82 min\n",
      "[105/240] Sample processed in 0.11s, ETA: 0.80 min\n",
      "[106/240] Sample processed in 0.17s, ETA: 0.79 min\n",
      "[107/240] Sample processed in 0.18s, ETA: 0.78 min\n",
      "[108/240] Sample processed in 0.18s, ETA: 0.77 min\n",
      "[109/240] Sample processed in 0.18s, ETA: 0.77 min\n",
      "[110/240] Sample processed in 0.18s, ETA: 0.76 min\n",
      "[111/240] Sample processed in 0.18s, ETA: 0.75 min\n",
      "[112/240] Sample processed in 0.18s, ETA: 0.74 min\n",
      "[113/240] Sample processed in 0.11s, ETA: 0.73 min\n",
      "[114/240] Sample processed in 0.18s, ETA: 0.72 min\n",
      "[115/240] Sample processed in 0.18s, ETA: 0.71 min\n",
      "[116/240] Sample processed in 0.18s, ETA: 0.70 min\n",
      "[117/240] Sample processed in 0.11s, ETA: 0.69 min\n",
      "[118/240] Sample processed in 0.11s, ETA: 0.68 min\n",
      "[119/240] Sample processed in 0.18s, ETA: 0.67 min\n",
      "[120/240] Sample processed in 0.18s, ETA: 0.67 min\n",
      "[121/240] Sample processed in 3.97s, ETA: 0.72 min\n",
      "[122/240] Sample processed in 0.66s, ETA: 0.72 min\n",
      "[123/240] Sample processed in 4.47s, ETA: 0.78 min\n",
      "[124/240] Sample processed in 4.47s, ETA: 0.83 min\n",
      "[125/240] Sample processed in 4.50s, ETA: 0.89 min\n",
      "[126/240] Sample processed in 4.46s, ETA: 0.94 min\n",
      "[127/240] Sample processed in 0.69s, ETA: 0.94 min\n",
      "[128/240] Sample processed in 4.16s, ETA: 0.98 min\n",
      "[129/240] Sample processed in 0.87s, ETA: 0.98 min\n",
      "[130/240] Sample processed in 4.26s, ETA: 1.02 min\n",
      "[131/240] Sample processed in 2.66s, ETA: 1.04 min\n",
      "[132/240] Sample processed in 4.49s, ETA: 1.09 min\n",
      "[133/240] Sample processed in 0.83s, ETA: 1.08 min\n",
      "[134/240] Sample processed in 4.26s, ETA: 1.12 min\n",
      "[135/240] Sample processed in 4.49s, ETA: 1.16 min\n",
      "[136/240] Sample processed in 4.52s, ETA: 1.19 min\n",
      "[137/240] Sample processed in 3.80s, ETA: 1.22 min\n",
      "[138/240] Sample processed in 4.51s, ETA: 1.26 min\n",
      "[139/240] Sample processed in 4.53s, ETA: 1.29 min\n",
      "[140/240] Sample processed in 4.55s, ETA: 1.32 min\n",
      "[141/240] Sample processed in 4.53s, ETA: 1.35 min\n",
      "[142/240] Sample processed in 4.54s, ETA: 1.38 min\n",
      "[143/240] Sample processed in 3.52s, ETA: 1.40 min\n",
      "[144/240] Sample processed in 4.52s, ETA: 1.42 min\n",
      "[145/240] Sample processed in 4.52s, ETA: 1.45 min\n",
      "[146/240] Sample processed in 2.01s, ETA: 1.45 min\n",
      "[147/240] Sample processed in 4.46s, ETA: 1.47 min\n",
      "[148/240] Sample processed in 4.53s, ETA: 1.49 min\n",
      "[149/240] Sample processed in 2.29s, ETA: 1.49 min\n",
      "[150/240] Sample processed in 4.53s, ETA: 1.51 min\n",
      "[151/240] Sample processed in 0.18s, ETA: 1.48 min\n",
      "[152/240] Sample processed in 0.19s, ETA: 1.46 min\n",
      "[153/240] Sample processed in 0.18s, ETA: 1.43 min\n",
      "[154/240] Sample processed in 0.19s, ETA: 1.41 min\n",
      "[155/240] Sample processed in 0.19s, ETA: 1.38 min\n",
      "[156/240] Sample processed in 0.18s, ETA: 1.36 min\n",
      "[157/240] Sample processed in 0.18s, ETA: 1.34 min\n",
      "[158/240] Sample processed in 0.22s, ETA: 1.32 min\n",
      "[159/240] Sample processed in 0.19s, ETA: 1.29 min\n",
      "[160/240] Sample processed in 0.18s, ETA: 1.27 min\n",
      "[161/240] Sample processed in 0.19s, ETA: 1.25 min\n",
      "[162/240] Sample processed in 0.18s, ETA: 1.23 min\n",
      "[163/240] Sample processed in 0.18s, ETA: 1.20 min\n",
      "[164/240] Sample processed in 0.18s, ETA: 1.18 min\n",
      "[165/240] Sample processed in 0.22s, ETA: 1.16 min\n",
      "[166/240] Sample processed in 0.18s, ETA: 1.14 min\n",
      "[167/240] Sample processed in 0.18s, ETA: 1.12 min\n",
      "[168/240] Sample processed in 0.18s, ETA: 1.10 min\n",
      "[169/240] Sample processed in 0.18s, ETA: 1.08 min\n",
      "[170/240] Sample processed in 0.18s, ETA: 1.06 min\n",
      "[171/240] Sample processed in 0.18s, ETA: 1.04 min\n",
      "[172/240] Sample processed in 0.18s, ETA: 1.02 min\n",
      "[173/240] Sample processed in 0.18s, ETA: 1.00 min\n",
      "[174/240] Sample processed in 0.18s, ETA: 0.98 min\n",
      "[175/240] Sample processed in 0.18s, ETA: 0.96 min\n",
      "[176/240] Sample processed in 0.18s, ETA: 0.94 min\n",
      "[177/240] Sample processed in 0.18s, ETA: 0.92 min\n",
      "[178/240] Sample processed in 0.18s, ETA: 0.90 min\n",
      "[179/240] Sample processed in 0.18s, ETA: 0.89 min\n",
      "[180/240] Sample processed in 0.18s, ETA: 0.87 min\n",
      "[181/240] Sample processed in 22.81s, ETA: 0.97 min\n",
      "[182/240] Sample processed in 7.15s, ETA: 0.99 min\n",
      "[183/240] Sample processed in 11.49s, ETA: 1.03 min\n",
      "[184/240] Sample processed in 22.73s, ETA: 1.12 min\n",
      "[185/240] Sample processed in 9.74s, ETA: 1.14 min\n",
      "[186/240] Sample processed in 7.75s, ETA: 1.15 min\n",
      "[187/240] Sample processed in 7.85s, ETA: 1.16 min\n",
      "[188/240] Sample processed in 8.80s, ETA: 1.17 min\n",
      "[189/240] Sample processed in 7.54s, ETA: 1.18 min\n",
      "[190/240] Sample processed in 15.22s, ETA: 1.22 min\n",
      "[191/240] Sample processed in 6.91s, ETA: 1.21 min\n",
      "[192/240] Sample processed in 5.80s, ETA: 1.21 min\n",
      "[193/240] Sample processed in 7.97s, ETA: 1.21 min\n",
      "[194/240] Sample processed in 8.62s, ETA: 1.21 min\n",
      "[195/240] Sample processed in 6.65s, ETA: 1.20 min\n",
      "[196/240] Sample processed in 7.75s, ETA: 1.20 min\n",
      "[197/240] Sample processed in 7.39s, ETA: 1.19 min\n",
      "[198/240] Sample processed in 9.23s, ETA: 1.19 min\n",
      "[199/240] Sample processed in 10.29s, ETA: 1.19 min\n",
      "[200/240] Sample processed in 13.32s, ETA: 1.20 min\n",
      "[201/240] Sample processed in 7.52s, ETA: 1.19 min\n",
      "[202/240] Sample processed in 5.36s, ETA: 1.17 min\n",
      "[203/240] Sample processed in 6.18s, ETA: 1.15 min\n",
      "[204/240] Sample processed in 7.91s, ETA: 1.14 min\n",
      "[205/240] Sample processed in 5.21s, ETA: 1.12 min\n",
      "[206/240] Sample processed in 8.65s, ETA: 1.11 min\n",
      "[207/240] Sample processed in 7.64s, ETA: 1.09 min\n",
      "[208/240] Sample processed in 17.14s, ETA: 1.09 min\n",
      "[209/240] Sample processed in 20.37s, ETA: 1.11 min\n",
      "[210/240] Sample processed in 5.51s, ETA: 1.08 min\n",
      "[211/240] Sample processed in 9.83s, ETA: 1.06 min\n",
      "[212/240] Sample processed in 6.21s, ETA: 1.03 min\n",
      "[213/240] Sample processed in 7.56s, ETA: 1.01 min\n",
      "[214/240] Sample processed in 6.36s, ETA: 0.98 min\n",
      "[215/240] Sample processed in 6.91s, ETA: 0.95 min\n",
      "[216/240] Sample processed in 12.42s, ETA: 0.93 min\n",
      "[217/240] Sample processed in 4.19s, ETA: 0.89 min\n",
      "[218/240] Sample processed in 19.91s, ETA: 0.88 min\n",
      "[219/240] Sample processed in 10.82s, ETA: 0.86 min\n",
      "[220/240] Sample processed in 4.85s, ETA: 0.82 min\n",
      "[221/240] Sample processed in 8.49s, ETA: 0.79 min\n",
      "[222/240] Sample processed in 7.88s, ETA: 0.75 min\n",
      "[223/240] Sample processed in 4.54s, ETA: 0.71 min\n",
      "[224/240] Sample processed in 5.02s, ETA: 0.68 min\n",
      "[225/240] Sample processed in 11.98s, ETA: 0.64 min\n",
      "[226/240] Sample processed in 12.73s, ETA: 0.61 min\n",
      "[227/240] Sample processed in 4.00s, ETA: 0.57 min\n",
      "[228/240] Sample processed in 6.45s, ETA: 0.53 min\n",
      "[229/240] Sample processed in 14.16s, ETA: 0.49 min\n",
      "[230/240] Sample processed in 6.17s, ETA: 0.45 min\n",
      "[231/240] Sample processed in 7.24s, ETA: 0.41 min\n",
      "[232/240] Sample processed in 6.52s, ETA: 0.37 min\n",
      "[233/240] Sample processed in 5.55s, ETA: 0.32 min\n",
      "[234/240] Sample processed in 25.78s, ETA: 0.29 min\n",
      "[235/240] Sample processed in 5.93s, ETA: 0.24 min\n",
      "[236/240] Sample processed in 4.20s, ETA: 0.19 min\n",
      "[237/240] Sample processed in 3.63s, ETA: 0.14 min\n",
      "[238/240] Sample processed in 6.05s, ETA: 0.10 min\n",
      "[239/240] Sample processed in 6.03s, ETA: 0.05 min\n",
      "[240/240] Sample processed in 4.19s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 11.64 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### âœ… Hugging Faceì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "dataset_name = \"passionMan/test_dataset4\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split ë¡œë“œ\n",
    "\n",
    "### âœ… JSONL ì €ì¥ í•¨ìˆ˜ (í‰ê°€ ê²°ê³¼ ì €ì¥ìš©)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### âœ… ëª¨ë¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # âœ… ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸° (ë³´í†µ 4096 ë˜ëŠ” 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # âœ… ì…ë ¥ í† í° ê¸¸ì´ í™•ì¸\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # ğŸ”¥ ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ì— ë§ê²Œ ìë¦„\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # âœ… ìƒì„± ìˆ˜í–‰ (max_new_tokensì„ ì ìš©)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # âœ… ìƒì„± ê¸¸ì´ ì ìš©\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# âœ… ë°ì´í„° ê²½ë¡œ ì„¤ì • (ê²°ê³¼ ì €ì¥ìš©)\n",
    "output_json_path = \"/data/jaesung/llm_for_diabetes/src/trial/model_response/model_output_cherry.jsonl\"\n",
    "\n",
    "# âœ… Taskë³„ ë°ì´í„° ê·¸ë£¹í™” (ê° íƒœìŠ¤í¬ë³„ 0~29ë²ˆ ìƒ˜í”Œ ì„ íƒ)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€í•  ë°ì´í„° ìƒì„± (ê° íƒœìŠ¤í¬ë³„ 30ê°œë§Œ ì¶”ì¶œ)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:30])  # ìµœëŒ€ 30ê°œ ì„ íƒ\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€ ì‹œì‘\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # âœ… ìƒì„±í•  í† í° ê¸¸ì´ ì„¤ì • (ìƒì„± í† í° ìˆ˜ ì¡°ì ˆ)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"re\"}  # ìƒì„± 32\n",
    "    medium_context_tasks = {\"ie\"} # ìƒì„± 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # ìƒì„± 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 128  # âœ… ìƒì„± ê¸¸ì´ 128\n",
    "    elif task in medium_context_tasks:\n",
    "        max_new_tokens = 128\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 1024  # âœ… ìƒì„± ê¸¸ì´ 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # ê¸°ë³¸ê°’\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial3/outputs/model_cherry/checkpoint-327\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetes\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "For a diabetes-friendly diet, it's important to focus on whole, unprocessed foods that are low in added sugars, salt, and unhealthy fats. Here are some key recommendations to help manage diabetes:\n",
      "\n",
      "1. **Vegetables**: Emphasize non-starchy vegetables like leafy greens, broccoli, bell peppers, and cucumbers. They are low in carbs and high in fiber, which helps regulate blood sugar levels.\n",
      "\n",
      "2. **Protein**: Include lean protein sources like poultry, fish, tofu, and legumes. Protein helps slow down the absorption of carbohydrates and keeps you full for longer.\n",
      "\n",
      "3. **Whole Grains\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Please recommend a diet for diabetes\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Context:\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite. Dice, chop, and stirâ€”thatâ€™s all you need to create broccoli and apple salad. This healthy recipe is low in sodium and high in flavor. Caution: this recipe contains walnuts. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Veggie Rich', 'Kid Friendly', 'Salads', 'Vegetarian', 'Sides', 'Quick & Easy'])\n",
      "Chicken Nuggets with Honey Mustard Dipping Sauce: This recipe is not only delicious, but itâ€™s also kidney-friendly. Try a variety of dipping sauces, such as barbecue sauce, curry sauce, fruit spreads, or low-sodium salad dressing. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "\n",
      "### Response: \n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad is perfect for diabetic patients as it combines lean protein from chicken with fiber-rich apples and whole grains. The salad is low in carbohydrates and high in fiber, which helps regulate blood sugar levels. To make it even more kidney-friendly, you can substitute the chicken with tofu or tempeh. The apple crunch topping adds natural sweetness without the need for added sugars. This salad is a great option for a quick and satisfying meal or snack that is also low in sodium and calories.\n",
      "\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a great option for diabetic patients as it\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# 2ï¸âƒ£ ìŒì‹ ë°ì´í„° ë¡œë“œ & FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "df = pd.read_csv(\"processed_food_data.csv\")  # ìŒì‹ ë°ì´í„° ë¡œë“œ\n",
    "index = faiss.read_index(\"food_faiss.index\")  # FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "embedding_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸\n",
    "\n",
    "# 3ï¸âƒ£ ìŒì‹ ê²€ìƒ‰ í•¨ìˆ˜ (FAISS ì‚¬ìš©)\n",
    "def search_food(query, top_k=3):\n",
    "    \"\"\"ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ FAISSì—ì„œ ìœ ì‚¬í•œ ìŒì‹ ê²€ìƒ‰\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # ê²€ìƒ‰ëœ ìŒì‹ ì •ë³´ ì¶”ì¶œ\n",
    "    results = df.iloc[indices[0]][[\"title\", \"description\", \"tags\"]]\n",
    "    \n",
    "    # ê²€ìƒ‰ëœ ìŒì‹ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    search_context = \"\\n\".join([f\"{row['title']}: {row['description']} (tags: {row['tags']})\" for _, row in results.iterrows()])\n",
    "    return search_context\n",
    "\n",
    "# 4ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ë° ê²€ìƒ‰ ìˆ˜í–‰\n",
    "query = \"Please recommend a diet for diabetic patients.\"\n",
    "search_results = search_food(query)\n",
    "\n",
    "# 5ï¸âƒ£ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_text = alpaca_prompt.format(\n",
    "    \"Please recommend a diet for diabetic patients.\",\n",
    "    query,\n",
    "    search_results\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
