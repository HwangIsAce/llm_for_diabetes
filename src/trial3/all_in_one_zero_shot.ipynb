{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/240] Sample processed in 5.81s, ETA: 23.16 min\n",
      "[2/240] Sample processed in 2.89s, ETA: 17.27 min\n",
      "[3/240] Sample processed in 3.62s, ETA: 16.23 min\n",
      "[4/240] Sample processed in 3.63s, ETA: 15.69 min\n",
      "[5/240] Sample processed in 3.62s, ETA: 15.33 min\n",
      "[6/240] Sample processed in 0.09s, ETA: 12.78 min\n",
      "[7/240] Sample processed in 3.63s, ETA: 12.92 min\n",
      "[8/240] Sample processed in 3.23s, ETA: 12.82 min\n",
      "[9/240] Sample processed in 1.94s, ETA: 12.18 min\n",
      "[10/240] Sample processed in 3.62s, ETA: 12.30 min\n",
      "[11/240] Sample processed in 0.98s, ETA: 11.47 min\n",
      "[12/240] Sample processed in 0.26s, ETA: 10.55 min\n",
      "[13/240] Sample processed in 0.13s, ETA: 9.73 min\n",
      "[14/240] Sample processed in 3.63s, ETA: 9.98 min\n",
      "[15/240] Sample processed in 3.64s, ETA: 10.18 min\n",
      "[16/240] Sample processed in 0.19s, ETA: 9.55 min\n",
      "[17/240] Sample processed in 3.66s, ETA: 9.74 min\n",
      "[18/240] Sample processed in 0.35s, ETA: 9.23 min\n",
      "[19/240] Sample processed in 0.13s, ETA: 8.73 min\n",
      "[20/240] Sample processed in 0.16s, ETA: 8.29 min\n",
      "[21/240] Sample processed in 3.66s, ETA: 8.49 min\n",
      "[22/240] Sample processed in 0.50s, ETA: 8.15 min\n",
      "[23/240] Sample processed in 3.65s, ETA: 8.34 min\n",
      "[24/240] Sample processed in 3.65s, ETA: 8.50 min\n",
      "[25/240] Sample processed in 3.65s, ETA: 8.65 min\n",
      "[26/240] Sample processed in 2.18s, ETA: 8.57 min\n",
      "[27/240] Sample processed in 0.30s, ETA: 8.26 min\n",
      "[28/240] Sample processed in 0.92s, ETA: 8.04 min\n",
      "[29/240] Sample processed in 3.64s, ETA: 8.17 min\n",
      "[30/240] Sample processed in 0.15s, ETA: 7.88 min\n",
      "[31/240] Sample processed in 3.65s, ETA: 8.00 min\n",
      "[32/240] Sample processed in 3.66s, ETA: 8.11 min\n",
      "[33/240] Sample processed in 3.65s, ETA: 8.20 min\n",
      "[34/240] Sample processed in 3.65s, ETA: 8.29 min\n",
      "[35/240] Sample processed in 0.07s, ETA: 8.02 min\n",
      "[36/240] Sample processed in 3.67s, ETA: 8.11 min\n",
      "[37/240] Sample processed in 3.64s, ETA: 8.18 min\n",
      "[38/240] Sample processed in 2.92s, ETA: 8.19 min\n",
      "[39/240] Sample processed in 3.67s, ETA: 8.25 min\n",
      "[40/240] Sample processed in 3.63s, ETA: 8.31 min\n",
      "[41/240] Sample processed in 3.60s, ETA: 8.36 min\n",
      "[42/240] Sample processed in 3.63s, ETA: 8.40 min\n",
      "[43/240] Sample processed in 3.64s, ETA: 8.44 min\n",
      "[44/240] Sample processed in 0.18s, ETA: 8.22 min\n",
      "[45/240] Sample processed in 3.61s, ETA: 8.26 min\n",
      "[46/240] Sample processed in 1.03s, ETA: 8.11 min\n",
      "[47/240] Sample processed in 1.63s, ETA: 8.01 min\n",
      "[48/240] Sample processed in 3.62s, ETA: 8.04 min\n",
      "[49/240] Sample processed in 0.86s, ETA: 7.89 min\n",
      "[50/240] Sample processed in 3.62s, ETA: 7.93 min\n",
      "[51/240] Sample processed in 1.49s, ETA: 7.82 min\n",
      "[52/240] Sample processed in 3.66s, ETA: 7.85 min\n",
      "[53/240] Sample processed in 0.10s, ETA: 7.67 min\n",
      "[54/240] Sample processed in 0.24s, ETA: 7.50 min\n",
      "[55/240] Sample processed in 0.10s, ETA: 7.33 min\n",
      "[56/240] Sample processed in 3.18s, ETA: 7.33 min\n",
      "[57/240] Sample processed in 3.69s, ETA: 7.36 min\n",
      "[58/240] Sample processed in 0.52s, ETA: 7.22 min\n",
      "[59/240] Sample processed in 0.33s, ETA: 7.08 min\n",
      "[60/240] Sample processed in 0.13s, ETA: 6.93 min\n",
      "[61/240] Sample processed in 0.78s, ETA: 6.82 min\n",
      "[62/240] Sample processed in 0.18s, ETA: 6.68 min\n",
      "[63/240] Sample processed in 3.17s, ETA: 6.68 min\n",
      "[64/240] Sample processed in 0.26s, ETA: 6.55 min\n",
      "[65/240] Sample processed in 3.67s, ETA: 6.58 min\n",
      "[66/240] Sample processed in 3.63s, ETA: 6.60 min\n",
      "[67/240] Sample processed in 3.66s, ETA: 6.63 min\n",
      "[68/240] Sample processed in 3.67s, ETA: 6.64 min\n",
      "[69/240] Sample processed in 0.11s, ETA: 6.51 min\n",
      "[70/240] Sample processed in 0.19s, ETA: 6.39 min\n",
      "[71/240] Sample processed in 3.68s, ETA: 6.41 min\n",
      "[72/240] Sample processed in 2.43s, ETA: 6.38 min\n",
      "[73/240] Sample processed in 3.65s, ETA: 6.39 min\n",
      "[74/240] Sample processed in 3.68s, ETA: 6.41 min\n",
      "[75/240] Sample processed in 3.15s, ETA: 6.40 min\n",
      "[76/240] Sample processed in 3.61s, ETA: 6.41 min\n",
      "[77/240] Sample processed in 0.15s, ETA: 6.29 min\n",
      "[78/240] Sample processed in 0.09s, ETA: 6.17 min\n",
      "[79/240] Sample processed in 3.69s, ETA: 6.18 min\n",
      "[80/240] Sample processed in 3.72s, ETA: 6.19 min\n",
      "[81/240] Sample processed in 3.68s, ETA: 6.20 min\n",
      "[82/240] Sample processed in 3.70s, ETA: 6.20 min\n",
      "[83/240] Sample processed in 3.69s, ETA: 6.21 min\n",
      "[84/240] Sample processed in 0.13s, ETA: 6.10 min\n",
      "[85/240] Sample processed in 0.41s, ETA: 6.00 min\n",
      "[86/240] Sample processed in 0.16s, ETA: 5.90 min\n",
      "[87/240] Sample processed in 3.07s, ETA: 5.88 min\n",
      "[88/240] Sample processed in 3.67s, ETA: 5.88 min\n",
      "[89/240] Sample processed in 3.71s, ETA: 5.88 min\n",
      "[90/240] Sample processed in 0.56s, ETA: 5.79 min\n",
      "[91/240] Sample processed in 0.08s, ETA: 5.69 min\n",
      "[92/240] Sample processed in 0.07s, ETA: 5.60 min\n",
      "[93/240] Sample processed in 0.15s, ETA: 5.50 min\n",
      "[94/240] Sample processed in 0.07s, ETA: 5.41 min\n",
      "[95/240] Sample processed in 0.06s, ETA: 5.32 min\n",
      "[96/240] Sample processed in 0.07s, ETA: 5.23 min\n",
      "[97/240] Sample processed in 0.07s, ETA: 5.14 min\n",
      "[98/240] Sample processed in 0.07s, ETA: 5.05 min\n",
      "[99/240] Sample processed in 0.06s, ETA: 4.97 min\n",
      "[100/240] Sample processed in 0.07s, ETA: 4.88 min\n",
      "[101/240] Sample processed in 0.07s, ETA: 4.80 min\n",
      "[102/240] Sample processed in 0.06s, ETA: 4.72 min\n",
      "[103/240] Sample processed in 0.07s, ETA: 4.64 min\n",
      "[104/240] Sample processed in 0.06s, ETA: 4.57 min\n",
      "[105/240] Sample processed in 1.97s, ETA: 4.53 min\n",
      "[106/240] Sample processed in 0.06s, ETA: 4.46 min\n",
      "[107/240] Sample processed in 0.06s, ETA: 4.38 min\n",
      "[108/240] Sample processed in 0.07s, ETA: 4.31 min\n",
      "[109/240] Sample processed in 0.24s, ETA: 4.25 min\n",
      "[110/240] Sample processed in 0.07s, ETA: 4.18 min\n",
      "[111/240] Sample processed in 0.07s, ETA: 4.11 min\n",
      "[112/240] Sample processed in 0.07s, ETA: 4.04 min\n",
      "[113/240] Sample processed in 0.06s, ETA: 3.98 min\n",
      "[114/240] Sample processed in 0.06s, ETA: 3.91 min\n",
      "[115/240] Sample processed in 0.07s, ETA: 3.85 min\n",
      "[116/240] Sample processed in 0.07s, ETA: 3.78 min\n",
      "[117/240] Sample processed in 0.07s, ETA: 3.72 min\n",
      "[118/240] Sample processed in 0.06s, ETA: 3.66 min\n",
      "[119/240] Sample processed in 0.07s, ETA: 3.60 min\n",
      "[120/240] Sample processed in 0.06s, ETA: 3.54 min\n",
      "[121/240] Sample processed in 3.69s, ETA: 3.55 min\n",
      "[122/240] Sample processed in 0.58s, ETA: 3.50 min\n",
      "[123/240] Sample processed in 1.28s, ETA: 3.46 min\n",
      "[124/240] Sample processed in 3.69s, ETA: 3.46 min\n",
      "[125/240] Sample processed in 0.09s, ETA: 3.40 min\n",
      "[126/240] Sample processed in 3.68s, ETA: 3.40 min\n",
      "[127/240] Sample processed in 0.66s, ETA: 3.36 min\n",
      "[128/240] Sample processed in 3.67s, ETA: 3.35 min\n",
      "[129/240] Sample processed in 0.95s, ETA: 3.31 min\n",
      "[130/240] Sample processed in 3.67s, ETA: 3.31 min\n",
      "[131/240] Sample processed in 0.06s, ETA: 3.25 min\n",
      "[132/240] Sample processed in 3.68s, ETA: 3.25 min\n",
      "[133/240] Sample processed in 3.66s, ETA: 3.25 min\n",
      "[134/240] Sample processed in 3.69s, ETA: 3.24 min\n",
      "[135/240] Sample processed in 3.68s, ETA: 3.23 min\n",
      "[136/240] Sample processed in 3.73s, ETA: 3.23 min\n",
      "[137/240] Sample processed in 3.70s, ETA: 3.22 min\n",
      "[138/240] Sample processed in 3.62s, ETA: 3.21 min\n",
      "[139/240] Sample processed in 3.70s, ETA: 3.20 min\n",
      "[140/240] Sample processed in 3.71s, ETA: 3.19 min\n",
      "[141/240] Sample processed in 1.73s, ETA: 3.15 min\n",
      "[142/240] Sample processed in 3.69s, ETA: 3.14 min\n",
      "[143/240] Sample processed in 3.68s, ETA: 3.13 min\n",
      "[144/240] Sample processed in 3.66s, ETA: 3.12 min\n",
      "[145/240] Sample processed in 2.71s, ETA: 3.09 min\n",
      "[146/240] Sample processed in 3.67s, ETA: 3.08 min\n",
      "[147/240] Sample processed in 3.69s, ETA: 3.07 min\n",
      "[148/240] Sample processed in 3.64s, ETA: 3.05 min\n",
      "[149/240] Sample processed in 3.71s, ETA: 3.03 min\n",
      "[150/240] Sample processed in 3.35s, ETA: 3.01 min\n",
      "[151/240] Sample processed in 0.22s, ETA: 2.96 min\n",
      "[152/240] Sample processed in 0.19s, ETA: 2.91 min\n",
      "[153/240] Sample processed in 0.64s, ETA: 2.87 min\n",
      "[154/240] Sample processed in 2.17s, ETA: 2.84 min\n",
      "[155/240] Sample processed in 0.16s, ETA: 2.79 min\n",
      "[156/240] Sample processed in 0.16s, ETA: 2.74 min\n",
      "[157/240] Sample processed in 0.30s, ETA: 2.69 min\n",
      "[158/240] Sample processed in 0.16s, ETA: 2.64 min\n",
      "[159/240] Sample processed in 0.19s, ETA: 2.59 min\n",
      "[160/240] Sample processed in 1.92s, ETA: 2.56 min\n",
      "[161/240] Sample processed in 0.16s, ETA: 2.52 min\n",
      "[162/240] Sample processed in 1.93s, ETA: 2.48 min\n",
      "[163/240] Sample processed in 3.45s, ETA: 2.46 min\n",
      "[164/240] Sample processed in 0.19s, ETA: 2.42 min\n",
      "[165/240] Sample processed in 3.25s, ETA: 2.40 min\n",
      "[166/240] Sample processed in 0.16s, ETA: 2.35 min\n",
      "[167/240] Sample processed in 0.19s, ETA: 2.31 min\n",
      "[168/240] Sample processed in 1.78s, ETA: 2.28 min\n",
      "[169/240] Sample processed in 0.19s, ETA: 2.23 min\n",
      "[170/240] Sample processed in 0.19s, ETA: 2.19 min\n",
      "[171/240] Sample processed in 0.19s, ETA: 2.15 min\n",
      "[172/240] Sample processed in 1.41s, ETA: 2.11 min\n",
      "[173/240] Sample processed in 2.70s, ETA: 2.09 min\n",
      "[174/240] Sample processed in 1.95s, ETA: 2.06 min\n",
      "[175/240] Sample processed in 0.16s, ETA: 2.01 min\n",
      "[176/240] Sample processed in 0.19s, ETA: 1.97 min\n",
      "[177/240] Sample processed in 0.19s, ETA: 1.93 min\n",
      "[178/240] Sample processed in 0.16s, ETA: 1.89 min\n",
      "[179/240] Sample processed in 0.19s, ETA: 1.85 min\n",
      "[180/240] Sample processed in 1.23s, ETA: 1.82 min\n",
      "[181/240] Sample processed in 17.82s, ETA: 1.88 min\n",
      "[182/240] Sample processed in 5.56s, ETA: 1.86 min\n",
      "[183/240] Sample processed in 3.41s, ETA: 1.84 min\n",
      "[184/240] Sample processed in 0.11s, ETA: 1.80 min\n",
      "[185/240] Sample processed in 9.70s, ETA: 1.80 min\n",
      "[186/240] Sample processed in 9.56s, ETA: 1.81 min\n",
      "[187/240] Sample processed in 12.02s, ETA: 1.82 min\n",
      "[188/240] Sample processed in 0.09s, ETA: 1.78 min\n",
      "[189/240] Sample processed in 9.24s, ETA: 1.78 min\n",
      "[190/240] Sample processed in 1.14s, ETA: 1.74 min\n",
      "[191/240] Sample processed in 13.58s, ETA: 1.75 min\n",
      "[192/240] Sample processed in 29.28s, ETA: 1.83 min\n",
      "[193/240] Sample processed in 0.10s, ETA: 1.78 min\n",
      "[194/240] Sample processed in 0.11s, ETA: 1.74 min\n",
      "[195/240] Sample processed in 6.76s, ETA: 1.71 min\n",
      "[196/240] Sample processed in 7.04s, ETA: 1.69 min\n",
      "[197/240] Sample processed in 2.14s, ETA: 1.66 min\n",
      "[198/240] Sample processed in 29.30s, ETA: 1.71 min\n",
      "[199/240] Sample processed in 0.10s, ETA: 1.66 min\n",
      "[200/240] Sample processed in 0.10s, ETA: 1.61 min\n",
      "[201/240] Sample processed in 2.06s, ETA: 1.57 min\n",
      "[202/240] Sample processed in 2.39s, ETA: 1.53 min\n",
      "[203/240] Sample processed in 0.09s, ETA: 1.49 min\n",
      "[204/240] Sample processed in 6.58s, ETA: 1.46 min\n",
      "[205/240] Sample processed in 9.95s, ETA: 1.44 min\n",
      "[206/240] Sample processed in 0.12s, ETA: 1.39 min\n",
      "[207/240] Sample processed in 0.09s, ETA: 1.34 min\n",
      "[208/240] Sample processed in 0.11s, ETA: 1.30 min\n",
      "[209/240] Sample processed in 11.07s, ETA: 1.28 min\n",
      "[210/240] Sample processed in 29.39s, ETA: 1.30 min\n",
      "[211/240] Sample processed in 29.38s, ETA: 1.32 min\n",
      "[212/240] Sample processed in 29.10s, ETA: 1.33 min\n",
      "[213/240] Sample processed in 5.97s, ETA: 1.29 min\n",
      "[214/240] Sample processed in 2.68s, ETA: 1.24 min\n",
      "[215/240] Sample processed in 0.06s, ETA: 1.19 min\n",
      "[216/240] Sample processed in 8.58s, ETA: 1.15 min\n",
      "[217/240] Sample processed in 8.26s, ETA: 1.11 min\n",
      "[218/240] Sample processed in 13.61s, ETA: 1.08 min\n",
      "[219/240] Sample processed in 9.56s, ETA: 1.04 min\n",
      "[220/240] Sample processed in 10.90s, ETA: 1.01 min\n",
      "[221/240] Sample processed in 14.49s, ETA: 0.97 min\n",
      "[222/240] Sample processed in 8.53s, ETA: 0.93 min\n",
      "[223/240] Sample processed in 7.05s, ETA: 0.88 min\n",
      "[224/240] Sample processed in 0.06s, ETA: 0.83 min\n",
      "[225/240] Sample processed in 11.64s, ETA: 0.78 min\n",
      "[226/240] Sample processed in 9.03s, ETA: 0.74 min\n",
      "[227/240] Sample processed in 28.84s, ETA: 0.71 min\n",
      "[228/240] Sample processed in 28.88s, ETA: 0.68 min\n",
      "[229/240] Sample processed in 7.21s, ETA: 0.62 min\n",
      "[230/240] Sample processed in 0.06s, ETA: 0.57 min\n",
      "[231/240] Sample processed in 0.07s, ETA: 0.51 min\n",
      "[232/240] Sample processed in 3.99s, ETA: 0.45 min\n",
      "[233/240] Sample processed in 0.07s, ETA: 0.39 min\n",
      "[234/240] Sample processed in 6.86s, ETA: 0.34 min\n",
      "[235/240] Sample processed in 9.69s, ETA: 0.28 min\n",
      "[236/240] Sample processed in 6.88s, ETA: 0.23 min\n",
      "[237/240] Sample processed in 9.66s, ETA: 0.17 min\n",
      "[238/240] Sample processed in 0.07s, ETA: 0.11 min\n",
      "[239/240] Sample processed in 13.90s, ETA: 0.06 min\n",
      "[240/240] Sample processed in 0.15s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 13.86 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### ✅ Hugging Face에서 데이터 로드\n",
    "dataset_name = \"passionMan/test_dataset4\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split 로드\n",
    "\n",
    "### ✅ JSONL 저장 함수 (평가 결과 저장용)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### ✅ 모델 응답 생성 함수\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # ✅ 모델의 최대 입력 길이 가져오기 (보통 4096 또는 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # ✅ 입력 토큰 길이 확인\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # 🔥 입력이 너무 길면 최대 입력 길이에 맞게 자름\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # ✅ 생성 수행 (max_new_tokens을 적용)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # ✅ 생성 길이 적용\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ✅ 데이터 경로 설정 (결과 저장용)\n",
    "output_json_path = \"/data/jaesung/llm_for_diabetes/src/trial3/model_response/model_output_zero_shot.jsonl\"\n",
    "\n",
    "# ✅ Task별 데이터 그룹화 (각 태스크별 0~29번 샘플 선택)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# ✅ 성능 평가할 데이터 생성 (각 태스크별 30개만 추출)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:30])  # 최대 30개 선택\n",
    "\n",
    "# ✅ 성능 평가 시작\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # ✅ 생성할 토큰 길이 설정 (생성 토큰 수 조절)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"ie\", \"re\"}  # 생성 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # 생성 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 128  # ✅ 생성 길이 128\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 1024  # ✅ 생성 길이 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # 기본값\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "1. A diabetic diet is a diet that can help you manage your blood glucose level. It should be low in fat, high in fiber, and rich in complex carbohydrates. It should also be low in calories and high in protein. You should avoid eating foods that are high in sugar and refined carbohydrates, such as white bread, pasta, and rice. Instead, opt for whole grains, fruits, and vegetables. You should also avoid eating foods that are high in saturated fat, such as red meat, butter, and cheese. Instead, opt for lean proteins, such as chicken and fish. You should also avoid eating foods that are high\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Please recommend a diet for diabetic patients.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Context:\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite. Dice, chop, and stir—that’s all you need to create broccoli and apple salad. This healthy recipe is low in sodium and high in flavor. Caution: this recipe contains walnuts. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Veggie Rich', 'Kid Friendly', 'Salads', 'Vegetarian', 'Sides', 'Quick & Easy'])\n",
      "Chicken Nuggets with Honey Mustard Dipping Sauce: This recipe is not only delicious, but it’s also kidney-friendly. Try a variety of dipping sauces, such as barbecue sauce, curry sauce, fruit spreads, or low-sodium salad dressing. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "\n",
      "### Response: \n",
      "\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# 2️⃣ 음식 데이터 로드 & FAISS 인덱스 로드\n",
    "df = pd.read_csv(\"processed_food_data.csv\")  # 음식 데이터 로드\n",
    "index = faiss.read_index(\"food_faiss.index\")  # FAISS 인덱스 로드\n",
    "embedding_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # 한국어 임베딩 모델\n",
    "\n",
    "# 3️⃣ 음식 검색 함수 (FAISS 사용)\n",
    "def search_food(query, top_k=3):\n",
    "    \"\"\"질문을 벡터화하여 FAISS에서 유사한 음식 검색\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # 검색된 음식 정보 추출\n",
    "    results = df.iloc[indices[0]][[\"title\", \"description\", \"tags\"]]\n",
    "    \n",
    "    # 검색된 음식 정보를 프롬프트용 문자열로 변환\n",
    "    search_context = \"\\n\".join([f\"{row['title']}: {row['description']} (tags: {row['tags']})\" for _, row in results.iterrows()])\n",
    "    return search_context\n",
    "\n",
    "# 4️⃣ 사용자 입력 및 검색 수행\n",
    "query = \"Please recommend a diet for diabetic patients.\"\n",
    "search_results = search_food(query)\n",
    "\n",
    "# 5️⃣ LLM 프롬프트 생성\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_text = alpaca_prompt.format(\n",
    "    \"Please recommend a diet for diabetic patients.\",\n",
    "    query,\n",
    "    search_results\n",
    ")\n",
    "\n",
    "# 6️⃣ LLM을 사용하여 검색된 정보 기반으로 답변 생성\n",
    "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
