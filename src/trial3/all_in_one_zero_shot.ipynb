{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/240] Sample processed in 5.81s, ETA: 23.16 min\n",
      "[2/240] Sample processed in 2.89s, ETA: 17.27 min\n",
      "[3/240] Sample processed in 3.62s, ETA: 16.23 min\n",
      "[4/240] Sample processed in 3.63s, ETA: 15.69 min\n",
      "[5/240] Sample processed in 3.62s, ETA: 15.33 min\n",
      "[6/240] Sample processed in 0.09s, ETA: 12.78 min\n",
      "[7/240] Sample processed in 3.63s, ETA: 12.92 min\n",
      "[8/240] Sample processed in 3.23s, ETA: 12.82 min\n",
      "[9/240] Sample processed in 1.94s, ETA: 12.18 min\n",
      "[10/240] Sample processed in 3.62s, ETA: 12.30 min\n",
      "[11/240] Sample processed in 0.98s, ETA: 11.47 min\n",
      "[12/240] Sample processed in 0.26s, ETA: 10.55 min\n",
      "[13/240] Sample processed in 0.13s, ETA: 9.73 min\n",
      "[14/240] Sample processed in 3.63s, ETA: 9.98 min\n",
      "[15/240] Sample processed in 3.64s, ETA: 10.18 min\n",
      "[16/240] Sample processed in 0.19s, ETA: 9.55 min\n",
      "[17/240] Sample processed in 3.66s, ETA: 9.74 min\n",
      "[18/240] Sample processed in 0.35s, ETA: 9.23 min\n",
      "[19/240] Sample processed in 0.13s, ETA: 8.73 min\n",
      "[20/240] Sample processed in 0.16s, ETA: 8.29 min\n",
      "[21/240] Sample processed in 3.66s, ETA: 8.49 min\n",
      "[22/240] Sample processed in 0.50s, ETA: 8.15 min\n",
      "[23/240] Sample processed in 3.65s, ETA: 8.34 min\n",
      "[24/240] Sample processed in 3.65s, ETA: 8.50 min\n",
      "[25/240] Sample processed in 3.65s, ETA: 8.65 min\n",
      "[26/240] Sample processed in 2.18s, ETA: 8.57 min\n",
      "[27/240] Sample processed in 0.30s, ETA: 8.26 min\n",
      "[28/240] Sample processed in 0.92s, ETA: 8.04 min\n",
      "[29/240] Sample processed in 3.64s, ETA: 8.17 min\n",
      "[30/240] Sample processed in 0.15s, ETA: 7.88 min\n",
      "[31/240] Sample processed in 3.65s, ETA: 8.00 min\n",
      "[32/240] Sample processed in 3.66s, ETA: 8.11 min\n",
      "[33/240] Sample processed in 3.65s, ETA: 8.20 min\n",
      "[34/240] Sample processed in 3.65s, ETA: 8.29 min\n",
      "[35/240] Sample processed in 0.07s, ETA: 8.02 min\n",
      "[36/240] Sample processed in 3.67s, ETA: 8.11 min\n",
      "[37/240] Sample processed in 3.64s, ETA: 8.18 min\n",
      "[38/240] Sample processed in 2.92s, ETA: 8.19 min\n",
      "[39/240] Sample processed in 3.67s, ETA: 8.25 min\n",
      "[40/240] Sample processed in 3.63s, ETA: 8.31 min\n",
      "[41/240] Sample processed in 3.60s, ETA: 8.36 min\n",
      "[42/240] Sample processed in 3.63s, ETA: 8.40 min\n",
      "[43/240] Sample processed in 3.64s, ETA: 8.44 min\n",
      "[44/240] Sample processed in 0.18s, ETA: 8.22 min\n",
      "[45/240] Sample processed in 3.61s, ETA: 8.26 min\n",
      "[46/240] Sample processed in 1.03s, ETA: 8.11 min\n",
      "[47/240] Sample processed in 1.63s, ETA: 8.01 min\n",
      "[48/240] Sample processed in 3.62s, ETA: 8.04 min\n",
      "[49/240] Sample processed in 0.86s, ETA: 7.89 min\n",
      "[50/240] Sample processed in 3.62s, ETA: 7.93 min\n",
      "[51/240] Sample processed in 1.49s, ETA: 7.82 min\n",
      "[52/240] Sample processed in 3.66s, ETA: 7.85 min\n",
      "[53/240] Sample processed in 0.10s, ETA: 7.67 min\n",
      "[54/240] Sample processed in 0.24s, ETA: 7.50 min\n",
      "[55/240] Sample processed in 0.10s, ETA: 7.33 min\n",
      "[56/240] Sample processed in 3.18s, ETA: 7.33 min\n",
      "[57/240] Sample processed in 3.69s, ETA: 7.36 min\n",
      "[58/240] Sample processed in 0.52s, ETA: 7.22 min\n",
      "[59/240] Sample processed in 0.33s, ETA: 7.08 min\n",
      "[60/240] Sample processed in 0.13s, ETA: 6.93 min\n",
      "[61/240] Sample processed in 0.78s, ETA: 6.82 min\n",
      "[62/240] Sample processed in 0.18s, ETA: 6.68 min\n",
      "[63/240] Sample processed in 3.17s, ETA: 6.68 min\n",
      "[64/240] Sample processed in 0.26s, ETA: 6.55 min\n",
      "[65/240] Sample processed in 3.67s, ETA: 6.58 min\n",
      "[66/240] Sample processed in 3.63s, ETA: 6.60 min\n",
      "[67/240] Sample processed in 3.66s, ETA: 6.63 min\n",
      "[68/240] Sample processed in 3.67s, ETA: 6.64 min\n",
      "[69/240] Sample processed in 0.11s, ETA: 6.51 min\n",
      "[70/240] Sample processed in 0.19s, ETA: 6.39 min\n",
      "[71/240] Sample processed in 3.68s, ETA: 6.41 min\n",
      "[72/240] Sample processed in 2.43s, ETA: 6.38 min\n",
      "[73/240] Sample processed in 3.65s, ETA: 6.39 min\n",
      "[74/240] Sample processed in 3.68s, ETA: 6.41 min\n",
      "[75/240] Sample processed in 3.15s, ETA: 6.40 min\n",
      "[76/240] Sample processed in 3.61s, ETA: 6.41 min\n",
      "[77/240] Sample processed in 0.15s, ETA: 6.29 min\n",
      "[78/240] Sample processed in 0.09s, ETA: 6.17 min\n",
      "[79/240] Sample processed in 3.69s, ETA: 6.18 min\n",
      "[80/240] Sample processed in 3.72s, ETA: 6.19 min\n",
      "[81/240] Sample processed in 3.68s, ETA: 6.20 min\n",
      "[82/240] Sample processed in 3.70s, ETA: 6.20 min\n",
      "[83/240] Sample processed in 3.69s, ETA: 6.21 min\n",
      "[84/240] Sample processed in 0.13s, ETA: 6.10 min\n",
      "[85/240] Sample processed in 0.41s, ETA: 6.00 min\n",
      "[86/240] Sample processed in 0.16s, ETA: 5.90 min\n",
      "[87/240] Sample processed in 3.07s, ETA: 5.88 min\n",
      "[88/240] Sample processed in 3.67s, ETA: 5.88 min\n",
      "[89/240] Sample processed in 3.71s, ETA: 5.88 min\n",
      "[90/240] Sample processed in 0.56s, ETA: 5.79 min\n",
      "[91/240] Sample processed in 0.08s, ETA: 5.69 min\n",
      "[92/240] Sample processed in 0.07s, ETA: 5.60 min\n",
      "[93/240] Sample processed in 0.15s, ETA: 5.50 min\n",
      "[94/240] Sample processed in 0.07s, ETA: 5.41 min\n",
      "[95/240] Sample processed in 0.06s, ETA: 5.32 min\n",
      "[96/240] Sample processed in 0.07s, ETA: 5.23 min\n",
      "[97/240] Sample processed in 0.07s, ETA: 5.14 min\n",
      "[98/240] Sample processed in 0.07s, ETA: 5.05 min\n",
      "[99/240] Sample processed in 0.06s, ETA: 4.97 min\n",
      "[100/240] Sample processed in 0.07s, ETA: 4.88 min\n",
      "[101/240] Sample processed in 0.07s, ETA: 4.80 min\n",
      "[102/240] Sample processed in 0.06s, ETA: 4.72 min\n",
      "[103/240] Sample processed in 0.07s, ETA: 4.64 min\n",
      "[104/240] Sample processed in 0.06s, ETA: 4.57 min\n",
      "[105/240] Sample processed in 1.97s, ETA: 4.53 min\n",
      "[106/240] Sample processed in 0.06s, ETA: 4.46 min\n",
      "[107/240] Sample processed in 0.06s, ETA: 4.38 min\n",
      "[108/240] Sample processed in 0.07s, ETA: 4.31 min\n",
      "[109/240] Sample processed in 0.24s, ETA: 4.25 min\n",
      "[110/240] Sample processed in 0.07s, ETA: 4.18 min\n",
      "[111/240] Sample processed in 0.07s, ETA: 4.11 min\n",
      "[112/240] Sample processed in 0.07s, ETA: 4.04 min\n",
      "[113/240] Sample processed in 0.06s, ETA: 3.98 min\n",
      "[114/240] Sample processed in 0.06s, ETA: 3.91 min\n",
      "[115/240] Sample processed in 0.07s, ETA: 3.85 min\n",
      "[116/240] Sample processed in 0.07s, ETA: 3.78 min\n",
      "[117/240] Sample processed in 0.07s, ETA: 3.72 min\n",
      "[118/240] Sample processed in 0.06s, ETA: 3.66 min\n",
      "[119/240] Sample processed in 0.07s, ETA: 3.60 min\n",
      "[120/240] Sample processed in 0.06s, ETA: 3.54 min\n",
      "[121/240] Sample processed in 3.69s, ETA: 3.55 min\n",
      "[122/240] Sample processed in 0.58s, ETA: 3.50 min\n",
      "[123/240] Sample processed in 1.28s, ETA: 3.46 min\n",
      "[124/240] Sample processed in 3.69s, ETA: 3.46 min\n",
      "[125/240] Sample processed in 0.09s, ETA: 3.40 min\n",
      "[126/240] Sample processed in 3.68s, ETA: 3.40 min\n",
      "[127/240] Sample processed in 0.66s, ETA: 3.36 min\n",
      "[128/240] Sample processed in 3.67s, ETA: 3.35 min\n",
      "[129/240] Sample processed in 0.95s, ETA: 3.31 min\n",
      "[130/240] Sample processed in 3.67s, ETA: 3.31 min\n",
      "[131/240] Sample processed in 0.06s, ETA: 3.25 min\n",
      "[132/240] Sample processed in 3.68s, ETA: 3.25 min\n",
      "[133/240] Sample processed in 3.66s, ETA: 3.25 min\n",
      "[134/240] Sample processed in 3.69s, ETA: 3.24 min\n",
      "[135/240] Sample processed in 3.68s, ETA: 3.23 min\n",
      "[136/240] Sample processed in 3.73s, ETA: 3.23 min\n",
      "[137/240] Sample processed in 3.70s, ETA: 3.22 min\n",
      "[138/240] Sample processed in 3.62s, ETA: 3.21 min\n",
      "[139/240] Sample processed in 3.70s, ETA: 3.20 min\n",
      "[140/240] Sample processed in 3.71s, ETA: 3.19 min\n",
      "[141/240] Sample processed in 1.73s, ETA: 3.15 min\n",
      "[142/240] Sample processed in 3.69s, ETA: 3.14 min\n",
      "[143/240] Sample processed in 3.68s, ETA: 3.13 min\n",
      "[144/240] Sample processed in 3.66s, ETA: 3.12 min\n",
      "[145/240] Sample processed in 2.71s, ETA: 3.09 min\n",
      "[146/240] Sample processed in 3.67s, ETA: 3.08 min\n",
      "[147/240] Sample processed in 3.69s, ETA: 3.07 min\n",
      "[148/240] Sample processed in 3.64s, ETA: 3.05 min\n",
      "[149/240] Sample processed in 3.71s, ETA: 3.03 min\n",
      "[150/240] Sample processed in 3.35s, ETA: 3.01 min\n",
      "[151/240] Sample processed in 0.22s, ETA: 2.96 min\n",
      "[152/240] Sample processed in 0.19s, ETA: 2.91 min\n",
      "[153/240] Sample processed in 0.64s, ETA: 2.87 min\n",
      "[154/240] Sample processed in 2.17s, ETA: 2.84 min\n",
      "[155/240] Sample processed in 0.16s, ETA: 2.79 min\n",
      "[156/240] Sample processed in 0.16s, ETA: 2.74 min\n",
      "[157/240] Sample processed in 0.30s, ETA: 2.69 min\n",
      "[158/240] Sample processed in 0.16s, ETA: 2.64 min\n",
      "[159/240] Sample processed in 0.19s, ETA: 2.59 min\n",
      "[160/240] Sample processed in 1.92s, ETA: 2.56 min\n",
      "[161/240] Sample processed in 0.16s, ETA: 2.52 min\n",
      "[162/240] Sample processed in 1.93s, ETA: 2.48 min\n",
      "[163/240] Sample processed in 3.45s, ETA: 2.46 min\n",
      "[164/240] Sample processed in 0.19s, ETA: 2.42 min\n",
      "[165/240] Sample processed in 3.25s, ETA: 2.40 min\n",
      "[166/240] Sample processed in 0.16s, ETA: 2.35 min\n",
      "[167/240] Sample processed in 0.19s, ETA: 2.31 min\n",
      "[168/240] Sample processed in 1.78s, ETA: 2.28 min\n",
      "[169/240] Sample processed in 0.19s, ETA: 2.23 min\n",
      "[170/240] Sample processed in 0.19s, ETA: 2.19 min\n",
      "[171/240] Sample processed in 0.19s, ETA: 2.15 min\n",
      "[172/240] Sample processed in 1.41s, ETA: 2.11 min\n",
      "[173/240] Sample processed in 2.70s, ETA: 2.09 min\n",
      "[174/240] Sample processed in 1.95s, ETA: 2.06 min\n",
      "[175/240] Sample processed in 0.16s, ETA: 2.01 min\n",
      "[176/240] Sample processed in 0.19s, ETA: 1.97 min\n",
      "[177/240] Sample processed in 0.19s, ETA: 1.93 min\n",
      "[178/240] Sample processed in 0.16s, ETA: 1.89 min\n",
      "[179/240] Sample processed in 0.19s, ETA: 1.85 min\n",
      "[180/240] Sample processed in 1.23s, ETA: 1.82 min\n",
      "[181/240] Sample processed in 17.82s, ETA: 1.88 min\n",
      "[182/240] Sample processed in 5.56s, ETA: 1.86 min\n",
      "[183/240] Sample processed in 3.41s, ETA: 1.84 min\n",
      "[184/240] Sample processed in 0.11s, ETA: 1.80 min\n",
      "[185/240] Sample processed in 9.70s, ETA: 1.80 min\n",
      "[186/240] Sample processed in 9.56s, ETA: 1.81 min\n",
      "[187/240] Sample processed in 12.02s, ETA: 1.82 min\n",
      "[188/240] Sample processed in 0.09s, ETA: 1.78 min\n",
      "[189/240] Sample processed in 9.24s, ETA: 1.78 min\n",
      "[190/240] Sample processed in 1.14s, ETA: 1.74 min\n",
      "[191/240] Sample processed in 13.58s, ETA: 1.75 min\n",
      "[192/240] Sample processed in 29.28s, ETA: 1.83 min\n",
      "[193/240] Sample processed in 0.10s, ETA: 1.78 min\n",
      "[194/240] Sample processed in 0.11s, ETA: 1.74 min\n",
      "[195/240] Sample processed in 6.76s, ETA: 1.71 min\n",
      "[196/240] Sample processed in 7.04s, ETA: 1.69 min\n",
      "[197/240] Sample processed in 2.14s, ETA: 1.66 min\n",
      "[198/240] Sample processed in 29.30s, ETA: 1.71 min\n",
      "[199/240] Sample processed in 0.10s, ETA: 1.66 min\n",
      "[200/240] Sample processed in 0.10s, ETA: 1.61 min\n",
      "[201/240] Sample processed in 2.06s, ETA: 1.57 min\n",
      "[202/240] Sample processed in 2.39s, ETA: 1.53 min\n",
      "[203/240] Sample processed in 0.09s, ETA: 1.49 min\n",
      "[204/240] Sample processed in 6.58s, ETA: 1.46 min\n",
      "[205/240] Sample processed in 9.95s, ETA: 1.44 min\n",
      "[206/240] Sample processed in 0.12s, ETA: 1.39 min\n",
      "[207/240] Sample processed in 0.09s, ETA: 1.34 min\n",
      "[208/240] Sample processed in 0.11s, ETA: 1.30 min\n",
      "[209/240] Sample processed in 11.07s, ETA: 1.28 min\n",
      "[210/240] Sample processed in 29.39s, ETA: 1.30 min\n",
      "[211/240] Sample processed in 29.38s, ETA: 1.32 min\n",
      "[212/240] Sample processed in 29.10s, ETA: 1.33 min\n",
      "[213/240] Sample processed in 5.97s, ETA: 1.29 min\n",
      "[214/240] Sample processed in 2.68s, ETA: 1.24 min\n",
      "[215/240] Sample processed in 0.06s, ETA: 1.19 min\n",
      "[216/240] Sample processed in 8.58s, ETA: 1.15 min\n",
      "[217/240] Sample processed in 8.26s, ETA: 1.11 min\n",
      "[218/240] Sample processed in 13.61s, ETA: 1.08 min\n",
      "[219/240] Sample processed in 9.56s, ETA: 1.04 min\n",
      "[220/240] Sample processed in 10.90s, ETA: 1.01 min\n",
      "[221/240] Sample processed in 14.49s, ETA: 0.97 min\n",
      "[222/240] Sample processed in 8.53s, ETA: 0.93 min\n",
      "[223/240] Sample processed in 7.05s, ETA: 0.88 min\n",
      "[224/240] Sample processed in 0.06s, ETA: 0.83 min\n",
      "[225/240] Sample processed in 11.64s, ETA: 0.78 min\n",
      "[226/240] Sample processed in 9.03s, ETA: 0.74 min\n",
      "[227/240] Sample processed in 28.84s, ETA: 0.71 min\n",
      "[228/240] Sample processed in 28.88s, ETA: 0.68 min\n",
      "[229/240] Sample processed in 7.21s, ETA: 0.62 min\n",
      "[230/240] Sample processed in 0.06s, ETA: 0.57 min\n",
      "[231/240] Sample processed in 0.07s, ETA: 0.51 min\n",
      "[232/240] Sample processed in 3.99s, ETA: 0.45 min\n",
      "[233/240] Sample processed in 0.07s, ETA: 0.39 min\n",
      "[234/240] Sample processed in 6.86s, ETA: 0.34 min\n",
      "[235/240] Sample processed in 9.69s, ETA: 0.28 min\n",
      "[236/240] Sample processed in 6.88s, ETA: 0.23 min\n",
      "[237/240] Sample processed in 9.66s, ETA: 0.17 min\n",
      "[238/240] Sample processed in 0.07s, ETA: 0.11 min\n",
      "[239/240] Sample processed in 13.90s, ETA: 0.06 min\n",
      "[240/240] Sample processed in 0.15s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 13.86 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### âœ… Hugging Faceì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "dataset_name = \"passionMan/test_dataset4\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split ë¡œë“œ\n",
    "\n",
    "### âœ… JSONL ì €ì¥ í•¨ìˆ˜ (í‰ê°€ ê²°ê³¼ ì €ì¥ìš©)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### âœ… ëª¨ë¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # âœ… ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸° (ë³´í†µ 4096 ë˜ëŠ” 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # âœ… ì…ë ¥ í† í° ê¸¸ì´ í™•ì¸\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # ğŸ”¥ ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ì— ë§ê²Œ ìë¦„\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # âœ… ìƒì„± ìˆ˜í–‰ (max_new_tokensì„ ì ìš©)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # âœ… ìƒì„± ê¸¸ì´ ì ìš©\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# âœ… ë°ì´í„° ê²½ë¡œ ì„¤ì • (ê²°ê³¼ ì €ì¥ìš©)\n",
    "output_json_path = \"/data/jaesung/llm_for_diabetes/src/trial3/model_response/model_output_zero_shot.jsonl\"\n",
    "\n",
    "# âœ… Taskë³„ ë°ì´í„° ê·¸ë£¹í™” (ê° íƒœìŠ¤í¬ë³„ 0~29ë²ˆ ìƒ˜í”Œ ì„ íƒ)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€í•  ë°ì´í„° ìƒì„± (ê° íƒœìŠ¤í¬ë³„ 30ê°œë§Œ ì¶”ì¶œ)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:30])  # ìµœëŒ€ 30ê°œ ì„ íƒ\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€ ì‹œì‘\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # âœ… ìƒì„±í•  í† í° ê¸¸ì´ ì„¤ì • (ìƒì„± í† í° ìˆ˜ ì¡°ì ˆ)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"ie\", \"re\"}  # ìƒì„± 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # ìƒì„± 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 128  # âœ… ìƒì„± ê¸¸ì´ 128\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 1024  # âœ… ìƒì„± ê¸¸ì´ 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # ê¸°ë³¸ê°’\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/dataset_cherry\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "1. A diabetic diet is a diet that can help you manage your blood glucose level. It should be low in fat, high in fiber, and rich in complex carbohydrates. It should also be low in calories and high in protein. You should avoid eating foods that are high in sugar and refined carbohydrates, such as white bread, pasta, and rice. Instead, opt for whole grains, fruits, and vegetables. You should also avoid eating foods that are high in saturated fat, such as red meat, butter, and cheese. Instead, opt for lean proteins, such as chicken and fish. You should also avoid eating foods that are high\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Please recommend a diet for diabetic patients.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Input:\n",
      "Please recommend a diet for diabetic patients.\n",
      "\n",
      "### Context:\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite. Dice, chop, and stirâ€”thatâ€™s all you need to create broccoli and apple salad. This healthy recipe is low in sodium and high in flavor. Caution: this recipe contains walnuts. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Veggie Rich', 'Kid Friendly', 'Salads', 'Vegetarian', 'Sides', 'Quick & Easy'])\n",
      "Chicken Nuggets with Honey Mustard Dipping Sauce: This recipe is not only delicious, but itâ€™s also kidney-friendly. Try a variety of dipping sauces, such as barbecue sauce, curry sauce, fruit spreads, or low-sodium salad dressing. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "\n",
      "### Response: \n",
      "\n",
      "Chicken Apple Crunch Salad: This savory and sweet chicken apple crunch salad will delight your taste buds by pairing fresh flavors with nutrition. Chicken apple crunch salad is delicious and light, good for lunch, dinner, or a protein-filled snack. It has been modified for the dialysis diet to encourage healthy eating and reduce food-related stress. (tags: ['CKD Non-Dialysis', 'CKD Dialysis', 'Kidney-Friendly', 'Kid Friendly', 'Main Dish', 'Budget Friendly', 'Dinner', 'Lunch', 'Quick & Easy'])\n",
      "Broccoli and Apple Salad: This kidney-friendly recipe is a kid favorite\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# 2ï¸âƒ£ ìŒì‹ ë°ì´í„° ë¡œë“œ & FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "df = pd.read_csv(\"processed_food_data.csv\")  # ìŒì‹ ë°ì´í„° ë¡œë“œ\n",
    "index = faiss.read_index(\"food_faiss.index\")  # FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "embedding_model = SentenceTransformer(\"jhgan/ko-sbert-nli\")  # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸\n",
    "\n",
    "# 3ï¸âƒ£ ìŒì‹ ê²€ìƒ‰ í•¨ìˆ˜ (FAISS ì‚¬ìš©)\n",
    "def search_food(query, top_k=3):\n",
    "    \"\"\"ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ FAISSì—ì„œ ìœ ì‚¬í•œ ìŒì‹ ê²€ìƒ‰\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # ê²€ìƒ‰ëœ ìŒì‹ ì •ë³´ ì¶”ì¶œ\n",
    "    results = df.iloc[indices[0]][[\"title\", \"description\", \"tags\"]]\n",
    "    \n",
    "    # ê²€ìƒ‰ëœ ìŒì‹ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    search_context = \"\\n\".join([f\"{row['title']}: {row['description']} (tags: {row['tags']})\" for _, row in results.iterrows()])\n",
    "    return search_context\n",
    "\n",
    "# 4ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ë° ê²€ìƒ‰ ìˆ˜í–‰\n",
    "query = \"Please recommend a diet for diabetic patients.\"\n",
    "search_results = search_food(query)\n",
    "\n",
    "# 5ï¸âƒ£ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_text = alpaca_prompt.format(\n",
    "    \"Please recommend a diet for diabetic patients.\",\n",
    "    query,\n",
    "    search_results\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
