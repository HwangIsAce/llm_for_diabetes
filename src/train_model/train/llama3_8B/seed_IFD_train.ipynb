{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b4191fde97421eabd26e6febb87484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 15156\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "seed = load_dataset(\"passionMan/real_seed_IFD\")\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 13:33:59 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f58a75a5187427588fc32d8161affb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial8/train/llama3_8B/outputs/real_seed_IFD_rIFD14/checkpoint-474\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/real_seed_IFD\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4b8807352e45a9a3680f60036595c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495ce06ac7c40a0926fa6c073152663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b1f0d0dcf848e599d187ed92621109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/15156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 150, # 50\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/real_seed_IFD\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 15,156 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 474\n",
      " \"-____-\"     Number of trainable parameters = 13,631,488\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 1:25:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/900] Sample processed in 3.57s, ETA: 53.54 min\n",
      "[2/900] Sample processed in 4.11s, ETA: 57.47 min\n",
      "[3/900] Sample processed in 9.15s, ETA: 83.86 min\n",
      "[4/900] Sample processed in 4.13s, ETA: 78.23 min\n",
      "[5/900] Sample processed in 3.48s, ETA: 72.91 min\n",
      "[6/900] Sample processed in 10.79s, ETA: 87.49 min\n",
      "[7/900] Sample processed in 9.05s, ETA: 94.16 min\n",
      "[8/900] Sample processed in 5.20s, ETA: 91.95 min\n",
      "[9/900] Sample processed in 4.33s, ETA: 88.79 min\n",
      "[10/900] Sample processed in 4.85s, ETA: 87.03 min\n",
      "[11/900] Sample processed in 6.39s, ETA: 87.63 min\n",
      "[12/900] Sample processed in 4.95s, ETA: 86.35 min\n",
      "[13/900] Sample processed in 5.55s, ETA: 85.93 min\n",
      "[14/900] Sample processed in 5.84s, ETA: 85.86 min\n",
      "[15/900] Sample processed in 7.08s, ETA: 87.01 min\n",
      "[16/900] Sample processed in 31.42s, ETA: 110.41 min\n",
      "[17/900] Sample processed in 10.21s, ETA: 112.63 min\n",
      "[18/900] Sample processed in 9.99s, ETA: 114.41 min\n",
      "[19/900] Sample processed in 4.23s, ETA: 111.54 min\n",
      "[20/900] Sample processed in 4.28s, ETA: 108.98 min\n",
      "[21/900] Sample processed in 6.72s, ETA: 108.36 min\n",
      "[22/900] Sample processed in 10.13s, ETA: 110.06 min\n",
      "[23/900] Sample processed in 4.88s, ETA: 108.26 min\n",
      "[24/900] Sample processed in 7.61s, ETA: 108.26 min\n",
      "[25/900] Sample processed in 8.63s, ETA: 108.84 min\n",
      "[26/900] Sample processed in 6.37s, ETA: 108.11 min\n",
      "[27/900] Sample processed in 3.75s, ETA: 106.01 min\n",
      "[28/900] Sample processed in 6.97s, ETA: 105.72 min\n",
      "[29/900] Sample processed in 6.69s, ETA: 105.31 min\n",
      "[30/900] Sample processed in 8.93s, ETA: 105.99 min\n",
      "[31/900] Sample processed in 8.31s, ETA: 106.34 min\n",
      "[32/900] Sample processed in 4.01s, ETA: 104.71 min\n",
      "[33/900] Sample processed in 5.30s, ETA: 103.74 min\n",
      "[34/900] Sample processed in 7.82s, ETA: 103.89 min\n",
      "[35/900] Sample processed in 4.36s, ETA: 102.60 min\n",
      "[36/900] Sample processed in 5.99s, ETA: 102.03 min\n",
      "[37/900] Sample processed in 5.45s, ETA: 101.27 min\n",
      "[38/900] Sample processed in 5.73s, ETA: 100.66 min\n",
      "[39/900] Sample processed in 3.73s, ETA: 99.34 min\n",
      "[40/900] Sample processed in 5.41s, ETA: 98.68 min\n",
      "[41/900] Sample processed in 4.04s, ETA: 97.57 min\n",
      "[42/900] Sample processed in 5.59s, ETA: 97.04 min\n",
      "[43/900] Sample processed in 8.34s, ETA: 97.45 min\n",
      "[44/900] Sample processed in 4.46s, ETA: 96.57 min\n",
      "[45/900] Sample processed in 4.00s, ETA: 95.58 min\n",
      "[46/900] Sample processed in 4.50s, ETA: 94.78 min\n",
      "[47/900] Sample processed in 6.32s, ETA: 94.57 min\n",
      "[48/900] Sample processed in 6.40s, ETA: 94.38 min\n",
      "[49/900] Sample processed in 7.42s, ETA: 94.50 min\n",
      "[50/900] Sample processed in 5.53s, ETA: 94.06 min\n",
      "[51/900] Sample processed in 6.09s, ETA: 93.80 min\n",
      "[52/900] Sample processed in 8.71s, ETA: 94.26 min\n",
      "[53/900] Sample processed in 3.91s, ETA: 93.41 min\n",
      "[54/900] Sample processed in 7.21s, ETA: 93.45 min\n",
      "[55/900] Sample processed in 7.21s, ETA: 93.49 min\n",
      "[56/900] Sample processed in 4.73s, ETA: 92.90 min\n",
      "[57/900] Sample processed in 3.88s, ETA: 92.12 min\n",
      "[58/900] Sample processed in 3.63s, ETA: 91.30 min\n",
      "[59/900] Sample processed in 4.39s, ETA: 90.69 min\n",
      "[60/900] Sample processed in 6.55s, ETA: 90.60 min\n",
      "[61/900] Sample processed in 6.93s, ETA: 90.60 min\n",
      "[62/900] Sample processed in 8.33s, ETA: 90.91 min\n",
      "[63/900] Sample processed in 4.40s, ETA: 90.33 min\n",
      "[64/900] Sample processed in 12.31s, ETA: 91.50 min\n",
      "[65/900] Sample processed in 2.99s, ETA: 90.62 min\n",
      "[66/900] Sample processed in 4.51s, ETA: 90.09 min\n",
      "[67/900] Sample processed in 11.24s, ETA: 90.97 min\n",
      "[68/900] Sample processed in 7.74s, ETA: 91.10 min\n",
      "[69/900] Sample processed in 4.15s, ETA: 90.51 min\n",
      "[70/900] Sample processed in 12.57s, ETA: 91.59 min\n",
      "[71/900] Sample processed in 3.97s, ETA: 90.97 min\n",
      "[72/900] Sample processed in 5.71s, ETA: 90.69 min\n",
      "[73/900] Sample processed in 3.78s, ETA: 90.05 min\n",
      "[74/900] Sample processed in 10.94s, ETA: 90.76 min\n",
      "[75/900] Sample processed in 6.41s, ETA: 90.62 min\n",
      "[76/900] Sample processed in 8.11s, ETA: 90.78 min\n",
      "[77/900] Sample processed in 4.70s, ETA: 90.33 min\n",
      "[78/900] Sample processed in 3.91s, ETA: 89.75 min\n",
      "[79/900] Sample processed in 4.09s, ETA: 89.22 min\n",
      "[80/900] Sample processed in 5.28s, ETA: 88.90 min\n",
      "[81/900] Sample processed in 4.84s, ETA: 88.51 min\n",
      "[82/900] Sample processed in 4.51s, ETA: 88.07 min\n",
      "[83/900] Sample processed in 2.66s, ETA: 87.34 min\n",
      "[84/900] Sample processed in 4.66s, ETA: 86.95 min\n",
      "[85/900] Sample processed in 6.10s, ETA: 86.80 min\n",
      "[86/900] Sample processed in 7.78s, ETA: 86.91 min\n",
      "[87/900] Sample processed in 2.90s, ETA: 86.26 min\n",
      "[88/900] Sample processed in 4.36s, ETA: 85.84 min\n",
      "[89/900] Sample processed in 3.89s, ETA: 85.36 min\n",
      "[90/900] Sample processed in 7.04s, ETA: 85.37 min\n",
      "[91/900] Sample processed in 4.26s, ETA: 84.96 min\n",
      "[92/900] Sample processed in 7.80s, ETA: 85.07 min\n",
      "[93/900] Sample processed in 4.66s, ETA: 84.73 min\n",
      "[94/900] Sample processed in 5.09s, ETA: 84.45 min\n",
      "[95/900] Sample processed in 4.82s, ETA: 84.14 min\n",
      "[96/900] Sample processed in 5.44s, ETA: 83.92 min\n",
      "[97/900] Sample processed in 4.82s, ETA: 83.61 min\n",
      "[98/900] Sample processed in 5.48s, ETA: 83.40 min\n",
      "[99/900] Sample processed in 4.91s, ETA: 83.12 min\n",
      "[100/900] Sample processed in 4.88s, ETA: 82.84 min\n",
      "[101/900] Sample processed in 0.83s, ETA: 82.02 min\n",
      "[102/900] Sample processed in 0.74s, ETA: 81.21 min\n",
      "[103/900] Sample processed in 1.02s, ETA: 80.46 min\n",
      "[104/900] Sample processed in 1.03s, ETA: 79.71 min\n",
      "[105/900] Sample processed in 1.02s, ETA: 78.98 min\n",
      "[106/900] Sample processed in 1.03s, ETA: 78.27 min\n",
      "[107/900] Sample processed in 0.31s, ETA: 77.48 min\n",
      "[108/900] Sample processed in 1.02s, ETA: 76.79 min\n",
      "[109/900] Sample processed in 1.00s, ETA: 76.11 min\n",
      "[110/900] Sample processed in 1.03s, ETA: 75.45 min\n",
      "[111/900] Sample processed in 0.81s, ETA: 74.77 min\n",
      "[112/900] Sample processed in 1.01s, ETA: 74.12 min\n",
      "[113/900] Sample processed in 0.58s, ETA: 73.44 min\n",
      "[114/900] Sample processed in 1.02s, ETA: 72.82 min\n",
      "[115/900] Sample processed in 0.75s, ETA: 72.18 min\n",
      "[116/900] Sample processed in 0.97s, ETA: 71.58 min\n",
      "[117/900] Sample processed in 1.04s, ETA: 70.99 min\n",
      "[118/900] Sample processed in 1.00s, ETA: 70.41 min\n",
      "[119/900] Sample processed in 0.44s, ETA: 69.78 min\n",
      "[120/900] Sample processed in 0.83s, ETA: 69.20 min\n",
      "[121/900] Sample processed in 1.01s, ETA: 68.65 min\n",
      "[122/900] Sample processed in 0.25s, ETA: 68.03 min\n",
      "[123/900] Sample processed in 1.03s, ETA: 67.50 min\n",
      "[124/900] Sample processed in 0.61s, ETA: 66.93 min\n",
      "[125/900] Sample processed in 0.49s, ETA: 66.36 min\n",
      "[126/900] Sample processed in 0.61s, ETA: 65.81 min\n",
      "[127/900] Sample processed in 1.02s, ETA: 65.31 min\n",
      "[128/900] Sample processed in 1.01s, ETA: 64.82 min\n",
      "[129/900] Sample processed in 1.01s, ETA: 64.33 min\n",
      "[130/900] Sample processed in 0.31s, ETA: 63.79 min\n",
      "[131/900] Sample processed in 1.02s, ETA: 63.32 min\n",
      "[132/900] Sample processed in 0.56s, ETA: 62.81 min\n",
      "[133/900] Sample processed in 0.60s, ETA: 62.31 min\n",
      "[134/900] Sample processed in 1.02s, ETA: 61.86 min\n",
      "[135/900] Sample processed in 1.04s, ETA: 61.42 min\n",
      "[136/900] Sample processed in 1.02s, ETA: 60.99 min\n",
      "[137/900] Sample processed in 1.03s, ETA: 60.56 min\n",
      "[138/900] Sample processed in 1.01s, ETA: 60.14 min\n",
      "[139/900] Sample processed in 1.04s, ETA: 59.72 min\n",
      "[140/900] Sample processed in 1.02s, ETA: 59.31 min\n",
      "[141/900] Sample processed in 0.51s, ETA: 58.85 min\n",
      "[142/900] Sample processed in 1.01s, ETA: 58.45 min\n",
      "[143/900] Sample processed in 0.59s, ETA: 58.02 min\n",
      "[144/900] Sample processed in 0.79s, ETA: 57.61 min\n",
      "[145/900] Sample processed in 1.02s, ETA: 57.23 min\n",
      "[146/900] Sample processed in 0.74s, ETA: 56.82 min\n",
      "[147/900] Sample processed in 1.02s, ETA: 56.45 min\n",
      "[148/900] Sample processed in 0.92s, ETA: 56.07 min\n",
      "[149/900] Sample processed in 1.02s, ETA: 55.71 min\n",
      "[150/900] Sample processed in 1.02s, ETA: 55.35 min\n",
      "[151/900] Sample processed in 0.53s, ETA: 54.95 min\n",
      "[152/900] Sample processed in 1.03s, ETA: 54.60 min\n",
      "[153/900] Sample processed in 1.02s, ETA: 54.25 min\n",
      "[154/900] Sample processed in 1.01s, ETA: 53.91 min\n",
      "[155/900] Sample processed in 0.99s, ETA: 53.57 min\n",
      "[156/900] Sample processed in 1.02s, ETA: 53.24 min\n",
      "[157/900] Sample processed in 1.02s, ETA: 52.91 min\n",
      "[158/900] Sample processed in 1.03s, ETA: 52.58 min\n",
      "[159/900] Sample processed in 0.65s, ETA: 52.23 min\n",
      "[160/900] Sample processed in 0.84s, ETA: 51.90 min\n",
      "[161/900] Sample processed in 1.02s, ETA: 51.58 min\n",
      "[162/900] Sample processed in 0.89s, ETA: 51.26 min\n",
      "[163/900] Sample processed in 0.71s, ETA: 50.93 min\n",
      "[164/900] Sample processed in 1.02s, ETA: 50.63 min\n",
      "[165/900] Sample processed in 1.02s, ETA: 50.33 min\n",
      "[166/900] Sample processed in 0.62s, ETA: 50.01 min\n",
      "[167/900] Sample processed in 1.02s, ETA: 49.71 min\n",
      "[168/900] Sample processed in 1.03s, ETA: 49.43 min\n",
      "[169/900] Sample processed in 1.03s, ETA: 49.14 min\n",
      "[170/900] Sample processed in 0.38s, ETA: 48.81 min\n",
      "[171/900] Sample processed in 1.00s, ETA: 48.53 min\n",
      "[172/900] Sample processed in 1.02s, ETA: 48.25 min\n",
      "[173/900] Sample processed in 1.03s, ETA: 47.98 min\n",
      "[174/900] Sample processed in 1.03s, ETA: 47.71 min\n",
      "[175/900] Sample processed in 0.44s, ETA: 47.40 min\n",
      "[176/900] Sample processed in 1.02s, ETA: 47.14 min\n",
      "[177/900] Sample processed in 0.83s, ETA: 46.86 min\n",
      "[178/900] Sample processed in 0.58s, ETA: 46.58 min\n",
      "[179/900] Sample processed in 0.85s, ETA: 46.31 min\n",
      "[180/900] Sample processed in 1.01s, ETA: 46.05 min\n",
      "[181/900] Sample processed in 1.04s, ETA: 45.80 min\n",
      "[182/900] Sample processed in 1.00s, ETA: 45.56 min\n",
      "[183/900] Sample processed in 0.88s, ETA: 45.30 min\n",
      "[184/900] Sample processed in 1.04s, ETA: 45.06 min\n",
      "[185/900] Sample processed in 0.71s, ETA: 44.80 min\n",
      "[186/900] Sample processed in 1.03s, ETA: 44.56 min\n",
      "[187/900] Sample processed in 0.31s, ETA: 44.28 min\n",
      "[188/900] Sample processed in 1.02s, ETA: 44.05 min\n",
      "[189/900] Sample processed in 0.69s, ETA: 43.80 min\n",
      "[190/900] Sample processed in 1.00s, ETA: 43.57 min\n",
      "[191/900] Sample processed in 1.02s, ETA: 43.34 min\n",
      "[192/900] Sample processed in 1.03s, ETA: 43.12 min\n",
      "[193/900] Sample processed in 0.50s, ETA: 42.87 min\n",
      "[194/900] Sample processed in 0.58s, ETA: 42.62 min\n",
      "[195/900] Sample processed in 0.28s, ETA: 42.36 min\n",
      "[196/900] Sample processed in 1.02s, ETA: 42.14 min\n",
      "[197/900] Sample processed in 1.03s, ETA: 41.93 min\n",
      "[198/900] Sample processed in 0.57s, ETA: 41.69 min\n",
      "[199/900] Sample processed in 1.04s, ETA: 41.49 min\n",
      "[200/900] Sample processed in 1.02s, ETA: 41.28 min\n",
      "[201/900] Sample processed in 0.15s, ETA: 41.02 min\n",
      "[202/900] Sample processed in 0.09s, ETA: 40.77 min\n",
      "[203/900] Sample processed in 0.15s, ETA: 40.52 min\n",
      "[204/900] Sample processed in 0.16s, ETA: 40.27 min\n",
      "[205/900] Sample processed in 0.09s, ETA: 40.02 min\n",
      "[206/900] Sample processed in 0.12s, ETA: 39.78 min\n",
      "[207/900] Sample processed in 0.16s, ETA: 39.54 min\n",
      "[208/900] Sample processed in 0.16s, ETA: 39.30 min\n",
      "[209/900] Sample processed in 0.09s, ETA: 39.06 min\n",
      "[210/900] Sample processed in 0.12s, ETA: 38.82 min\n",
      "[211/900] Sample processed in 0.12s, ETA: 38.59 min\n",
      "[212/900] Sample processed in 0.16s, ETA: 38.36 min\n",
      "[213/900] Sample processed in 0.12s, ETA: 38.13 min\n",
      "[214/900] Sample processed in 0.16s, ETA: 37.91 min\n",
      "[215/900] Sample processed in 0.09s, ETA: 37.68 min\n",
      "[216/900] Sample processed in 0.09s, ETA: 37.46 min\n",
      "[217/900] Sample processed in 0.16s, ETA: 37.24 min\n",
      "[218/900] Sample processed in 0.15s, ETA: 37.02 min\n",
      "[219/900] Sample processed in 0.09s, ETA: 36.80 min\n",
      "[220/900] Sample processed in 0.09s, ETA: 36.59 min\n",
      "[221/900] Sample processed in 0.09s, ETA: 36.37 min\n",
      "[222/900] Sample processed in 0.15s, ETA: 36.16 min\n",
      "[223/900] Sample processed in 0.15s, ETA: 35.95 min\n",
      "[224/900] Sample processed in 0.16s, ETA: 35.75 min\n",
      "[225/900] Sample processed in 0.15s, ETA: 35.54 min\n",
      "[226/900] Sample processed in 0.15s, ETA: 35.34 min\n",
      "[227/900] Sample processed in 0.15s, ETA: 35.14 min\n",
      "[228/900] Sample processed in 0.16s, ETA: 34.94 min\n",
      "[229/900] Sample processed in 0.16s, ETA: 34.75 min\n",
      "[230/900] Sample processed in 0.16s, ETA: 34.55 min\n",
      "[231/900] Sample processed in 0.14s, ETA: 34.36 min\n",
      "[232/900] Sample processed in 0.21s, ETA: 34.17 min\n",
      "[233/900] Sample processed in 0.16s, ETA: 33.98 min\n",
      "[234/900] Sample processed in 0.12s, ETA: 33.79 min\n",
      "[235/900] Sample processed in 0.12s, ETA: 33.60 min\n",
      "[236/900] Sample processed in 0.12s, ETA: 33.41 min\n",
      "[237/900] Sample processed in 0.11s, ETA: 33.23 min\n",
      "[238/900] Sample processed in 0.20s, ETA: 33.05 min\n",
      "[239/900] Sample processed in 0.12s, ETA: 32.86 min\n",
      "[240/900] Sample processed in 0.12s, ETA: 32.68 min\n",
      "[241/900] Sample processed in 0.16s, ETA: 32.51 min\n",
      "[242/900] Sample processed in 0.12s, ETA: 32.33 min\n",
      "[243/900] Sample processed in 0.20s, ETA: 32.15 min\n",
      "[244/900] Sample processed in 0.12s, ETA: 31.98 min\n",
      "[245/900] Sample processed in 0.19s, ETA: 31.81 min\n",
      "[246/900] Sample processed in 0.20s, ETA: 31.64 min\n",
      "[247/900] Sample processed in 0.16s, ETA: 31.47 min\n",
      "[248/900] Sample processed in 0.16s, ETA: 31.30 min\n",
      "[249/900] Sample processed in 0.12s, ETA: 31.13 min\n",
      "[250/900] Sample processed in 0.20s, ETA: 30.97 min\n",
      "[251/900] Sample processed in 0.20s, ETA: 30.81 min\n",
      "[252/900] Sample processed in 0.20s, ETA: 30.65 min\n",
      "[253/900] Sample processed in 0.11s, ETA: 30.48 min\n",
      "[254/900] Sample processed in 0.20s, ETA: 30.32 min\n",
      "[255/900] Sample processed in 0.12s, ETA: 30.16 min\n",
      "[256/900] Sample processed in 0.12s, ETA: 30.00 min\n",
      "[257/900] Sample processed in 0.19s, ETA: 29.85 min\n",
      "[258/900] Sample processed in 0.16s, ETA: 29.69 min\n",
      "[259/900] Sample processed in 0.19s, ETA: 29.54 min\n",
      "[260/900] Sample processed in 0.19s, ETA: 29.39 min\n",
      "[261/900] Sample processed in 0.11s, ETA: 29.24 min\n",
      "[262/900] Sample processed in 0.12s, ETA: 29.08 min\n",
      "[263/900] Sample processed in 0.16s, ETA: 28.93 min\n",
      "[264/900] Sample processed in 0.16s, ETA: 28.79 min\n",
      "[265/900] Sample processed in 0.12s, ETA: 28.64 min\n",
      "[266/900] Sample processed in 0.12s, ETA: 28.49 min\n",
      "[267/900] Sample processed in 0.16s, ETA: 28.34 min\n",
      "[268/900] Sample processed in 0.20s, ETA: 28.20 min\n",
      "[269/900] Sample processed in 0.16s, ETA: 28.06 min\n",
      "[270/900] Sample processed in 0.12s, ETA: 27.91 min\n",
      "[271/900] Sample processed in 0.11s, ETA: 27.77 min\n",
      "[272/900] Sample processed in 0.20s, ETA: 27.63 min\n",
      "[273/900] Sample processed in 0.15s, ETA: 27.49 min\n",
      "[274/900] Sample processed in 0.20s, ETA: 27.36 min\n",
      "[275/900] Sample processed in 0.20s, ETA: 27.22 min\n",
      "[276/900] Sample processed in 0.11s, ETA: 27.08 min\n",
      "[277/900] Sample processed in 0.20s, ETA: 26.95 min\n",
      "[278/900] Sample processed in 0.20s, ETA: 26.82 min\n",
      "[279/900] Sample processed in 0.12s, ETA: 26.68 min\n",
      "[280/900] Sample processed in 0.15s, ETA: 26.55 min\n",
      "[281/900] Sample processed in 0.16s, ETA: 26.42 min\n",
      "[282/900] Sample processed in 0.16s, ETA: 26.29 min\n",
      "[283/900] Sample processed in 0.16s, ETA: 26.16 min\n",
      "[284/900] Sample processed in 0.11s, ETA: 26.03 min\n",
      "[285/900] Sample processed in 0.12s, ETA: 25.90 min\n",
      "[286/900] Sample processed in 0.16s, ETA: 25.77 min\n",
      "[287/900] Sample processed in 0.12s, ETA: 25.64 min\n",
      "[288/900] Sample processed in 0.12s, ETA: 25.52 min\n",
      "[289/900] Sample processed in 0.20s, ETA: 25.40 min\n",
      "[290/900] Sample processed in 0.11s, ETA: 25.27 min\n",
      "[291/900] Sample processed in 0.12s, ETA: 25.15 min\n",
      "[292/900] Sample processed in 0.20s, ETA: 25.03 min\n",
      "[293/900] Sample processed in 0.12s, ETA: 24.90 min\n",
      "[294/900] Sample processed in 0.20s, ETA: 24.78 min\n",
      "[295/900] Sample processed in 0.12s, ETA: 24.66 min\n",
      "[296/900] Sample processed in 0.15s, ETA: 24.54 min\n",
      "[297/900] Sample processed in 0.12s, ETA: 24.43 min\n",
      "[298/900] Sample processed in 0.16s, ETA: 24.31 min\n",
      "[299/900] Sample processed in 0.16s, ETA: 24.19 min\n",
      "[300/900] Sample processed in 0.16s, ETA: 24.08 min\n",
      "[301/900] Sample processed in 0.36s, ETA: 23.97 min\n",
      "[302/900] Sample processed in 0.32s, ETA: 23.86 min\n",
      "[303/900] Sample processed in 0.12s, ETA: 23.75 min\n",
      "[304/900] Sample processed in 0.40s, ETA: 23.64 min\n",
      "[305/900] Sample processed in 0.52s, ETA: 23.54 min\n",
      "[306/900] Sample processed in 0.44s, ETA: 23.44 min\n",
      "[307/900] Sample processed in 0.32s, ETA: 23.33 min\n",
      "[308/900] Sample processed in 0.12s, ETA: 23.22 min\n",
      "[309/900] Sample processed in 0.12s, ETA: 23.11 min\n",
      "[310/900] Sample processed in 0.12s, ETA: 23.00 min\n",
      "[311/900] Sample processed in 0.28s, ETA: 22.90 min\n",
      "[312/900] Sample processed in 0.12s, ETA: 22.79 min\n",
      "[313/900] Sample processed in 0.28s, ETA: 22.69 min\n",
      "[314/900] Sample processed in 0.12s, ETA: 22.58 min\n",
      "[315/900] Sample processed in 0.36s, ETA: 22.48 min\n",
      "[316/900] Sample processed in 0.44s, ETA: 22.39 min\n",
      "[317/900] Sample processed in 0.12s, ETA: 22.28 min\n",
      "[318/900] Sample processed in 0.12s, ETA: 22.18 min\n",
      "[319/900] Sample processed in 0.12s, ETA: 22.07 min\n",
      "[320/900] Sample processed in 0.12s, ETA: 21.97 min\n",
      "[321/900] Sample processed in 0.28s, ETA: 21.87 min\n",
      "[322/900] Sample processed in 0.12s, ETA: 21.77 min\n",
      "[323/900] Sample processed in 0.28s, ETA: 21.67 min\n",
      "[324/900] Sample processed in 0.32s, ETA: 21.58 min\n",
      "[325/900] Sample processed in 0.28s, ETA: 21.48 min\n",
      "[326/900] Sample processed in 0.12s, ETA: 21.38 min\n",
      "[327/900] Sample processed in 0.52s, ETA: 21.29 min\n",
      "[328/900] Sample processed in 0.48s, ETA: 21.21 min\n",
      "[329/900] Sample processed in 0.39s, ETA: 21.12 min\n",
      "[330/900] Sample processed in 0.63s, ETA: 21.03 min\n",
      "[331/900] Sample processed in 0.76s, ETA: 20.96 min\n",
      "[332/900] Sample processed in 0.47s, ETA: 20.87 min\n",
      "[333/900] Sample processed in 0.32s, ETA: 20.78 min\n",
      "[334/900] Sample processed in 0.52s, ETA: 20.69 min\n",
      "[335/900] Sample processed in 0.12s, ETA: 20.60 min\n",
      "[336/900] Sample processed in 0.40s, ETA: 20.51 min\n",
      "[337/900] Sample processed in 0.33s, ETA: 20.43 min\n",
      "[338/900] Sample processed in 0.12s, ETA: 20.33 min\n",
      "[339/900] Sample processed in 0.28s, ETA: 20.24 min\n",
      "[340/900] Sample processed in 0.43s, ETA: 20.16 min\n",
      "[341/900] Sample processed in 0.52s, ETA: 20.08 min\n",
      "[342/900] Sample processed in 0.12s, ETA: 19.99 min\n",
      "[343/900] Sample processed in 0.32s, ETA: 19.90 min\n",
      "[344/900] Sample processed in 0.12s, ETA: 19.81 min\n",
      "[345/900] Sample processed in 0.40s, ETA: 19.73 min\n",
      "[346/900] Sample processed in 0.24s, ETA: 19.64 min\n",
      "[347/900] Sample processed in 0.11s, ETA: 19.55 min\n",
      "[348/900] Sample processed in 0.12s, ETA: 19.47 min\n",
      "[349/900] Sample processed in 0.56s, ETA: 19.39 min\n",
      "[350/900] Sample processed in 0.51s, ETA: 19.31 min\n",
      "[351/900] Sample processed in 0.24s, ETA: 19.23 min\n",
      "[352/900] Sample processed in 0.56s, ETA: 19.15 min\n",
      "[353/900] Sample processed in 0.32s, ETA: 19.07 min\n",
      "[354/900] Sample processed in 0.32s, ETA: 18.99 min\n",
      "[355/900] Sample processed in 0.12s, ETA: 18.91 min\n",
      "[356/900] Sample processed in 0.52s, ETA: 18.83 min\n",
      "[357/900] Sample processed in 0.40s, ETA: 18.76 min\n",
      "[358/900] Sample processed in 0.12s, ETA: 18.67 min\n",
      "[359/900] Sample processed in 0.48s, ETA: 18.60 min\n",
      "[360/900] Sample processed in 0.44s, ETA: 18.52 min\n",
      "[361/900] Sample processed in 0.32s, ETA: 18.45 min\n",
      "[362/900] Sample processed in 0.35s, ETA: 18.37 min\n",
      "[363/900] Sample processed in 0.28s, ETA: 18.29 min\n",
      "[364/900] Sample processed in 0.28s, ETA: 18.21 min\n",
      "[365/900] Sample processed in 0.52s, ETA: 18.14 min\n",
      "[366/900] Sample processed in 0.44s, ETA: 18.07 min\n",
      "[367/900] Sample processed in 0.40s, ETA: 18.00 min\n",
      "[368/900] Sample processed in 0.12s, ETA: 17.92 min\n",
      "[369/900] Sample processed in 0.28s, ETA: 17.84 min\n",
      "[370/900] Sample processed in 0.40s, ETA: 17.77 min\n",
      "[371/900] Sample processed in 0.11s, ETA: 17.69 min\n",
      "[372/900] Sample processed in 0.12s, ETA: 17.61 min\n",
      "[373/900] Sample processed in 0.11s, ETA: 17.54 min\n",
      "[374/900] Sample processed in 0.12s, ETA: 17.46 min\n",
      "[375/900] Sample processed in 0.28s, ETA: 17.38 min\n",
      "[376/900] Sample processed in 0.48s, ETA: 17.32 min\n",
      "[377/900] Sample processed in 0.11s, ETA: 17.24 min\n",
      "[378/900] Sample processed in 0.12s, ETA: 17.16 min\n",
      "[379/900] Sample processed in 0.24s, ETA: 17.09 min\n",
      "[380/900] Sample processed in 0.36s, ETA: 17.02 min\n",
      "[381/900] Sample processed in 0.44s, ETA: 16.96 min\n",
      "[382/900] Sample processed in 1.00s, ETA: 16.90 min\n",
      "[383/900] Sample processed in 0.32s, ETA: 16.83 min\n",
      "[384/900] Sample processed in 0.80s, ETA: 16.77 min\n",
      "[385/900] Sample processed in 0.12s, ETA: 16.70 min\n",
      "[386/900] Sample processed in 0.32s, ETA: 16.63 min\n",
      "[387/900] Sample processed in 0.32s, ETA: 16.56 min\n",
      "[388/900] Sample processed in 0.32s, ETA: 16.50 min\n",
      "[389/900] Sample processed in 0.28s, ETA: 16.43 min\n",
      "[390/900] Sample processed in 0.23s, ETA: 16.36 min\n",
      "[391/900] Sample processed in 1.29s, ETA: 16.31 min\n",
      "[392/900] Sample processed in 0.13s, ETA: 16.24 min\n",
      "[393/900] Sample processed in 0.34s, ETA: 16.17 min\n",
      "[394/900] Sample processed in 0.12s, ETA: 16.10 min\n",
      "[395/900] Sample processed in 0.32s, ETA: 16.04 min\n",
      "[396/900] Sample processed in 0.36s, ETA: 15.97 min\n",
      "[397/900] Sample processed in 0.28s, ETA: 15.91 min\n",
      "[398/900] Sample processed in 0.28s, ETA: 15.84 min\n",
      "[399/900] Sample processed in 0.24s, ETA: 15.78 min\n",
      "[400/900] Sample processed in 0.12s, ETA: 15.71 min\n",
      "[401/900] Sample processed in 0.56s, ETA: 15.65 min\n",
      "[402/900] Sample processed in 0.12s, ETA: 15.58 min\n",
      "[403/900] Sample processed in 0.32s, ETA: 15.52 min\n",
      "[404/900] Sample processed in 0.12s, ETA: 15.45 min\n",
      "[405/900] Sample processed in 0.12s, ETA: 15.38 min\n",
      "[406/900] Sample processed in 0.36s, ETA: 15.32 min\n",
      "[407/900] Sample processed in 0.44s, ETA: 15.26 min\n",
      "[408/900] Sample processed in 0.28s, ETA: 15.20 min\n",
      "[409/900] Sample processed in 0.28s, ETA: 15.14 min\n",
      "[410/900] Sample processed in 0.12s, ETA: 15.07 min\n",
      "[411/900] Sample processed in 0.12s, ETA: 15.01 min\n",
      "[412/900] Sample processed in 0.11s, ETA: 14.94 min\n",
      "[413/900] Sample processed in 0.28s, ETA: 14.88 min\n",
      "[414/900] Sample processed in 0.28s, ETA: 14.82 min\n",
      "[415/900] Sample processed in 0.12s, ETA: 14.76 min\n",
      "[416/900] Sample processed in 0.12s, ETA: 14.69 min\n",
      "[417/900] Sample processed in 0.11s, ETA: 14.63 min\n",
      "[418/900] Sample processed in 0.12s, ETA: 14.57 min\n",
      "[419/900] Sample processed in 0.12s, ETA: 14.50 min\n",
      "[420/900] Sample processed in 0.20s, ETA: 14.44 min\n",
      "[421/900] Sample processed in 0.11s, ETA: 14.38 min\n",
      "[422/900] Sample processed in 0.12s, ETA: 14.32 min\n",
      "[423/900] Sample processed in 0.12s, ETA: 14.26 min\n",
      "[424/900] Sample processed in 0.32s, ETA: 14.20 min\n",
      "[425/900] Sample processed in 0.31s, ETA: 14.14 min\n",
      "[426/900] Sample processed in 0.32s, ETA: 14.09 min\n",
      "[427/900] Sample processed in 0.32s, ETA: 14.03 min\n",
      "[428/900] Sample processed in 0.11s, ETA: 13.97 min\n",
      "[429/900] Sample processed in 0.12s, ETA: 13.91 min\n",
      "[430/900] Sample processed in 0.11s, ETA: 13.85 min\n",
      "[431/900] Sample processed in 0.12s, ETA: 13.79 min\n",
      "[432/900] Sample processed in 0.29s, ETA: 13.73 min\n",
      "[433/900] Sample processed in 0.24s, ETA: 13.68 min\n",
      "[434/900] Sample processed in 0.12s, ETA: 13.62 min\n",
      "[435/900] Sample processed in 0.12s, ETA: 13.56 min\n",
      "[436/900] Sample processed in 0.12s, ETA: 13.50 min\n",
      "[437/900] Sample processed in 0.12s, ETA: 13.44 min\n",
      "[438/900] Sample processed in 0.11s, ETA: 13.39 min\n",
      "[439/900] Sample processed in 0.23s, ETA: 13.33 min\n",
      "[440/900] Sample processed in 0.28s, ETA: 13.28 min\n",
      "[441/900] Sample processed in 0.31s, ETA: 13.22 min\n",
      "[442/900] Sample processed in 0.11s, ETA: 13.17 min\n",
      "[443/900] Sample processed in 0.24s, ETA: 13.11 min\n",
      "[444/900] Sample processed in 0.11s, ETA: 13.06 min\n",
      "[445/900] Sample processed in 0.23s, ETA: 13.00 min\n",
      "[446/900] Sample processed in 0.11s, ETA: 12.95 min\n",
      "[447/900] Sample processed in 0.36s, ETA: 12.90 min\n",
      "[448/900] Sample processed in 0.12s, ETA: 12.84 min\n",
      "[449/900] Sample processed in 0.12s, ETA: 12.79 min\n",
      "[450/900] Sample processed in 0.11s, ETA: 12.73 min\n",
      "[451/900] Sample processed in 0.12s, ETA: 12.68 min\n",
      "[452/900] Sample processed in 0.84s, ETA: 12.63 min\n",
      "[453/900] Sample processed in 0.24s, ETA: 12.58 min\n",
      "[454/900] Sample processed in 0.11s, ETA: 12.53 min\n",
      "[455/900] Sample processed in 0.40s, ETA: 12.48 min\n",
      "[456/900] Sample processed in 0.12s, ETA: 12.42 min\n",
      "[457/900] Sample processed in 0.12s, ETA: 12.37 min\n",
      "[458/900] Sample processed in 0.12s, ETA: 12.32 min\n",
      "[459/900] Sample processed in 0.12s, ETA: 12.27 min\n",
      "[460/900] Sample processed in 0.72s, ETA: 12.22 min\n",
      "[461/900] Sample processed in 0.11s, ETA: 12.17 min\n",
      "[462/900] Sample processed in 0.32s, ETA: 12.12 min\n",
      "[463/900] Sample processed in 0.27s, ETA: 12.07 min\n",
      "[464/900] Sample processed in 0.11s, ETA: 12.02 min\n",
      "[465/900] Sample processed in 0.20s, ETA: 11.97 min\n",
      "[466/900] Sample processed in 0.64s, ETA: 11.93 min\n",
      "[467/900] Sample processed in 0.56s, ETA: 11.88 min\n",
      "[468/900] Sample processed in 0.12s, ETA: 11.83 min\n",
      "[469/900] Sample processed in 0.12s, ETA: 11.78 min\n",
      "[470/900] Sample processed in 0.44s, ETA: 11.74 min\n",
      "[471/900] Sample processed in 0.11s, ETA: 11.68 min\n",
      "[472/900] Sample processed in 0.12s, ETA: 11.63 min\n",
      "[473/900] Sample processed in 0.11s, ETA: 11.58 min\n",
      "[474/900] Sample processed in 0.20s, ETA: 11.54 min\n",
      "[475/900] Sample processed in 0.11s, ETA: 11.49 min\n",
      "[476/900] Sample processed in 0.31s, ETA: 11.44 min\n",
      "[477/900] Sample processed in 0.44s, ETA: 11.40 min\n",
      "[478/900] Sample processed in 0.43s, ETA: 11.35 min\n",
      "[479/900] Sample processed in 0.11s, ETA: 11.30 min\n",
      "[480/900] Sample processed in 0.12s, ETA: 11.25 min\n",
      "[481/900] Sample processed in 0.48s, ETA: 11.21 min\n",
      "[482/900] Sample processed in 0.11s, ETA: 11.16 min\n",
      "[483/900] Sample processed in 0.12s, ETA: 11.11 min\n",
      "[484/900] Sample processed in 0.24s, ETA: 11.07 min\n",
      "[485/900] Sample processed in 0.11s, ETA: 11.02 min\n",
      "[486/900] Sample processed in 0.32s, ETA: 10.98 min\n",
      "[487/900] Sample processed in 0.11s, ETA: 10.93 min\n",
      "[488/900] Sample processed in 0.24s, ETA: 10.88 min\n",
      "[489/900] Sample processed in 0.12s, ETA: 10.84 min\n",
      "[490/900] Sample processed in 0.12s, ETA: 10.79 min\n",
      "[491/900] Sample processed in 0.56s, ETA: 10.75 min\n",
      "[492/900] Sample processed in 0.72s, ETA: 10.71 min\n",
      "[493/900] Sample processed in 0.52s, ETA: 10.67 min\n",
      "[494/900] Sample processed in 0.12s, ETA: 10.62 min\n",
      "[495/900] Sample processed in 0.28s, ETA: 10.58 min\n",
      "[496/900] Sample processed in 0.27s, ETA: 10.54 min\n",
      "[497/900] Sample processed in 0.12s, ETA: 10.49 min\n",
      "[498/900] Sample processed in 0.20s, ETA: 10.45 min\n",
      "[499/900] Sample processed in 0.27s, ETA: 10.40 min\n",
      "[500/900] Sample processed in 0.28s, ETA: 10.36 min\n",
      "[501/900] Sample processed in 0.12s, ETA: 10.31 min\n",
      "[502/900] Sample processed in 0.12s, ETA: 10.27 min\n",
      "[503/900] Sample processed in 0.12s, ETA: 10.23 min\n",
      "[504/900] Sample processed in 0.12s, ETA: 10.18 min\n",
      "[505/900] Sample processed in 0.12s, ETA: 10.14 min\n",
      "[506/900] Sample processed in 0.12s, ETA: 10.09 min\n",
      "[507/900] Sample processed in 0.12s, ETA: 10.05 min\n",
      "[508/900] Sample processed in 0.12s, ETA: 10.01 min\n",
      "[509/900] Sample processed in 0.12s, ETA: 9.96 min\n",
      "[510/900] Sample processed in 0.12s, ETA: 9.92 min\n",
      "[511/900] Sample processed in 0.12s, ETA: 9.87 min\n",
      "[512/900] Sample processed in 0.12s, ETA: 9.83 min\n",
      "[513/900] Sample processed in 0.12s, ETA: 9.79 min\n",
      "[514/900] Sample processed in 0.12s, ETA: 9.75 min\n",
      "[515/900] Sample processed in 0.12s, ETA: 9.70 min\n",
      "[516/900] Sample processed in 0.12s, ETA: 9.66 min\n",
      "[517/900] Sample processed in 0.12s, ETA: 9.62 min\n",
      "[518/900] Sample processed in 0.12s, ETA: 9.58 min\n",
      "[519/900] Sample processed in 0.12s, ETA: 9.53 min\n",
      "[520/900] Sample processed in 0.12s, ETA: 9.49 min\n",
      "[521/900] Sample processed in 0.12s, ETA: 9.45 min\n",
      "[522/900] Sample processed in 0.12s, ETA: 9.41 min\n",
      "[523/900] Sample processed in 0.12s, ETA: 9.37 min\n",
      "[524/900] Sample processed in 0.12s, ETA: 9.33 min\n",
      "[525/900] Sample processed in 0.12s, ETA: 9.29 min\n",
      "[526/900] Sample processed in 0.12s, ETA: 9.24 min\n",
      "[527/900] Sample processed in 0.12s, ETA: 9.20 min\n",
      "[528/900] Sample processed in 0.12s, ETA: 9.16 min\n",
      "[529/900] Sample processed in 0.12s, ETA: 9.12 min\n",
      "[530/900] Sample processed in 0.12s, ETA: 9.08 min\n",
      "[531/900] Sample processed in 0.12s, ETA: 9.04 min\n",
      "[532/900] Sample processed in 0.12s, ETA: 9.00 min\n",
      "[533/900] Sample processed in 0.12s, ETA: 8.96 min\n",
      "[534/900] Sample processed in 0.12s, ETA: 8.92 min\n",
      "[535/900] Sample processed in 0.12s, ETA: 8.88 min\n",
      "[536/900] Sample processed in 0.12s, ETA: 8.84 min\n",
      "[537/900] Sample processed in 0.12s, ETA: 8.80 min\n",
      "[538/900] Sample processed in 0.12s, ETA: 8.76 min\n",
      "[539/900] Sample processed in 0.12s, ETA: 8.72 min\n",
      "[540/900] Sample processed in 0.12s, ETA: 8.69 min\n",
      "[541/900] Sample processed in 0.12s, ETA: 8.65 min\n",
      "[542/900] Sample processed in 0.12s, ETA: 8.61 min\n",
      "[543/900] Sample processed in 0.12s, ETA: 8.57 min\n",
      "[544/900] Sample processed in 0.12s, ETA: 8.53 min\n",
      "[545/900] Sample processed in 0.12s, ETA: 8.49 min\n",
      "[546/900] Sample processed in 0.12s, ETA: 8.45 min\n",
      "[547/900] Sample processed in 0.12s, ETA: 8.42 min\n",
      "[548/900] Sample processed in 0.12s, ETA: 8.38 min\n",
      "[549/900] Sample processed in 0.12s, ETA: 8.34 min\n",
      "[550/900] Sample processed in 0.12s, ETA: 8.30 min\n",
      "[551/900] Sample processed in 0.12s, ETA: 8.27 min\n",
      "[552/900] Sample processed in 0.12s, ETA: 8.23 min\n",
      "[553/900] Sample processed in 0.13s, ETA: 8.19 min\n",
      "[554/900] Sample processed in 0.12s, ETA: 8.15 min\n",
      "[555/900] Sample processed in 0.12s, ETA: 8.12 min\n",
      "[556/900] Sample processed in 0.12s, ETA: 8.08 min\n",
      "[557/900] Sample processed in 0.12s, ETA: 8.04 min\n",
      "[558/900] Sample processed in 0.12s, ETA: 8.01 min\n",
      "[559/900] Sample processed in 0.12s, ETA: 7.97 min\n",
      "[560/900] Sample processed in 0.12s, ETA: 7.93 min\n",
      "[561/900] Sample processed in 0.12s, ETA: 7.90 min\n",
      "[562/900] Sample processed in 0.12s, ETA: 7.86 min\n",
      "[563/900] Sample processed in 0.12s, ETA: 7.83 min\n",
      "[564/900] Sample processed in 0.12s, ETA: 7.79 min\n",
      "[565/900] Sample processed in 0.12s, ETA: 7.75 min\n",
      "[566/900] Sample processed in 0.12s, ETA: 7.72 min\n",
      "[567/900] Sample processed in 0.12s, ETA: 7.68 min\n",
      "[568/900] Sample processed in 0.12s, ETA: 7.65 min\n",
      "[569/900] Sample processed in 0.12s, ETA: 7.61 min\n",
      "[570/900] Sample processed in 0.12s, ETA: 7.58 min\n",
      "[571/900] Sample processed in 0.12s, ETA: 7.54 min\n",
      "[572/900] Sample processed in 0.12s, ETA: 7.51 min\n",
      "[573/900] Sample processed in 0.12s, ETA: 7.47 min\n",
      "[574/900] Sample processed in 0.12s, ETA: 7.44 min\n",
      "[575/900] Sample processed in 0.12s, ETA: 7.40 min\n",
      "[576/900] Sample processed in 0.12s, ETA: 7.37 min\n",
      "[577/900] Sample processed in 0.12s, ETA: 7.33 min\n",
      "[578/900] Sample processed in 0.12s, ETA: 7.30 min\n",
      "[579/900] Sample processed in 0.12s, ETA: 7.27 min\n",
      "[580/900] Sample processed in 0.12s, ETA: 7.23 min\n",
      "[581/900] Sample processed in 0.12s, ETA: 7.20 min\n",
      "[582/900] Sample processed in 0.15s, ETA: 7.16 min\n",
      "[583/900] Sample processed in 0.12s, ETA: 7.13 min\n",
      "[584/900] Sample processed in 0.12s, ETA: 7.10 min\n",
      "[585/900] Sample processed in 0.12s, ETA: 7.06 min\n",
      "[586/900] Sample processed in 0.12s, ETA: 7.03 min\n",
      "[587/900] Sample processed in 0.12s, ETA: 7.00 min\n",
      "[588/900] Sample processed in 0.12s, ETA: 6.96 min\n",
      "[589/900] Sample processed in 0.12s, ETA: 6.93 min\n",
      "[590/900] Sample processed in 0.12s, ETA: 6.90 min\n",
      "[591/900] Sample processed in 0.12s, ETA: 6.86 min\n",
      "[592/900] Sample processed in 0.12s, ETA: 6.83 min\n",
      "[593/900] Sample processed in 0.12s, ETA: 6.80 min\n",
      "[594/900] Sample processed in 0.12s, ETA: 6.77 min\n",
      "[595/900] Sample processed in 0.12s, ETA: 6.73 min\n",
      "[596/900] Sample processed in 0.12s, ETA: 6.70 min\n",
      "[597/900] Sample processed in 0.12s, ETA: 6.67 min\n",
      "[598/900] Sample processed in 0.12s, ETA: 6.64 min\n",
      "[599/900] Sample processed in 0.12s, ETA: 6.61 min\n",
      "[600/900] Sample processed in 0.12s, ETA: 6.57 min\n",
      "[601/900] Sample processed in 0.28s, ETA: 6.54 min\n",
      "[602/900] Sample processed in 0.28s, ETA: 6.51 min\n",
      "[603/900] Sample processed in 0.20s, ETA: 6.48 min\n",
      "[604/900] Sample processed in 0.20s, ETA: 6.45 min\n",
      "[605/900] Sample processed in 0.20s, ETA: 6.42 min\n",
      "[606/900] Sample processed in 0.20s, ETA: 6.39 min\n",
      "[607/900] Sample processed in 0.19s, ETA: 6.36 min\n",
      "[608/900] Sample processed in 0.20s, ETA: 6.33 min\n",
      "[609/900] Sample processed in 0.20s, ETA: 6.30 min\n",
      "[610/900] Sample processed in 0.20s, ETA: 6.27 min\n",
      "[611/900] Sample processed in 0.19s, ETA: 6.24 min\n",
      "[612/900] Sample processed in 0.20s, ETA: 6.21 min\n",
      "[613/900] Sample processed in 0.23s, ETA: 6.18 min\n",
      "[614/900] Sample processed in 0.19s, ETA: 6.15 min\n",
      "[615/900] Sample processed in 0.20s, ETA: 6.12 min\n",
      "[616/900] Sample processed in 0.20s, ETA: 6.09 min\n",
      "[617/900] Sample processed in 0.28s, ETA: 6.06 min\n",
      "[618/900] Sample processed in 0.20s, ETA: 6.03 min\n",
      "[619/900] Sample processed in 0.19s, ETA: 6.00 min\n",
      "[620/900] Sample processed in 0.19s, ETA: 5.97 min\n",
      "[621/900] Sample processed in 0.28s, ETA: 5.94 min\n",
      "[622/900] Sample processed in 0.20s, ETA: 5.91 min\n",
      "[623/900] Sample processed in 0.19s, ETA: 5.88 min\n",
      "[624/900] Sample processed in 0.28s, ETA: 5.85 min\n",
      "[625/900] Sample processed in 0.19s, ETA: 5.82 min\n",
      "[626/900] Sample processed in 0.28s, ETA: 5.80 min\n",
      "[627/900] Sample processed in 0.20s, ETA: 5.77 min\n",
      "[628/900] Sample processed in 0.31s, ETA: 5.74 min\n",
      "[629/900] Sample processed in 0.19s, ETA: 5.71 min\n",
      "[630/900] Sample processed in 0.20s, ETA: 5.68 min\n",
      "[631/900] Sample processed in 0.19s, ETA: 5.65 min\n",
      "[632/900] Sample processed in 0.24s, ETA: 5.62 min\n",
      "[633/900] Sample processed in 0.19s, ETA: 5.60 min\n",
      "[634/900] Sample processed in 0.27s, ETA: 5.57 min\n",
      "[635/900] Sample processed in 0.19s, ETA: 5.54 min\n",
      "[636/900] Sample processed in 0.20s, ETA: 5.51 min\n",
      "[637/900] Sample processed in 0.20s, ETA: 5.48 min\n",
      "[638/900] Sample processed in 0.28s, ETA: 5.46 min\n",
      "[639/900] Sample processed in 0.24s, ETA: 5.43 min\n",
      "[640/900] Sample processed in 0.19s, ETA: 5.40 min\n",
      "[641/900] Sample processed in 0.20s, ETA: 5.37 min\n",
      "[642/900] Sample processed in 0.20s, ETA: 5.34 min\n",
      "[643/900] Sample processed in 0.20s, ETA: 5.32 min\n",
      "[644/900] Sample processed in 0.24s, ETA: 5.29 min\n",
      "[645/900] Sample processed in 0.20s, ETA: 5.26 min\n",
      "[646/900] Sample processed in 0.23s, ETA: 5.24 min\n",
      "[647/900] Sample processed in 0.20s, ETA: 5.21 min\n",
      "[648/900] Sample processed in 0.28s, ETA: 5.18 min\n",
      "[649/900] Sample processed in 0.20s, ETA: 5.15 min\n",
      "[650/900] Sample processed in 0.23s, ETA: 5.13 min\n",
      "[651/900] Sample processed in 0.27s, ETA: 5.10 min\n",
      "[652/900] Sample processed in 0.19s, ETA: 5.07 min\n",
      "[653/900] Sample processed in 0.27s, ETA: 5.05 min\n",
      "[654/900] Sample processed in 0.20s, ETA: 5.02 min\n",
      "[655/900] Sample processed in 0.20s, ETA: 4.99 min\n",
      "[656/900] Sample processed in 0.20s, ETA: 4.97 min\n",
      "[657/900] Sample processed in 0.20s, ETA: 4.94 min\n",
      "[658/900] Sample processed in 0.19s, ETA: 4.91 min\n",
      "[659/900] Sample processed in 0.20s, ETA: 4.89 min\n",
      "[660/900] Sample processed in 0.28s, ETA: 4.86 min\n",
      "[661/900] Sample processed in 0.19s, ETA: 4.83 min\n",
      "[662/900] Sample processed in 0.20s, ETA: 4.81 min\n",
      "[663/900] Sample processed in 0.27s, ETA: 4.78 min\n",
      "[664/900] Sample processed in 0.27s, ETA: 4.76 min\n",
      "[665/900] Sample processed in 0.19s, ETA: 4.73 min\n",
      "[666/900] Sample processed in 0.19s, ETA: 4.70 min\n",
      "[667/900] Sample processed in 0.20s, ETA: 4.68 min\n",
      "[668/900] Sample processed in 0.20s, ETA: 4.65 min\n",
      "[669/900] Sample processed in 0.24s, ETA: 4.63 min\n",
      "[670/900] Sample processed in 0.19s, ETA: 4.60 min\n",
      "[671/900] Sample processed in 0.19s, ETA: 4.58 min\n",
      "[672/900] Sample processed in 0.20s, ETA: 4.55 min\n",
      "[673/900] Sample processed in 0.24s, ETA: 4.52 min\n",
      "[674/900] Sample processed in 0.19s, ETA: 4.50 min\n",
      "[675/900] Sample processed in 0.20s, ETA: 4.47 min\n",
      "[676/900] Sample processed in 0.20s, ETA: 4.45 min\n",
      "[677/900] Sample processed in 0.20s, ETA: 4.42 min\n",
      "[678/900] Sample processed in 0.27s, ETA: 4.40 min\n",
      "[679/900] Sample processed in 0.19s, ETA: 4.37 min\n",
      "[680/900] Sample processed in 0.19s, ETA: 4.35 min\n",
      "[681/900] Sample processed in 0.19s, ETA: 4.32 min\n",
      "[682/900] Sample processed in 0.19s, ETA: 4.30 min\n",
      "[683/900] Sample processed in 0.19s, ETA: 4.27 min\n",
      "[684/900] Sample processed in 0.20s, ETA: 4.25 min\n",
      "[685/900] Sample processed in 0.20s, ETA: 4.22 min\n",
      "[686/900] Sample processed in 0.27s, ETA: 4.20 min\n",
      "[687/900] Sample processed in 0.24s, ETA: 4.17 min\n",
      "[688/900] Sample processed in 0.19s, ETA: 4.15 min\n",
      "[689/900] Sample processed in 0.20s, ETA: 4.12 min\n",
      "[690/900] Sample processed in 0.19s, ETA: 4.10 min\n",
      "[691/900] Sample processed in 0.20s, ETA: 4.08 min\n",
      "[692/900] Sample processed in 0.28s, ETA: 4.05 min\n",
      "[693/900] Sample processed in 0.19s, ETA: 4.03 min\n",
      "[694/900] Sample processed in 0.27s, ETA: 4.00 min\n",
      "[695/900] Sample processed in 0.19s, ETA: 3.98 min\n",
      "[696/900] Sample processed in 0.20s, ETA: 3.96 min\n",
      "[697/900] Sample processed in 0.19s, ETA: 3.93 min\n",
      "[698/900] Sample processed in 0.19s, ETA: 3.91 min\n",
      "[699/900] Sample processed in 0.28s, ETA: 3.88 min\n",
      "[700/900] Sample processed in 0.19s, ETA: 3.86 min\n",
      "[701/900] Sample processed in 0.67s, ETA: 3.84 min\n",
      "[702/900] Sample processed in 0.64s, ETA: 3.82 min\n",
      "[703/900] Sample processed in 0.60s, ETA: 3.79 min\n",
      "[704/900] Sample processed in 0.63s, ETA: 3.77 min\n",
      "[705/900] Sample processed in 0.64s, ETA: 3.75 min\n",
      "[706/900] Sample processed in 0.56s, ETA: 3.73 min\n",
      "[707/900] Sample processed in 0.64s, ETA: 3.71 min\n",
      "[708/900] Sample processed in 0.55s, ETA: 3.69 min\n",
      "[709/900] Sample processed in 0.61s, ETA: 3.66 min\n",
      "[710/900] Sample processed in 0.76s, ETA: 3.64 min\n",
      "[711/900] Sample processed in 0.56s, ETA: 3.62 min\n",
      "[712/900] Sample processed in 0.99s, ETA: 3.60 min\n",
      "[713/900] Sample processed in 0.64s, ETA: 3.58 min\n",
      "[714/900] Sample processed in 0.60s, ETA: 3.56 min\n",
      "[715/900] Sample processed in 0.51s, ETA: 3.54 min\n",
      "[716/900] Sample processed in 0.60s, ETA: 3.52 min\n",
      "[717/900] Sample processed in 0.40s, ETA: 3.49 min\n",
      "[718/900] Sample processed in 0.59s, ETA: 3.47 min\n",
      "[719/900] Sample processed in 0.44s, ETA: 3.45 min\n",
      "[720/900] Sample processed in 0.55s, ETA: 3.43 min\n",
      "[721/900] Sample processed in 0.63s, ETA: 3.41 min\n",
      "[722/900] Sample processed in 0.69s, ETA: 3.39 min\n",
      "[723/900] Sample processed in 0.56s, ETA: 3.36 min\n",
      "[724/900] Sample processed in 0.55s, ETA: 3.34 min\n",
      "[725/900] Sample processed in 0.67s, ETA: 3.32 min\n",
      "[726/900] Sample processed in 0.59s, ETA: 3.30 min\n",
      "[727/900] Sample processed in 0.55s, ETA: 3.28 min\n",
      "[728/900] Sample processed in 0.51s, ETA: 3.26 min\n",
      "[729/900] Sample processed in 0.48s, ETA: 3.24 min\n",
      "[730/900] Sample processed in 0.43s, ETA: 3.22 min\n",
      "[731/900] Sample processed in 0.68s, ETA: 3.19 min\n",
      "[732/900] Sample processed in 0.52s, ETA: 3.17 min\n",
      "[733/900] Sample processed in 0.44s, ETA: 3.15 min\n",
      "[734/900] Sample processed in 0.47s, ETA: 3.13 min\n",
      "[735/900] Sample processed in 0.59s, ETA: 3.11 min\n",
      "[736/900] Sample processed in 0.56s, ETA: 3.09 min\n",
      "[737/900] Sample processed in 0.68s, ETA: 3.07 min\n",
      "[738/900] Sample processed in 0.52s, ETA: 3.05 min\n",
      "[739/900] Sample processed in 0.39s, ETA: 3.03 min\n",
      "[740/900] Sample processed in 0.68s, ETA: 3.00 min\n",
      "[741/900] Sample processed in 0.60s, ETA: 2.98 min\n",
      "[742/900] Sample processed in 0.64s, ETA: 2.96 min\n",
      "[743/900] Sample processed in 0.63s, ETA: 2.94 min\n",
      "[744/900] Sample processed in 0.55s, ETA: 2.92 min\n",
      "[745/900] Sample processed in 0.39s, ETA: 2.90 min\n",
      "[746/900] Sample processed in 0.51s, ETA: 2.88 min\n",
      "[747/900] Sample processed in 0.67s, ETA: 2.86 min\n",
      "[748/900] Sample processed in 0.55s, ETA: 2.84 min\n",
      "[749/900] Sample processed in 0.52s, ETA: 2.82 min\n",
      "[750/900] Sample processed in 0.48s, ETA: 2.80 min\n",
      "[751/900] Sample processed in 0.60s, ETA: 2.78 min\n",
      "[752/900] Sample processed in 0.75s, ETA: 2.76 min\n",
      "[753/900] Sample processed in 0.56s, ETA: 2.74 min\n",
      "[754/900] Sample processed in 0.43s, ETA: 2.72 min\n",
      "[755/900] Sample processed in 0.47s, ETA: 2.70 min\n",
      "[756/900] Sample processed in 0.55s, ETA: 2.68 min\n",
      "[757/900] Sample processed in 0.39s, ETA: 2.65 min\n",
      "[758/900] Sample processed in 0.44s, ETA: 2.63 min\n",
      "[759/900] Sample processed in 0.51s, ETA: 2.61 min\n",
      "[760/900] Sample processed in 0.96s, ETA: 2.59 min\n",
      "[761/900] Sample processed in 0.44s, ETA: 2.57 min\n",
      "[762/900] Sample processed in 0.52s, ETA: 2.55 min\n",
      "[763/900] Sample processed in 0.51s, ETA: 2.53 min\n",
      "[764/900] Sample processed in 0.43s, ETA: 2.51 min\n",
      "[765/900] Sample processed in 0.63s, ETA: 2.49 min\n",
      "[766/900] Sample processed in 0.55s, ETA: 2.47 min\n",
      "[767/900] Sample processed in 0.52s, ETA: 2.45 min\n",
      "[768/900] Sample processed in 0.43s, ETA: 2.43 min\n",
      "[769/900] Sample processed in 0.52s, ETA: 2.41 min\n",
      "[770/900] Sample processed in 0.59s, ETA: 2.39 min\n",
      "[771/900] Sample processed in 0.91s, ETA: 2.37 min\n",
      "[772/900] Sample processed in 0.47s, ETA: 2.35 min\n",
      "[773/900] Sample processed in 0.63s, ETA: 2.33 min\n",
      "[774/900] Sample processed in 0.59s, ETA: 2.31 min\n",
      "[775/900] Sample processed in 0.68s, ETA: 2.29 min\n",
      "[776/900] Sample processed in 0.84s, ETA: 2.28 min\n",
      "[777/900] Sample processed in 0.48s, ETA: 2.26 min\n",
      "[778/900] Sample processed in 0.59s, ETA: 2.24 min\n",
      "[779/900] Sample processed in 0.59s, ETA: 2.22 min\n",
      "[780/900] Sample processed in 0.55s, ETA: 2.20 min\n",
      "[781/900] Sample processed in 0.67s, ETA: 2.18 min\n",
      "[782/900] Sample processed in 0.47s, ETA: 2.16 min\n",
      "[783/900] Sample processed in 0.51s, ETA: 2.14 min\n",
      "[784/900] Sample processed in 0.48s, ETA: 2.12 min\n",
      "[785/900] Sample processed in 0.63s, ETA: 2.10 min\n",
      "[786/900] Sample processed in 0.55s, ETA: 2.08 min\n",
      "[787/900] Sample processed in 0.44s, ETA: 2.06 min\n",
      "[788/900] Sample processed in 0.63s, ETA: 2.04 min\n",
      "[789/900] Sample processed in 0.91s, ETA: 2.02 min\n",
      "[790/900] Sample processed in 0.71s, ETA: 2.00 min\n",
      "[791/900] Sample processed in 0.59s, ETA: 1.98 min\n",
      "[792/900] Sample processed in 0.44s, ETA: 1.96 min\n",
      "[793/900] Sample processed in 0.75s, ETA: 1.94 min\n",
      "[794/900] Sample processed in 0.59s, ETA: 1.92 min\n",
      "[795/900] Sample processed in 0.60s, ETA: 1.91 min\n",
      "[796/900] Sample processed in 0.68s, ETA: 1.89 min\n",
      "[797/900] Sample processed in 0.63s, ETA: 1.87 min\n",
      "[798/900] Sample processed in 0.43s, ETA: 1.85 min\n",
      "[799/900] Sample processed in 0.67s, ETA: 1.83 min\n",
      "[800/900] Sample processed in 0.64s, ETA: 1.81 min\n",
      "[801/900] Sample processed in 8.54s, ETA: 1.81 min\n",
      "[802/900] Sample processed in 12.12s, ETA: 1.81 min\n",
      "[803/900] Sample processed in 5.09s, ETA: 1.80 min\n",
      "[804/900] Sample processed in 7.67s, ETA: 1.80 min\n",
      "[805/900] Sample processed in 7.10s, ETA: 1.79 min\n",
      "[806/900] Sample processed in 25.18s, ETA: 1.82 min\n",
      "[807/900] Sample processed in 5.13s, ETA: 1.80 min\n",
      "[808/900] Sample processed in 13.02s, ETA: 1.81 min\n",
      "[809/900] Sample processed in 7.20s, ETA: 1.80 min\n",
      "[810/900] Sample processed in 12.22s, ETA: 1.80 min\n",
      "[811/900] Sample processed in 13.59s, ETA: 1.80 min\n",
      "[812/900] Sample processed in 3.82s, ETA: 1.79 min\n",
      "[813/900] Sample processed in 17.70s, ETA: 1.80 min\n",
      "[814/900] Sample processed in 10.51s, ETA: 1.79 min\n",
      "[815/900] Sample processed in 3.90s, ETA: 1.78 min\n",
      "[816/900] Sample processed in 18.44s, ETA: 1.78 min\n",
      "[817/900] Sample processed in 10.88s, ETA: 1.78 min\n",
      "[818/900] Sample processed in 4.55s, ETA: 1.76 min\n",
      "[819/900] Sample processed in 11.13s, ETA: 1.76 min\n",
      "[820/900] Sample processed in 7.59s, ETA: 1.75 min\n",
      "[821/900] Sample processed in 2.41s, ETA: 1.73 min\n",
      "[822/900] Sample processed in 11.10s, ETA: 1.72 min\n",
      "[823/900] Sample processed in 16.30s, ETA: 1.72 min\n",
      "[824/900] Sample processed in 13.59s, ETA: 1.72 min\n",
      "[825/900] Sample processed in 9.52s, ETA: 1.71 min\n",
      "[826/900] Sample processed in 16.02s, ETA: 1.71 min\n",
      "[827/900] Sample processed in 8.77s, ETA: 1.69 min\n",
      "[828/900] Sample processed in 2.45s, ETA: 1.67 min\n",
      "[829/900] Sample processed in 27.56s, ETA: 1.69 min\n",
      "[830/900] Sample processed in 8.50s, ETA: 1.67 min\n",
      "[831/900] Sample processed in 6.03s, ETA: 1.66 min\n",
      "[832/900] Sample processed in 2.67s, ETA: 1.63 min\n",
      "[833/900] Sample processed in 18.23s, ETA: 1.63 min\n",
      "[834/900] Sample processed in 8.90s, ETA: 1.62 min\n",
      "[835/900] Sample processed in 5.01s, ETA: 1.60 min\n",
      "[836/900] Sample processed in 6.56s, ETA: 1.58 min\n",
      "[837/900] Sample processed in 22.71s, ETA: 1.58 min\n",
      "[838/900] Sample processed in 11.07s, ETA: 1.57 min\n",
      "[839/900] Sample processed in 5.24s, ETA: 1.55 min\n",
      "[840/900] Sample processed in 4.00s, ETA: 1.52 min\n",
      "[841/900] Sample processed in 14.37s, ETA: 1.51 min\n",
      "[842/900] Sample processed in 12.71s, ETA: 1.50 min\n",
      "[843/900] Sample processed in 7.43s, ETA: 1.48 min\n",
      "[844/900] Sample processed in 27.28s, ETA: 1.48 min\n",
      "[845/900] Sample processed in 5.66s, ETA: 1.46 min\n",
      "[846/900] Sample processed in 8.42s, ETA: 1.44 min\n",
      "[847/900] Sample processed in 14.93s, ETA: 1.43 min\n",
      "[848/900] Sample processed in 4.10s, ETA: 1.41 min\n",
      "[849/900] Sample processed in 2.31s, ETA: 1.38 min\n",
      "[850/900] Sample processed in 13.40s, ETA: 1.36 min\n",
      "[851/900] Sample processed in 4.59s, ETA: 1.34 min\n",
      "[852/900] Sample processed in 9.57s, ETA: 1.32 min\n",
      "[853/900] Sample processed in 4.64s, ETA: 1.29 min\n",
      "[854/900] Sample processed in 3.92s, ETA: 1.27 min\n",
      "[855/900] Sample processed in 19.52s, ETA: 1.26 min\n",
      "[856/900] Sample processed in 8.59s, ETA: 1.24 min\n",
      "[857/900] Sample processed in 14.08s, ETA: 1.22 min\n",
      "[858/900] Sample processed in 13.39s, ETA: 1.20 min\n",
      "[859/900] Sample processed in 6.46s, ETA: 1.17 min\n",
      "[860/900] Sample processed in 11.21s, ETA: 1.15 min\n",
      "[861/900] Sample processed in 2.33s, ETA: 1.12 min\n",
      "[862/900] Sample processed in 11.46s, ETA: 1.10 min\n",
      "[863/900] Sample processed in 16.92s, ETA: 1.08 min\n",
      "[864/900] Sample processed in 8.86s, ETA: 1.06 min\n",
      "[865/900] Sample processed in 14.79s, ETA: 1.04 min\n",
      "[866/900] Sample processed in 17.02s, ETA: 1.02 min\n",
      "[867/900] Sample processed in 4.31s, ETA: 0.99 min\n",
      "[868/900] Sample processed in 11.23s, ETA: 0.97 min\n",
      "[869/900] Sample processed in 2.62s, ETA: 0.94 min\n",
      "[870/900] Sample processed in 4.68s, ETA: 0.91 min\n",
      "[871/900] Sample processed in 5.11s, ETA: 0.88 min\n",
      "[872/900] Sample processed in 5.34s, ETA: 0.85 min\n",
      "[873/900] Sample processed in 15.61s, ETA: 0.83 min\n",
      "[874/900] Sample processed in 8.09s, ETA: 0.80 min\n",
      "[875/900] Sample processed in 2.33s, ETA: 0.77 min\n",
      "[876/900] Sample processed in 8.08s, ETA: 0.74 min\n",
      "[877/900] Sample processed in 3.84s, ETA: 0.71 min\n",
      "[878/900] Sample processed in 6.65s, ETA: 0.68 min\n",
      "[879/900] Sample processed in 6.28s, ETA: 0.65 min\n",
      "[880/900] Sample processed in 5.86s, ETA: 0.62 min\n",
      "[881/900] Sample processed in 12.44s, ETA: 0.60 min\n",
      "[882/900] Sample processed in 10.26s, ETA: 0.57 min\n",
      "[883/900] Sample processed in 6.58s, ETA: 0.54 min\n",
      "[884/900] Sample processed in 16.87s, ETA: 0.51 min\n",
      "[885/900] Sample processed in 8.36s, ETA: 0.48 min\n",
      "[886/900] Sample processed in 23.10s, ETA: 0.45 min\n",
      "[887/900] Sample processed in 8.89s, ETA: 0.42 min\n",
      "[888/900] Sample processed in 9.21s, ETA: 0.39 min\n",
      "[889/900] Sample processed in 18.67s, ETA: 0.36 min\n",
      "[890/900] Sample processed in 3.86s, ETA: 0.33 min\n",
      "[891/900] Sample processed in 12.15s, ETA: 0.30 min\n",
      "[892/900] Sample processed in 13.35s, ETA: 0.27 min\n",
      "[893/900] Sample processed in 17.17s, ETA: 0.24 min\n",
      "[894/900] Sample processed in 5.92s, ETA: 0.20 min\n",
      "[895/900] Sample processed in 10.90s, ETA: 0.17 min\n",
      "[896/900] Sample processed in 3.76s, ETA: 0.14 min\n",
      "[897/900] Sample processed in 27.27s, ETA: 0.10 min\n",
      "[898/900] Sample processed in 9.56s, ETA: 0.07 min\n",
      "[899/900] Sample processed in 19.60s, ETA: 0.03 min\n",
      "[900/900] Sample processed in 3.13s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 31.49 min\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### âœ… Hugging Faceì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "dataset_name = \"passionMan/test_dataset10\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split ë¡œë“œ\n",
    "\n",
    "### âœ… JSONL ì €ìž¥ í•¨ìˆ˜ (í‰ê°€ ê²°ê³¼ ì €ìž¥ìš©)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### âœ… ëª¨ë¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # âœ… ëª¨ë¸ì˜ ìµœëŒ€ ìž…ë ¥ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸° (ë³´í†µ 4096 ë˜ëŠ” 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # âœ… ìž…ë ¥ í† í° ê¸¸ì´ í™•ì¸\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # ðŸ”¥ ìž…ë ¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìµœëŒ€ ìž…ë ¥ ê¸¸ì´ì— ë§žê²Œ ìžë¦„\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # âœ… ìƒì„± ìˆ˜í–‰ (max_new_tokensì„ ì ìš©)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # âœ… ìƒì„± ê¸¸ì´ ì ìš©\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# âœ… ë°ì´í„° ê²½ë¡œ ì„¤ì • (ê²°ê³¼ ì €ìž¥ìš©)\n",
    "output_json_path = \"response/real_seed_IFD.jsonl\"\n",
    "\n",
    "# âœ… Taskë³„ ë°ì´í„° ê·¸ë£¹í™” (ê° íƒœìŠ¤í¬ë³„ 0~29ë²ˆ ìƒ˜í”Œ ì„ íƒ)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€í•  ë°ì´í„° ìƒì„± (ê° íƒœìŠ¤í¬ë³„ 30ê°œë§Œ ì¶”ì¶œ)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:100])  # ìµœëŒ€ 30ê°œ ì„ íƒ\n",
    "\n",
    "# âœ… ì„±ëŠ¥ í‰ê°€ ì‹œìž‘\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # âœ… ìƒì„±í•  í† í° ê¸¸ì´ ì„¤ì • (ìƒì„± í† í° ìˆ˜ ì¡°ì ˆ)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"re\", \"re2\"}  # ìƒì„± 32\n",
    "    medium_context_tasks = {\"ie\"} # ìƒì„± 128\n",
    "    long_context_tasks = {\"summarization\", \"generation\", \"daily_diets\", \"alternative_diet\"}  # ìƒì„± 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 32  # âœ… ìƒì„± ê¸¸ì´ 128\n",
    "    elif task in medium_context_tasks:\n",
    "        max_new_tokens = 32\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 1024  # âœ… ìƒì„± ê¸¸ì´ 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # ê¸°ë³¸ê°’\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 13:34:22 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial8/train/llama3_8B/outputs/real_seed_IFD_rIFD14/checkpoint-474\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the nutritional content of a 100g serving of grilled chicken breast with brown rice, including the breakdown of carbohydrates, proteins, fats, and total energy content. Provide the information in grams and calories.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "A 100g serving of grilled chicken breast with brown rice provides 20g of carbohydrates, 22g of proteins, 4g of fats, and 220 calories.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(900) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Can you recommend a daily diet recipe? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Prepare a healthy daily meal plan for a female aged 35 with 1400 kcal, including portion sizes in grams.\",\n",
    "        # \"Prepare a healthy daily meal plan for diebeic patient, including portion sizes.\",\n",
    "        \"Analyze the nutritional content of a 100g serving of grilled chicken breast with brown rice, including the breakdown of carbohydrates, proteins, fats, and total energy content. Provide the information in grams and calories.\",\n",
    "        # \"Breakfast: Oatmeal with fresh fruits and nuts\\nLunch: Grilled chicken breast with steamed vegetables and brown rice\\nSnack: Greek yogurt with berries\\nDinner: Baked salmon with roasted sweet potatoes and steamed broccoli\",\n",
    "        \"\",\n",
    "        \"\"\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "prepare a healthy daily meal plan for diabetic patient, including portion sizes.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "breakfast: 1 cup of oatmeal with 1 tablespoon of chia seeds, 1/2 cup of blueberries, and 1 tablespoon of honey\n",
      "\n",
      "lunch: 1 cup of mixed greens salad with 1/2 cup of roasted sweet potatoes, 1/2 cup of chickpeas, 1 tablespoon of olive oil, and 1 tablespoon of balsamic vinegar\n",
      "\n",
      "dinner: 1 cup of brown rice with 1 cup of roasted chicken breast, 1 cup of steamed broccoli, and 1 tablespoon of olive oil\n",
      "\n",
      "snack: 1 cup of non-fat plain greek yogurt with 1 tablespoon of honey<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1000) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        \"prepare a healthy daily meal plan for diabetic patient, including portion sizes.\",\n",
    "        \"\",\n",
    "        \"\"\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
