{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b4191fde97421eabd26e6febb87484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'ifd_loss'],\n",
       "        num_rows: 15156\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "seed = load_dataset(\"passionMan/real_seed_IFD_rIFD14\")\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-31 13:22:04 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928d62c5347e49b2a5e81e77310e0a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B\",\n",
    "    # model_name = \"/data/jaesung/llm_for_diabetes/src/trial8/train/llama3_8B/outputs/real_seed_IFD_rIFD14/checkpoint-474\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsloth: Your model already has LoRA adapters. Your new parameters are different.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;66;03m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     target_modules \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     lora_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      6\u001b[0m     lora_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;66;03m# Supports any, but = 0 is optimized\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \u001b[38;5;66;03m# Supports any, but = \"none\" is optimized\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     use_gradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# True or \"unsloth\" for very long context\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3407\u001b[39m,\n\u001b[1;32m     11\u001b[0m     use_rslora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# We support rank stabilized LoRA\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loftq_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# And LoftQ\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:2148\u001b[0m, in \u001b[0;36mFastLlamaModel.get_peft_model\u001b[0;34m(model, r, target_modules, lora_alpha, lora_dropout, bias, layers_to_transform, layers_pattern, use_gradient_checkpointing, random_state, max_seq_length, use_rslora, modules_to_save, init_lora_weights, loftq_config, temporary_location, **kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Your model already has LoRA adapters. Your new parameters are different.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2150\u001b[0m         )\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsloth: Your model already has LoRA adapters. Your new parameters are different."
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.01, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd5cb1af0dd4e83998aee47d9e76e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"passionMan/real_10_percent\", split = \"train\")\n",
    "# dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd123e80795e4362bffed870bafbaabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=8):   0%|          | 0/1502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83142a05d99b421f87ec745b0e4eaa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/1502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b07cba803a74b9a9a35c8109a69052a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=8):   0%|          | 0/1502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10, # 50\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs/real_seed_IFD_rIFD14_10_percent\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_steps = 20,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,502 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 23\n",
      " \"-____-\"     Number of trainable parameters = 13,631,488\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 04:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.279500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/900] Sample processed in 4.60s, ETA: 68.94 min\n",
      "[2/900] Sample processed in 4.78s, ETA: 70.19 min\n",
      "[3/900] Sample processed in 5.96s, ETA: 76.42 min\n",
      "[4/900] Sample processed in 4.28s, ETA: 73.21 min\n",
      "[5/900] Sample processed in 5.16s, ETA: 73.89 min\n",
      "[6/900] Sample processed in 9.64s, ETA: 85.45 min\n",
      "[7/900] Sample processed in 7.07s, ETA: 88.18 min\n",
      "[8/900] Sample processed in 10.29s, ETA: 96.20 min\n",
      "[9/900] Sample processed in 3.72s, ETA: 91.55 min\n",
      "[10/900] Sample processed in 4.79s, ETA: 89.41 min\n",
      "[11/900] Sample processed in 7.22s, ETA: 90.91 min\n",
      "[12/900] Sample processed in 7.77s, ETA: 92.82 min\n",
      "[13/900] Sample processed in 8.39s, ETA: 95.12 min\n",
      "[14/900] Sample processed in 6.71s, ETA: 95.31 min\n",
      "[15/900] Sample processed in 4.69s, ETA: 93.47 min\n",
      "[16/900] Sample processed in 6.65s, ETA: 93.65 min\n",
      "[17/900] Sample processed in 2.92s, ETA: 90.57 min\n",
      "[18/900] Sample processed in 9.23s, ETA: 92.98 min\n",
      "[19/900] Sample processed in 7.18s, ETA: 93.53 min\n",
      "[20/900] Sample processed in 7.13s, ETA: 93.99 min\n",
      "[21/900] Sample processed in 4.73s, ETA: 92.71 min\n",
      "[22/900] Sample processed in 12.73s, ETA: 96.86 min\n",
      "[23/900] Sample processed in 3.77s, ETA: 94.94 min\n",
      "[24/900] Sample processed in 5.72s, ETA: 94.36 min\n",
      "[25/900] Sample processed in 7.42s, ETA: 94.81 min\n",
      "[26/900] Sample processed in 4.68s, ETA: 93.68 min\n",
      "[27/900] Sample processed in 5.18s, ETA: 92.89 min\n",
      "[28/900] Sample processed in 12.12s, ETA: 95.76 min\n",
      "[29/900] Sample processed in 3.29s, ETA: 94.00 min\n",
      "[30/900] Sample processed in 6.64s, ETA: 93.97 min\n",
      "[31/900] Sample processed in 4.44s, ETA: 92.91 min\n",
      "[32/900] Sample processed in 15.45s, ETA: 96.89 min\n",
      "[33/900] Sample processed in 4.08s, ETA: 95.63 min\n",
      "[34/900] Sample processed in 7.04s, ETA: 95.70 min\n",
      "[35/900] Sample processed in 6.14s, ETA: 95.39 min\n",
      "[36/900] Sample processed in 5.06s, ETA: 94.65 min\n",
      "[37/900] Sample processed in 4.03s, ETA: 93.56 min\n",
      "[38/900] Sample processed in 4.52s, ETA: 92.70 min\n",
      "[39/900] Sample processed in 7.99s, ETA: 93.16 min\n",
      "[40/900] Sample processed in 5.05s, ETA: 92.53 min\n",
      "[41/900] Sample processed in 5.85s, ETA: 92.21 min\n",
      "[42/900] Sample processed in 3.36s, ETA: 91.06 min\n",
      "[43/900] Sample processed in 5.30s, ETA: 90.60 min\n",
      "[44/900] Sample processed in 5.98s, ETA: 90.37 min\n",
      "[45/900] Sample processed in 7.82s, ETA: 90.74 min\n",
      "[46/900] Sample processed in 5.17s, ETA: 90.26 min\n",
      "[47/900] Sample processed in 4.79s, ETA: 89.69 min\n",
      "[48/900] Sample processed in 7.18s, ETA: 89.84 min\n",
      "[49/900] Sample processed in 4.50s, ETA: 89.20 min\n",
      "[50/900] Sample processed in 7.30s, ETA: 89.39 min\n",
      "[51/900] Sample processed in 4.68s, ETA: 88.83 min\n",
      "[52/900] Sample processed in 6.52s, ETA: 88.79 min\n",
      "[53/900] Sample processed in 5.61s, ETA: 88.51 min\n",
      "[54/900] Sample processed in 7.13s, ETA: 88.63 min\n",
      "[55/900] Sample processed in 5.81s, ETA: 88.40 min\n",
      "[56/900] Sample processed in 3.55s, ETA: 87.61 min\n",
      "[57/900] Sample processed in 6.29s, ETA: 87.52 min\n",
      "[58/900] Sample processed in 4.35s, ETA: 86.96 min\n",
      "[59/900] Sample processed in 5.37s, ETA: 86.66 min\n",
      "[60/900] Sample processed in 4.11s, ETA: 86.08 min\n",
      "[61/900] Sample processed in 3.25s, ETA: 85.31 min\n",
      "[62/900] Sample processed in 7.52s, ETA: 85.53 min\n",
      "[63/900] Sample processed in 5.04s, ETA: 85.19 min\n",
      "[64/900] Sample processed in 6.73s, ETA: 85.22 min\n",
      "[65/900] Sample processed in 3.61s, ETA: 84.58 min\n",
      "[66/900] Sample processed in 4.02s, ETA: 84.05 min\n",
      "[67/900] Sample processed in 4.45s, ETA: 83.61 min\n",
      "[68/900] Sample processed in 6.97s, ETA: 83.71 min\n",
      "[69/900] Sample processed in 5.06s, ETA: 83.41 min\n",
      "[70/900] Sample processed in 10.51s, ETA: 84.20 min\n",
      "[71/900] Sample processed in 6.92s, ETA: 84.26 min\n",
      "[72/900] Sample processed in 9.08s, ETA: 84.73 min\n",
      "[73/900] Sample processed in 2.01s, ETA: 83.85 min\n",
      "[74/900] Sample processed in 10.15s, ETA: 84.50 min\n",
      "[75/900] Sample processed in 15.36s, ETA: 86.09 min\n",
      "[76/900] Sample processed in 6.08s, ETA: 85.95 min\n",
      "[77/900] Sample processed in 3.41s, ETA: 85.34 min\n",
      "[78/900] Sample processed in 7.45s, ETA: 85.45 min\n",
      "[79/900] Sample processed in 9.54s, ETA: 85.92 min\n",
      "[80/900] Sample processed in 6.80s, ETA: 85.91 min\n",
      "[81/900] Sample processed in 7.19s, ETA: 85.95 min\n",
      "[82/900] Sample processed in 3.97s, ETA: 85.46 min\n",
      "[83/900] Sample processed in 4.49s, ETA: 85.07 min\n",
      "[84/900] Sample processed in 15.39s, ETA: 86.44 min\n",
      "[85/900] Sample processed in 3.85s, ETA: 85.94 min\n",
      "[86/900] Sample processed in 5.16s, ETA: 85.65 min\n",
      "[87/900] Sample processed in 8.34s, ETA: 85.86 min\n",
      "[88/900] Sample processed in 3.42s, ETA: 85.30 min\n",
      "[89/900] Sample processed in 3.66s, ETA: 84.80 min\n",
      "[90/900] Sample processed in 8.24s, ETA: 84.99 min\n",
      "[91/900] Sample processed in 4.67s, ETA: 84.64 min\n",
      "[92/900] Sample processed in 3.72s, ETA: 84.16 min\n",
      "[93/900] Sample processed in 8.12s, ETA: 84.33 min\n",
      "[94/900] Sample processed in 6.61s, ETA: 84.27 min\n",
      "[95/900] Sample processed in 4.45s, ETA: 83.91 min\n",
      "[96/900] Sample processed in 5.80s, ETA: 83.74 min\n",
      "[97/900] Sample processed in 3.83s, ETA: 83.30 min\n",
      "[98/900] Sample processed in 4.14s, ETA: 82.91 min\n",
      "[99/900] Sample processed in 2.93s, ETA: 82.37 min\n",
      "[100/900] Sample processed in 3.88s, ETA: 81.96 min\n",
      "[101/900] Sample processed in 0.65s, ETA: 81.13 min\n",
      "[102/900] Sample processed in 0.97s, ETA: 80.36 min\n",
      "[103/900] Sample processed in 0.98s, ETA: 79.61 min\n",
      "[104/900] Sample processed in 0.99s, ETA: 78.87 min\n",
      "[105/900] Sample processed in 1.00s, ETA: 78.15 min\n",
      "[106/900] Sample processed in 1.04s, ETA: 77.44 min\n",
      "[107/900] Sample processed in 0.96s, ETA: 76.74 min\n",
      "[108/900] Sample processed in 1.00s, ETA: 76.06 min\n",
      "[109/900] Sample processed in 1.01s, ETA: 75.39 min\n",
      "[110/900] Sample processed in 1.03s, ETA: 74.73 min\n",
      "[111/900] Sample processed in 1.00s, ETA: 74.08 min\n",
      "[112/900] Sample processed in 0.98s, ETA: 73.44 min\n",
      "[113/900] Sample processed in 0.60s, ETA: 72.77 min\n",
      "[114/900] Sample processed in 1.06s, ETA: 72.16 min\n",
      "[115/900] Sample processed in 0.98s, ETA: 71.55 min\n",
      "[116/900] Sample processed in 0.99s, ETA: 70.96 min\n",
      "[117/900] Sample processed in 1.01s, ETA: 70.37 min\n",
      "[118/900] Sample processed in 0.92s, ETA: 69.79 min\n",
      "[119/900] Sample processed in 0.99s, ETA: 69.22 min\n",
      "[120/900] Sample processed in 0.65s, ETA: 68.63 min\n",
      "[121/900] Sample processed in 0.98s, ETA: 68.08 min\n",
      "[122/900] Sample processed in 0.59s, ETA: 67.50 min\n",
      "[123/900] Sample processed in 1.02s, ETA: 66.97 min\n",
      "[124/900] Sample processed in 0.57s, ETA: 66.40 min\n",
      "[125/900] Sample processed in 0.63s, ETA: 65.85 min\n",
      "[126/900] Sample processed in 0.66s, ETA: 65.31 min\n",
      "[127/900] Sample processed in 0.99s, ETA: 64.82 min\n",
      "[128/900] Sample processed in 0.98s, ETA: 64.32 min\n",
      "[129/900] Sample processed in 0.99s, ETA: 63.84 min\n",
      "[130/900] Sample processed in 0.78s, ETA: 63.35 min\n",
      "[131/900] Sample processed in 1.02s, ETA: 62.88 min\n",
      "[132/900] Sample processed in 0.99s, ETA: 62.42 min\n",
      "[133/900] Sample processed in 0.85s, ETA: 61.95 min\n",
      "[134/900] Sample processed in 1.01s, ETA: 61.50 min\n",
      "[135/900] Sample processed in 1.04s, ETA: 61.07 min\n",
      "[136/900] Sample processed in 0.98s, ETA: 60.63 min\n",
      "[137/900] Sample processed in 1.01s, ETA: 60.20 min\n",
      "[138/900] Sample processed in 0.71s, ETA: 59.75 min\n",
      "[139/900] Sample processed in 1.05s, ETA: 59.34 min\n",
      "[140/900] Sample processed in 0.98s, ETA: 58.93 min\n",
      "[141/900] Sample processed in 0.48s, ETA: 58.48 min\n",
      "[142/900] Sample processed in 0.99s, ETA: 58.08 min\n",
      "[143/900] Sample processed in 1.04s, ETA: 57.69 min\n",
      "[144/900] Sample processed in 0.59s, ETA: 57.26 min\n",
      "[145/900] Sample processed in 0.98s, ETA: 56.88 min\n",
      "[146/900] Sample processed in 0.71s, ETA: 56.47 min\n",
      "[147/900] Sample processed in 1.00s, ETA: 56.10 min\n",
      "[148/900] Sample processed in 0.93s, ETA: 55.73 min\n",
      "[149/900] Sample processed in 0.90s, ETA: 55.35 min\n",
      "[150/900] Sample processed in 0.98s, ETA: 54.99 min\n",
      "[151/900] Sample processed in 0.51s, ETA: 54.60 min\n",
      "[152/900] Sample processed in 1.00s, ETA: 54.25 min\n",
      "[153/900] Sample processed in 0.78s, ETA: 53.89 min\n",
      "[154/900] Sample processed in 1.01s, ETA: 53.54 min\n",
      "[155/900] Sample processed in 0.80s, ETA: 53.19 min\n",
      "[156/900] Sample processed in 0.95s, ETA: 52.86 min\n",
      "[157/900] Sample processed in 0.98s, ETA: 52.53 min\n",
      "[158/900] Sample processed in 1.03s, ETA: 52.20 min\n",
      "[159/900] Sample processed in 0.66s, ETA: 51.86 min\n",
      "[160/900] Sample processed in 1.00s, ETA: 51.54 min\n",
      "[161/900] Sample processed in 0.99s, ETA: 51.23 min\n",
      "[162/900] Sample processed in 0.86s, ETA: 50.91 min\n",
      "[163/900] Sample processed in 0.44s, ETA: 50.56 min\n",
      "[164/900] Sample processed in 1.01s, ETA: 50.26 min\n",
      "[165/900] Sample processed in 1.04s, ETA: 49.96 min\n",
      "[166/900] Sample processed in 0.45s, ETA: 49.63 min\n",
      "[167/900] Sample processed in 0.99s, ETA: 49.34 min\n",
      "[168/900] Sample processed in 1.03s, ETA: 49.05 min\n",
      "[169/900] Sample processed in 0.96s, ETA: 48.76 min\n",
      "[170/900] Sample processed in 0.37s, ETA: 48.44 min\n",
      "[171/900] Sample processed in 0.59s, ETA: 48.13 min\n",
      "[172/900] Sample processed in 0.99s, ETA: 47.85 min\n",
      "[173/900] Sample processed in 1.00s, ETA: 47.58 min\n",
      "[174/900] Sample processed in 0.99s, ETA: 47.31 min\n",
      "[175/900] Sample processed in 0.63s, ETA: 47.02 min\n",
      "[176/900] Sample processed in 1.02s, ETA: 46.76 min\n",
      "[177/900] Sample processed in 0.51s, ETA: 46.46 min\n",
      "[178/900] Sample processed in 0.38s, ETA: 46.17 min\n",
      "[179/900] Sample processed in 1.00s, ETA: 45.91 min\n",
      "[180/900] Sample processed in 0.98s, ETA: 45.66 min\n",
      "[181/900] Sample processed in 1.00s, ETA: 45.41 min\n",
      "[182/900] Sample processed in 0.99s, ETA: 45.16 min\n",
      "[183/900] Sample processed in 0.74s, ETA: 44.90 min\n",
      "[184/900] Sample processed in 1.01s, ETA: 44.66 min\n",
      "[185/900] Sample processed in 0.98s, ETA: 44.42 min\n",
      "[186/900] Sample processed in 0.99s, ETA: 44.18 min\n",
      "[187/900] Sample processed in 0.46s, ETA: 43.91 min\n",
      "[188/900] Sample processed in 1.03s, ETA: 43.68 min\n",
      "[189/900] Sample processed in 0.99s, ETA: 43.45 min\n",
      "[190/900] Sample processed in 0.97s, ETA: 43.22 min\n",
      "[191/900] Sample processed in 0.99s, ETA: 43.00 min\n",
      "[192/900] Sample processed in 1.04s, ETA: 42.78 min\n",
      "[193/900] Sample processed in 0.99s, ETA: 42.56 min\n",
      "[194/900] Sample processed in 0.57s, ETA: 42.31 min\n",
      "[195/900] Sample processed in 0.74s, ETA: 42.08 min\n",
      "[196/900] Sample processed in 1.01s, ETA: 41.87 min\n",
      "[197/900] Sample processed in 1.03s, ETA: 41.66 min\n",
      "[198/900] Sample processed in 0.56s, ETA: 41.42 min\n",
      "[199/900] Sample processed in 1.00s, ETA: 41.21 min\n",
      "[200/900] Sample processed in 0.98s, ETA: 41.01 min\n",
      "[201/900] Sample processed in 0.09s, ETA: 40.75 min\n",
      "[202/900] Sample processed in 0.15s, ETA: 40.50 min\n",
      "[203/900] Sample processed in 0.12s, ETA: 40.25 min\n",
      "[204/900] Sample processed in 0.09s, ETA: 40.00 min\n",
      "[205/900] Sample processed in 0.15s, ETA: 39.75 min\n",
      "[206/900] Sample processed in 0.09s, ETA: 39.51 min\n",
      "[207/900] Sample processed in 0.15s, ETA: 39.27 min\n",
      "[208/900] Sample processed in 0.15s, ETA: 39.03 min\n",
      "[209/900] Sample processed in 0.15s, ETA: 38.80 min\n",
      "[210/900] Sample processed in 0.16s, ETA: 38.57 min\n",
      "[211/900] Sample processed in 0.15s, ETA: 38.34 min\n",
      "[212/900] Sample processed in 0.09s, ETA: 38.10 min\n",
      "[213/900] Sample processed in 0.13s, ETA: 37.88 min\n",
      "[214/900] Sample processed in 0.12s, ETA: 37.65 min\n",
      "[215/900] Sample processed in 0.09s, ETA: 37.43 min\n",
      "[216/900] Sample processed in 0.12s, ETA: 37.21 min\n",
      "[217/900] Sample processed in 0.13s, ETA: 36.99 min\n",
      "[218/900] Sample processed in 0.10s, ETA: 36.77 min\n",
      "[219/900] Sample processed in 0.13s, ETA: 36.55 min\n",
      "[220/900] Sample processed in 0.13s, ETA: 36.34 min\n",
      "[221/900] Sample processed in 0.16s, ETA: 36.13 min\n",
      "[222/900] Sample processed in 0.09s, ETA: 35.92 min\n",
      "[223/900] Sample processed in 0.09s, ETA: 35.71 min\n",
      "[224/900] Sample processed in 0.09s, ETA: 35.50 min\n",
      "[225/900] Sample processed in 0.12s, ETA: 35.30 min\n",
      "[226/900] Sample processed in 0.15s, ETA: 35.10 min\n",
      "[227/900] Sample processed in 0.12s, ETA: 34.90 min\n",
      "[228/900] Sample processed in 0.09s, ETA: 34.70 min\n",
      "[229/900] Sample processed in 0.12s, ETA: 34.50 min\n",
      "[230/900] Sample processed in 0.09s, ETA: 34.30 min\n",
      "[231/900] Sample processed in 0.12s, ETA: 34.11 min\n",
      "[232/900] Sample processed in 0.09s, ETA: 33.92 min\n",
      "[233/900] Sample processed in 0.12s, ETA: 33.73 min\n",
      "[234/900] Sample processed in 0.09s, ETA: 33.54 min\n",
      "[235/900] Sample processed in 0.09s, ETA: 33.35 min\n",
      "[236/900] Sample processed in 0.09s, ETA: 33.16 min\n",
      "[237/900] Sample processed in 0.09s, ETA: 32.98 min\n",
      "[238/900] Sample processed in 0.09s, ETA: 32.79 min\n",
      "[239/900] Sample processed in 0.12s, ETA: 32.61 min\n",
      "[240/900] Sample processed in 0.09s, ETA: 32.43 min\n",
      "[241/900] Sample processed in 0.12s, ETA: 32.25 min\n",
      "[242/900] Sample processed in 0.09s, ETA: 32.07 min\n",
      "[243/900] Sample processed in 0.12s, ETA: 31.90 min\n",
      "[244/900] Sample processed in 0.12s, ETA: 31.73 min\n",
      "[245/900] Sample processed in 0.09s, ETA: 31.55 min\n",
      "[246/900] Sample processed in 0.09s, ETA: 31.38 min\n",
      "[247/900] Sample processed in 0.09s, ETA: 31.21 min\n",
      "[248/900] Sample processed in 0.09s, ETA: 31.04 min\n",
      "[249/900] Sample processed in 0.09s, ETA: 30.87 min\n",
      "[250/900] Sample processed in 0.10s, ETA: 30.70 min\n",
      "[251/900] Sample processed in 0.09s, ETA: 30.54 min\n",
      "[252/900] Sample processed in 0.09s, ETA: 30.37 min\n",
      "[253/900] Sample processed in 0.15s, ETA: 30.21 min\n",
      "[254/900] Sample processed in 0.12s, ETA: 30.05 min\n",
      "[255/900] Sample processed in 0.09s, ETA: 29.89 min\n",
      "[256/900] Sample processed in 0.09s, ETA: 29.73 min\n",
      "[257/900] Sample processed in 0.15s, ETA: 29.58 min\n",
      "[258/900] Sample processed in 0.15s, ETA: 29.43 min\n",
      "[259/900] Sample processed in 0.16s, ETA: 29.27 min\n",
      "[260/900] Sample processed in 0.15s, ETA: 29.12 min\n",
      "[261/900] Sample processed in 0.17s, ETA: 28.97 min\n",
      "[262/900] Sample processed in 0.15s, ETA: 28.82 min\n",
      "[263/900] Sample processed in 0.15s, ETA: 28.67 min\n",
      "[264/900] Sample processed in 0.15s, ETA: 28.53 min\n",
      "[265/900] Sample processed in 0.12s, ETA: 28.38 min\n",
      "[266/900] Sample processed in 0.15s, ETA: 28.23 min\n",
      "[267/900] Sample processed in 0.12s, ETA: 28.09 min\n",
      "[268/900] Sample processed in 0.09s, ETA: 27.94 min\n",
      "[269/900] Sample processed in 0.15s, ETA: 27.80 min\n",
      "[270/900] Sample processed in 0.15s, ETA: 27.66 min\n",
      "[271/900] Sample processed in 0.12s, ETA: 27.52 min\n",
      "[272/900] Sample processed in 0.15s, ETA: 27.38 min\n",
      "[273/900] Sample processed in 0.12s, ETA: 27.24 min\n",
      "[274/900] Sample processed in 0.15s, ETA: 27.10 min\n",
      "[275/900] Sample processed in 0.15s, ETA: 26.97 min\n",
      "[276/900] Sample processed in 0.09s, ETA: 26.83 min\n",
      "[277/900] Sample processed in 0.09s, ETA: 26.69 min\n",
      "[278/900] Sample processed in 0.15s, ETA: 26.56 min\n",
      "[279/900] Sample processed in 0.09s, ETA: 26.42 min\n",
      "[280/900] Sample processed in 0.09s, ETA: 26.29 min\n",
      "[281/900] Sample processed in 0.09s, ETA: 26.16 min\n",
      "[282/900] Sample processed in 0.15s, ETA: 26.03 min\n",
      "[283/900] Sample processed in 0.15s, ETA: 25.90 min\n",
      "[284/900] Sample processed in 0.09s, ETA: 25.77 min\n",
      "[285/900] Sample processed in 0.09s, ETA: 25.64 min\n",
      "[286/900] Sample processed in 0.15s, ETA: 25.52 min\n",
      "[287/900] Sample processed in 0.09s, ETA: 25.39 min\n",
      "[288/900] Sample processed in 0.16s, ETA: 25.27 min\n",
      "[289/900] Sample processed in 0.15s, ETA: 25.14 min\n",
      "[290/900] Sample processed in 0.15s, ETA: 25.02 min\n",
      "[291/900] Sample processed in 0.09s, ETA: 24.90 min\n",
      "[292/900] Sample processed in 0.12s, ETA: 24.77 min\n",
      "[293/900] Sample processed in 0.09s, ETA: 24.65 min\n",
      "[294/900] Sample processed in 0.16s, ETA: 24.53 min\n",
      "[295/900] Sample processed in 0.09s, ETA: 24.41 min\n",
      "[296/900] Sample processed in 0.14s, ETA: 24.30 min\n",
      "[297/900] Sample processed in 0.16s, ETA: 24.18 min\n",
      "[298/900] Sample processed in 0.09s, ETA: 24.06 min\n",
      "[299/900] Sample processed in 0.09s, ETA: 23.94 min\n",
      "[300/900] Sample processed in 0.12s, ETA: 23.83 min\n",
      "[301/900] Sample processed in 0.31s, ETA: 23.72 min\n",
      "[302/900] Sample processed in 0.10s, ETA: 23.60 min\n",
      "[303/900] Sample processed in 0.27s, ETA: 23.50 min\n",
      "[304/900] Sample processed in 0.27s, ETA: 23.39 min\n",
      "[305/900] Sample processed in 0.09s, ETA: 23.28 min\n",
      "[306/900] Sample processed in 0.24s, ETA: 23.17 min\n",
      "[307/900] Sample processed in 0.24s, ETA: 23.06 min\n",
      "[308/900] Sample processed in 0.21s, ETA: 22.96 min\n",
      "[309/900] Sample processed in 0.09s, ETA: 22.84 min\n",
      "[310/900] Sample processed in 0.09s, ETA: 22.74 min\n",
      "[311/900] Sample processed in 0.21s, ETA: 22.63 min\n",
      "[312/900] Sample processed in 0.43s, ETA: 22.53 min\n",
      "[313/900] Sample processed in 0.21s, ETA: 22.43 min\n",
      "[314/900] Sample processed in 0.10s, ETA: 22.32 min\n",
      "[315/900] Sample processed in 0.10s, ETA: 22.22 min\n",
      "[316/900] Sample processed in 0.10s, ETA: 22.11 min\n",
      "[317/900] Sample processed in 0.10s, ETA: 22.01 min\n",
      "[318/900] Sample processed in 0.09s, ETA: 21.90 min\n",
      "[319/900] Sample processed in 0.29s, ETA: 21.81 min\n",
      "[320/900] Sample processed in 0.10s, ETA: 21.70 min\n",
      "[321/900] Sample processed in 0.12s, ETA: 21.60 min\n",
      "[322/900] Sample processed in 0.24s, ETA: 21.51 min\n",
      "[323/900] Sample processed in 0.09s, ETA: 21.40 min\n",
      "[324/900] Sample processed in 0.30s, ETA: 21.31 min\n",
      "[325/900] Sample processed in 0.09s, ETA: 21.21 min\n",
      "[326/900] Sample processed in 0.09s, ETA: 21.11 min\n",
      "[327/900] Sample processed in 0.10s, ETA: 21.01 min\n",
      "[328/900] Sample processed in 0.09s, ETA: 20.92 min\n",
      "[329/900] Sample processed in 0.09s, ETA: 20.82 min\n",
      "[330/900] Sample processed in 0.48s, ETA: 20.73 min\n",
      "[331/900] Sample processed in 0.09s, ETA: 20.64 min\n",
      "[332/900] Sample processed in 0.30s, ETA: 20.55 min\n",
      "[333/900] Sample processed in 0.09s, ETA: 20.45 min\n",
      "[334/900] Sample processed in 0.10s, ETA: 20.36 min\n",
      "[335/900] Sample processed in 0.27s, ETA: 20.27 min\n",
      "[336/900] Sample processed in 0.10s, ETA: 20.17 min\n",
      "[337/900] Sample processed in 0.26s, ETA: 20.09 min\n",
      "[338/900] Sample processed in 0.09s, ETA: 19.99 min\n",
      "[339/900] Sample processed in 0.09s, ETA: 19.90 min\n",
      "[340/900] Sample processed in 0.10s, ETA: 19.81 min\n",
      "[341/900] Sample processed in 0.09s, ETA: 19.72 min\n",
      "[342/900] Sample processed in 0.55s, ETA: 19.64 min\n",
      "[343/900] Sample processed in 0.09s, ETA: 19.55 min\n",
      "[344/900] Sample processed in 0.38s, ETA: 19.47 min\n",
      "[345/900] Sample processed in 0.11s, ETA: 19.38 min\n",
      "[346/900] Sample processed in 0.12s, ETA: 19.29 min\n",
      "[347/900] Sample processed in 0.27s, ETA: 19.21 min\n",
      "[348/900] Sample processed in 0.09s, ETA: 19.12 min\n",
      "[349/900] Sample processed in 0.42s, ETA: 19.05 min\n",
      "[350/900] Sample processed in 0.27s, ETA: 18.96 min\n",
      "[351/900] Sample processed in 0.10s, ETA: 18.88 min\n",
      "[352/900] Sample processed in 0.09s, ETA: 18.79 min\n",
      "[353/900] Sample processed in 0.27s, ETA: 18.71 min\n",
      "[354/900] Sample processed in 0.09s, ETA: 18.63 min\n",
      "[355/900] Sample processed in 0.24s, ETA: 18.55 min\n",
      "[356/900] Sample processed in 0.39s, ETA: 18.47 min\n",
      "[357/900] Sample processed in 1.02s, ETA: 18.41 min\n",
      "[358/900] Sample processed in 0.21s, ETA: 18.33 min\n",
      "[359/900] Sample processed in 0.30s, ETA: 18.25 min\n",
      "[360/900] Sample processed in 0.09s, ETA: 18.17 min\n",
      "[361/900] Sample processed in 0.10s, ETA: 18.09 min\n",
      "[362/900] Sample processed in 0.10s, ETA: 18.01 min\n",
      "[363/900] Sample processed in 0.10s, ETA: 17.93 min\n",
      "[364/900] Sample processed in 0.10s, ETA: 17.85 min\n",
      "[365/900] Sample processed in 0.29s, ETA: 17.77 min\n",
      "[366/900] Sample processed in 0.27s, ETA: 17.70 min\n",
      "[367/900] Sample processed in 0.10s, ETA: 17.62 min\n",
      "[368/900] Sample processed in 0.09s, ETA: 17.54 min\n",
      "[369/900] Sample processed in 0.21s, ETA: 17.47 min\n",
      "[370/900] Sample processed in 0.30s, ETA: 17.39 min\n",
      "[371/900] Sample processed in 0.09s, ETA: 17.31 min\n",
      "[372/900] Sample processed in 0.09s, ETA: 17.24 min\n",
      "[373/900] Sample processed in 0.09s, ETA: 17.16 min\n",
      "[374/900] Sample processed in 0.10s, ETA: 17.09 min\n",
      "[375/900] Sample processed in 0.09s, ETA: 17.01 min\n",
      "[376/900] Sample processed in 0.09s, ETA: 16.93 min\n",
      "[377/900] Sample processed in 0.21s, ETA: 16.86 min\n",
      "[378/900] Sample processed in 0.30s, ETA: 16.79 min\n",
      "[379/900] Sample processed in 0.21s, ETA: 16.72 min\n",
      "[380/900] Sample processed in 0.10s, ETA: 16.65 min\n",
      "[381/900] Sample processed in 0.09s, ETA: 16.57 min\n",
      "[382/900] Sample processed in 0.37s, ETA: 16.51 min\n",
      "[383/900] Sample processed in 0.32s, ETA: 16.44 min\n",
      "[384/900] Sample processed in 0.09s, ETA: 16.37 min\n",
      "[385/900] Sample processed in 0.09s, ETA: 16.29 min\n",
      "[386/900] Sample processed in 0.35s, ETA: 16.23 min\n",
      "[387/900] Sample processed in 0.25s, ETA: 16.16 min\n",
      "[388/900] Sample processed in 0.24s, ETA: 16.09 min\n",
      "[389/900] Sample processed in 0.10s, ETA: 16.02 min\n",
      "[390/900] Sample processed in 0.10s, ETA: 15.95 min\n",
      "[391/900] Sample processed in 1.00s, ETA: 15.90 min\n",
      "[392/900] Sample processed in 0.15s, ETA: 15.83 min\n",
      "[393/900] Sample processed in 0.09s, ETA: 15.76 min\n",
      "[394/900] Sample processed in 0.42s, ETA: 15.70 min\n",
      "[395/900] Sample processed in 0.45s, ETA: 15.64 min\n",
      "[396/900] Sample processed in 0.09s, ETA: 15.57 min\n",
      "[397/900] Sample processed in 0.28s, ETA: 15.51 min\n",
      "[398/900] Sample processed in 0.09s, ETA: 15.44 min\n",
      "[399/900] Sample processed in 0.09s, ETA: 15.37 min\n",
      "[400/900] Sample processed in 0.09s, ETA: 15.30 min\n",
      "[401/900] Sample processed in 0.09s, ETA: 15.24 min\n",
      "[402/900] Sample processed in 0.36s, ETA: 15.18 min\n",
      "[403/900] Sample processed in 0.24s, ETA: 15.11 min\n",
      "[404/900] Sample processed in 0.09s, ETA: 15.05 min\n",
      "[405/900] Sample processed in 0.09s, ETA: 14.98 min\n",
      "[406/900] Sample processed in 0.09s, ETA: 14.92 min\n",
      "[407/900] Sample processed in 0.09s, ETA: 14.85 min\n",
      "[408/900] Sample processed in 0.09s, ETA: 14.79 min\n",
      "[409/900] Sample processed in 0.24s, ETA: 14.73 min\n",
      "[410/900] Sample processed in 0.09s, ETA: 14.66 min\n",
      "[411/900] Sample processed in 0.09s, ETA: 14.60 min\n",
      "[412/900] Sample processed in 0.09s, ETA: 14.54 min\n",
      "[413/900] Sample processed in 0.09s, ETA: 14.47 min\n",
      "[414/900] Sample processed in 0.24s, ETA: 14.41 min\n",
      "[415/900] Sample processed in 0.09s, ETA: 14.35 min\n",
      "[416/900] Sample processed in 0.09s, ETA: 14.29 min\n",
      "[417/900] Sample processed in 0.09s, ETA: 14.23 min\n",
      "[418/900] Sample processed in 0.09s, ETA: 14.16 min\n",
      "[419/900] Sample processed in 0.09s, ETA: 14.10 min\n",
      "[420/900] Sample processed in 0.15s, ETA: 14.04 min\n",
      "[421/900] Sample processed in 0.09s, ETA: 13.98 min\n",
      "[422/900] Sample processed in 0.09s, ETA: 13.92 min\n",
      "[423/900] Sample processed in 0.15s, ETA: 13.86 min\n",
      "[424/900] Sample processed in 0.24s, ETA: 13.80 min\n",
      "[425/900] Sample processed in 0.09s, ETA: 13.74 min\n",
      "[426/900] Sample processed in 0.09s, ETA: 13.69 min\n",
      "[427/900] Sample processed in 0.09s, ETA: 13.63 min\n",
      "[428/900] Sample processed in 0.09s, ETA: 13.57 min\n",
      "[429/900] Sample processed in 0.09s, ETA: 13.51 min\n",
      "[430/900] Sample processed in 0.09s, ETA: 13.45 min\n",
      "[431/900] Sample processed in 0.09s, ETA: 13.39 min\n",
      "[432/900] Sample processed in 0.21s, ETA: 13.34 min\n",
      "[433/900] Sample processed in 0.09s, ETA: 13.28 min\n",
      "[434/900] Sample processed in 0.09s, ETA: 13.22 min\n",
      "[435/900] Sample processed in 0.09s, ETA: 13.16 min\n",
      "[436/900] Sample processed in 0.09s, ETA: 13.11 min\n",
      "[437/900] Sample processed in 0.09s, ETA: 13.05 min\n",
      "[438/900] Sample processed in 0.09s, ETA: 12.99 min\n",
      "[439/900] Sample processed in 0.09s, ETA: 12.94 min\n",
      "[440/900] Sample processed in 0.09s, ETA: 12.88 min\n",
      "[441/900] Sample processed in 0.09s, ETA: 12.83 min\n",
      "[442/900] Sample processed in 0.24s, ETA: 12.77 min\n",
      "[443/900] Sample processed in 0.09s, ETA: 12.72 min\n",
      "[444/900] Sample processed in 0.09s, ETA: 12.66 min\n",
      "[445/900] Sample processed in 0.20s, ETA: 12.61 min\n",
      "[446/900] Sample processed in 0.09s, ETA: 12.56 min\n",
      "[447/900] Sample processed in 0.09s, ETA: 12.50 min\n",
      "[448/900] Sample processed in 0.16s, ETA: 12.45 min\n",
      "[449/900] Sample processed in 0.09s, ETA: 12.40 min\n",
      "[450/900] Sample processed in 0.34s, ETA: 12.35 min\n",
      "[451/900] Sample processed in 0.25s, ETA: 12.30 min\n",
      "[452/900] Sample processed in 0.11s, ETA: 12.24 min\n",
      "[453/900] Sample processed in 0.09s, ETA: 12.19 min\n",
      "[454/900] Sample processed in 0.09s, ETA: 12.14 min\n",
      "[455/900] Sample processed in 0.09s, ETA: 12.09 min\n",
      "[456/900] Sample processed in 0.09s, ETA: 12.03 min\n",
      "[457/900] Sample processed in 0.24s, ETA: 11.98 min\n",
      "[458/900] Sample processed in 0.09s, ETA: 11.93 min\n",
      "[459/900] Sample processed in 0.09s, ETA: 11.88 min\n",
      "[460/900] Sample processed in 0.09s, ETA: 11.83 min\n",
      "[461/900] Sample processed in 0.09s, ETA: 11.78 min\n",
      "[462/900] Sample processed in 0.09s, ETA: 11.73 min\n",
      "[463/900] Sample processed in 0.09s, ETA: 11.68 min\n",
      "[464/900] Sample processed in 0.09s, ETA: 11.63 min\n",
      "[465/900] Sample processed in 0.09s, ETA: 11.58 min\n",
      "[466/900] Sample processed in 0.09s, ETA: 11.53 min\n",
      "[467/900] Sample processed in 0.39s, ETA: 11.48 min\n",
      "[468/900] Sample processed in 0.09s, ETA: 11.43 min\n",
      "[469/900] Sample processed in 0.09s, ETA: 11.38 min\n",
      "[470/900] Sample processed in 0.09s, ETA: 11.33 min\n",
      "[471/900] Sample processed in 0.09s, ETA: 11.28 min\n",
      "[472/900] Sample processed in 0.09s, ETA: 11.24 min\n",
      "[473/900] Sample processed in 0.09s, ETA: 11.19 min\n",
      "[474/900] Sample processed in 0.09s, ETA: 11.14 min\n",
      "[475/900] Sample processed in 0.10s, ETA: 11.09 min\n",
      "[476/900] Sample processed in 0.09s, ETA: 11.04 min\n",
      "[477/900] Sample processed in 0.09s, ETA: 10.99 min\n",
      "[478/900] Sample processed in 0.09s, ETA: 10.95 min\n",
      "[479/900] Sample processed in 0.09s, ETA: 10.90 min\n",
      "[480/900] Sample processed in 0.09s, ETA: 10.85 min\n",
      "[481/900] Sample processed in 0.09s, ETA: 10.81 min\n",
      "[482/900] Sample processed in 0.09s, ETA: 10.76 min\n",
      "[483/900] Sample processed in 0.09s, ETA: 10.71 min\n",
      "[484/900] Sample processed in 0.09s, ETA: 10.67 min\n",
      "[485/900] Sample processed in 0.09s, ETA: 10.62 min\n",
      "[486/900] Sample processed in 0.25s, ETA: 10.58 min\n",
      "[487/900] Sample processed in 0.09s, ETA: 10.53 min\n",
      "[488/900] Sample processed in 0.09s, ETA: 10.48 min\n",
      "[489/900] Sample processed in 0.10s, ETA: 10.44 min\n",
      "[490/900] Sample processed in 0.09s, ETA: 10.39 min\n",
      "[491/900] Sample processed in 0.42s, ETA: 10.35 min\n",
      "[492/900] Sample processed in 0.09s, ETA: 10.31 min\n",
      "[493/900] Sample processed in 0.09s, ETA: 10.26 min\n",
      "[494/900] Sample processed in 0.09s, ETA: 10.22 min\n",
      "[495/900] Sample processed in 0.21s, ETA: 10.17 min\n",
      "[496/900] Sample processed in 0.21s, ETA: 10.13 min\n",
      "[497/900] Sample processed in 0.24s, ETA: 10.09 min\n",
      "[498/900] Sample processed in 0.15s, ETA: 10.05 min\n",
      "[499/900] Sample processed in 0.21s, ETA: 10.00 min\n",
      "[500/900] Sample processed in 0.09s, ETA: 9.96 min\n",
      "[501/900] Sample processed in 0.10s, ETA: 9.92 min\n",
      "[502/900] Sample processed in 0.11s, ETA: 9.87 min\n",
      "[503/900] Sample processed in 0.10s, ETA: 9.83 min\n",
      "[504/900] Sample processed in 0.11s, ETA: 9.79 min\n",
      "[505/900] Sample processed in 0.10s, ETA: 9.75 min\n",
      "[506/900] Sample processed in 0.11s, ETA: 9.70 min\n",
      "[507/900] Sample processed in 0.09s, ETA: 9.66 min\n",
      "[508/900] Sample processed in 0.10s, ETA: 9.62 min\n",
      "[509/900] Sample processed in 0.11s, ETA: 9.58 min\n",
      "[510/900] Sample processed in 0.10s, ETA: 9.53 min\n",
      "[511/900] Sample processed in 0.10s, ETA: 9.49 min\n",
      "[512/900] Sample processed in 0.09s, ETA: 9.45 min\n",
      "[513/900] Sample processed in 0.09s, ETA: 9.41 min\n",
      "[514/900] Sample processed in 0.10s, ETA: 9.37 min\n",
      "[515/900] Sample processed in 0.65s, ETA: 9.33 min\n",
      "[516/900] Sample processed in 0.09s, ETA: 9.29 min\n",
      "[517/900] Sample processed in 0.09s, ETA: 9.25 min\n",
      "[518/900] Sample processed in 0.13s, ETA: 9.21 min\n",
      "[519/900] Sample processed in 0.09s, ETA: 9.17 min\n",
      "[520/900] Sample processed in 0.09s, ETA: 9.13 min\n",
      "[521/900] Sample processed in 0.10s, ETA: 9.09 min\n",
      "[522/900] Sample processed in 0.09s, ETA: 9.05 min\n",
      "[523/900] Sample processed in 0.09s, ETA: 9.01 min\n",
      "[524/900] Sample processed in 0.10s, ETA: 8.97 min\n",
      "[525/900] Sample processed in 0.10s, ETA: 8.93 min\n",
      "[526/900] Sample processed in 0.11s, ETA: 8.89 min\n",
      "[527/900] Sample processed in 0.11s, ETA: 8.85 min\n",
      "[528/900] Sample processed in 0.11s, ETA: 8.81 min\n",
      "[529/900] Sample processed in 0.09s, ETA: 8.77 min\n",
      "[530/900] Sample processed in 0.12s, ETA: 8.73 min\n",
      "[531/900] Sample processed in 0.09s, ETA: 8.69 min\n",
      "[532/900] Sample processed in 0.09s, ETA: 8.66 min\n",
      "[533/900] Sample processed in 0.10s, ETA: 8.62 min\n",
      "[534/900] Sample processed in 0.10s, ETA: 8.58 min\n",
      "[535/900] Sample processed in 0.10s, ETA: 8.54 min\n",
      "[536/900] Sample processed in 0.10s, ETA: 8.50 min\n",
      "[537/900] Sample processed in 0.10s, ETA: 8.46 min\n",
      "[538/900] Sample processed in 0.10s, ETA: 8.43 min\n",
      "[539/900] Sample processed in 0.10s, ETA: 8.39 min\n",
      "[540/900] Sample processed in 0.10s, ETA: 8.35 min\n",
      "[541/900] Sample processed in 0.09s, ETA: 8.31 min\n",
      "[542/900] Sample processed in 0.10s, ETA: 8.28 min\n",
      "[543/900] Sample processed in 0.10s, ETA: 8.24 min\n",
      "[544/900] Sample processed in 0.09s, ETA: 8.20 min\n",
      "[545/900] Sample processed in 0.09s, ETA: 8.16 min\n",
      "[546/900] Sample processed in 0.10s, ETA: 8.13 min\n",
      "[547/900] Sample processed in 0.09s, ETA: 8.09 min\n",
      "[548/900] Sample processed in 0.10s, ETA: 8.05 min\n",
      "[549/900] Sample processed in 0.10s, ETA: 8.02 min\n",
      "[550/900] Sample processed in 0.10s, ETA: 7.98 min\n",
      "[551/900] Sample processed in 0.88s, ETA: 7.95 min\n",
      "[552/900] Sample processed in 0.09s, ETA: 7.92 min\n",
      "[553/900] Sample processed in 0.12s, ETA: 7.88 min\n",
      "[554/900] Sample processed in 0.11s, ETA: 7.85 min\n",
      "[555/900] Sample processed in 0.10s, ETA: 7.81 min\n",
      "[556/900] Sample processed in 0.09s, ETA: 7.77 min\n",
      "[557/900] Sample processed in 0.10s, ETA: 7.74 min\n",
      "[558/900] Sample processed in 0.10s, ETA: 7.70 min\n",
      "[559/900] Sample processed in 0.10s, ETA: 7.67 min\n",
      "[560/900] Sample processed in 0.10s, ETA: 7.63 min\n",
      "[561/900] Sample processed in 0.10s, ETA: 7.60 min\n",
      "[562/900] Sample processed in 0.11s, ETA: 7.56 min\n",
      "[563/900] Sample processed in 0.10s, ETA: 7.53 min\n",
      "[564/900] Sample processed in 0.10s, ETA: 7.49 min\n",
      "[565/900] Sample processed in 0.10s, ETA: 7.46 min\n",
      "[566/900] Sample processed in 0.10s, ETA: 7.42 min\n",
      "[567/900] Sample processed in 0.11s, ETA: 7.39 min\n",
      "[568/900] Sample processed in 0.10s, ETA: 7.36 min\n",
      "[569/900] Sample processed in 0.10s, ETA: 7.32 min\n",
      "[570/900] Sample processed in 0.10s, ETA: 7.29 min\n",
      "[571/900] Sample processed in 0.10s, ETA: 7.25 min\n",
      "[572/900] Sample processed in 0.10s, ETA: 7.22 min\n",
      "[573/900] Sample processed in 0.12s, ETA: 7.19 min\n",
      "[574/900] Sample processed in 0.11s, ETA: 7.15 min\n",
      "[575/900] Sample processed in 0.11s, ETA: 7.12 min\n",
      "[576/900] Sample processed in 0.11s, ETA: 7.09 min\n",
      "[577/900] Sample processed in 0.12s, ETA: 7.05 min\n",
      "[578/900] Sample processed in 0.10s, ETA: 7.02 min\n",
      "[579/900] Sample processed in 0.10s, ETA: 6.99 min\n",
      "[580/900] Sample processed in 0.11s, ETA: 6.96 min\n",
      "[581/900] Sample processed in 0.10s, ETA: 6.92 min\n",
      "[582/900] Sample processed in 0.14s, ETA: 6.89 min\n",
      "[583/900] Sample processed in 0.11s, ETA: 6.86 min\n",
      "[584/900] Sample processed in 0.10s, ETA: 6.83 min\n",
      "[585/900] Sample processed in 0.10s, ETA: 6.79 min\n",
      "[586/900] Sample processed in 0.11s, ETA: 6.76 min\n",
      "[587/900] Sample processed in 0.10s, ETA: 6.73 min\n",
      "[588/900] Sample processed in 0.10s, ETA: 6.70 min\n",
      "[589/900] Sample processed in 0.10s, ETA: 6.66 min\n",
      "[590/900] Sample processed in 0.09s, ETA: 6.63 min\n",
      "[591/900] Sample processed in 0.10s, ETA: 6.60 min\n",
      "[592/900] Sample processed in 0.09s, ETA: 6.57 min\n",
      "[593/900] Sample processed in 0.10s, ETA: 6.54 min\n",
      "[594/900] Sample processed in 0.10s, ETA: 6.51 min\n",
      "[595/900] Sample processed in 0.11s, ETA: 6.48 min\n",
      "[596/900] Sample processed in 0.10s, ETA: 6.44 min\n",
      "[597/900] Sample processed in 0.10s, ETA: 6.41 min\n",
      "[598/900] Sample processed in 0.09s, ETA: 6.38 min\n",
      "[599/900] Sample processed in 0.10s, ETA: 6.35 min\n",
      "[600/900] Sample processed in 0.09s, ETA: 6.32 min\n",
      "[601/900] Sample processed in 0.15s, ETA: 6.29 min\n",
      "[602/900] Sample processed in 0.15s, ETA: 6.26 min\n",
      "[603/900] Sample processed in 0.21s, ETA: 6.23 min\n",
      "[604/900] Sample processed in 0.16s, ETA: 6.20 min\n",
      "[605/900] Sample processed in 0.15s, ETA: 6.17 min\n",
      "[606/900] Sample processed in 0.21s, ETA: 6.14 min\n",
      "[607/900] Sample processed in 0.15s, ETA: 6.11 min\n",
      "[608/900] Sample processed in 0.22s, ETA: 6.08 min\n",
      "[609/900] Sample processed in 0.16s, ETA: 6.05 min\n",
      "[610/900] Sample processed in 0.16s, ETA: 6.02 min\n",
      "[611/900] Sample processed in 0.17s, ETA: 5.99 min\n",
      "[612/900] Sample processed in 0.21s, ETA: 5.96 min\n",
      "[613/900] Sample processed in 0.15s, ETA: 5.94 min\n",
      "[614/900] Sample processed in 0.15s, ETA: 5.91 min\n",
      "[615/900] Sample processed in 0.15s, ETA: 5.88 min\n",
      "[616/900] Sample processed in 0.15s, ETA: 5.85 min\n",
      "[617/900] Sample processed in 0.15s, ETA: 5.82 min\n",
      "[618/900] Sample processed in 0.15s, ETA: 5.79 min\n",
      "[619/900] Sample processed in 0.24s, ETA: 5.76 min\n",
      "[620/900] Sample processed in 0.15s, ETA: 5.73 min\n",
      "[621/900] Sample processed in 0.18s, ETA: 5.71 min\n",
      "[622/900] Sample processed in 0.15s, ETA: 5.68 min\n",
      "[623/900] Sample processed in 0.15s, ETA: 5.65 min\n",
      "[624/900] Sample processed in 0.15s, ETA: 5.62 min\n",
      "[625/900] Sample processed in 0.15s, ETA: 5.59 min\n",
      "[626/900] Sample processed in 0.15s, ETA: 5.56 min\n",
      "[627/900] Sample processed in 0.15s, ETA: 5.54 min\n",
      "[628/900] Sample processed in 0.16s, ETA: 5.51 min\n",
      "[629/900] Sample processed in 0.15s, ETA: 5.48 min\n",
      "[630/900] Sample processed in 0.22s, ETA: 5.45 min\n",
      "[631/900] Sample processed in 0.15s, ETA: 5.43 min\n",
      "[632/900] Sample processed in 0.16s, ETA: 5.40 min\n",
      "[633/900] Sample processed in 0.16s, ETA: 5.37 min\n",
      "[634/900] Sample processed in 0.22s, ETA: 5.34 min\n",
      "[635/900] Sample processed in 0.25s, ETA: 5.32 min\n",
      "[636/900] Sample processed in 0.17s, ETA: 5.29 min\n",
      "[637/900] Sample processed in 0.15s, ETA: 5.26 min\n",
      "[638/900] Sample processed in 0.15s, ETA: 5.23 min\n",
      "[639/900] Sample processed in 0.15s, ETA: 5.21 min\n",
      "[640/900] Sample processed in 0.15s, ETA: 5.18 min\n",
      "[641/900] Sample processed in 0.15s, ETA: 5.15 min\n",
      "[642/900] Sample processed in 0.15s, ETA: 5.13 min\n",
      "[643/900] Sample processed in 0.15s, ETA: 5.10 min\n",
      "[644/900] Sample processed in 0.18s, ETA: 5.07 min\n",
      "[645/900] Sample processed in 0.21s, ETA: 5.05 min\n",
      "[646/900] Sample processed in 0.15s, ETA: 5.02 min\n",
      "[647/900] Sample processed in 0.15s, ETA: 4.99 min\n",
      "[648/900] Sample processed in 0.15s, ETA: 4.97 min\n",
      "[649/900] Sample processed in 0.15s, ETA: 4.94 min\n",
      "[650/900] Sample processed in 0.15s, ETA: 4.91 min\n",
      "[651/900] Sample processed in 0.15s, ETA: 4.89 min\n",
      "[652/900] Sample processed in 0.15s, ETA: 4.86 min\n",
      "[653/900] Sample processed in 0.15s, ETA: 4.84 min\n",
      "[654/900] Sample processed in 0.15s, ETA: 4.81 min\n",
      "[655/900] Sample processed in 0.15s, ETA: 4.78 min\n",
      "[656/900] Sample processed in 0.18s, ETA: 4.76 min\n",
      "[657/900] Sample processed in 0.17s, ETA: 4.73 min\n",
      "[658/900] Sample processed in 0.21s, ETA: 4.71 min\n",
      "[659/900] Sample processed in 0.18s, ETA: 4.68 min\n",
      "[660/900] Sample processed in 0.22s, ETA: 4.66 min\n",
      "[661/900] Sample processed in 0.15s, ETA: 4.63 min\n",
      "[662/900] Sample processed in 0.22s, ETA: 4.61 min\n",
      "[663/900] Sample processed in 0.17s, ETA: 4.58 min\n",
      "[664/900] Sample processed in 0.15s, ETA: 4.56 min\n",
      "[665/900] Sample processed in 0.15s, ETA: 4.53 min\n",
      "[666/900] Sample processed in 0.15s, ETA: 4.51 min\n",
      "[667/900] Sample processed in 0.15s, ETA: 4.48 min\n",
      "[668/900] Sample processed in 0.15s, ETA: 4.46 min\n",
      "[669/900] Sample processed in 0.15s, ETA: 4.43 min\n",
      "[670/900] Sample processed in 0.15s, ETA: 4.41 min\n",
      "[671/900] Sample processed in 0.15s, ETA: 4.38 min\n",
      "[672/900] Sample processed in 0.15s, ETA: 4.36 min\n",
      "[673/900] Sample processed in 0.18s, ETA: 4.33 min\n",
      "[674/900] Sample processed in 0.15s, ETA: 4.31 min\n",
      "[675/900] Sample processed in 0.15s, ETA: 4.28 min\n",
      "[676/900] Sample processed in 0.15s, ETA: 4.26 min\n",
      "[677/900] Sample processed in 0.15s, ETA: 4.23 min\n",
      "[678/900] Sample processed in 0.15s, ETA: 4.21 min\n",
      "[679/900] Sample processed in 0.15s, ETA: 4.18 min\n",
      "[680/900] Sample processed in 0.15s, ETA: 4.16 min\n",
      "[681/900] Sample processed in 0.18s, ETA: 4.14 min\n",
      "[682/900] Sample processed in 0.15s, ETA: 4.11 min\n",
      "[683/900] Sample processed in 0.15s, ETA: 4.09 min\n",
      "[684/900] Sample processed in 0.15s, ETA: 4.06 min\n",
      "[685/900] Sample processed in 0.15s, ETA: 4.04 min\n",
      "[686/900] Sample processed in 0.15s, ETA: 4.02 min\n",
      "[687/900] Sample processed in 0.16s, ETA: 3.99 min\n",
      "[688/900] Sample processed in 0.15s, ETA: 3.97 min\n",
      "[689/900] Sample processed in 0.15s, ETA: 3.94 min\n",
      "[690/900] Sample processed in 0.15s, ETA: 3.92 min\n",
      "[691/900] Sample processed in 0.15s, ETA: 3.90 min\n",
      "[692/900] Sample processed in 0.16s, ETA: 3.87 min\n",
      "[693/900] Sample processed in 0.15s, ETA: 3.85 min\n",
      "[694/900] Sample processed in 0.17s, ETA: 3.83 min\n",
      "[695/900] Sample processed in 0.15s, ETA: 3.80 min\n",
      "[696/900] Sample processed in 0.21s, ETA: 3.78 min\n",
      "[697/900] Sample processed in 0.15s, ETA: 3.76 min\n",
      "[698/900] Sample processed in 0.15s, ETA: 3.73 min\n",
      "[699/900] Sample processed in 0.15s, ETA: 3.71 min\n",
      "[700/900] Sample processed in 0.15s, ETA: 3.69 min\n",
      "[701/900] Sample processed in 0.50s, ETA: 3.67 min\n",
      "[702/900] Sample processed in 0.45s, ETA: 3.65 min\n",
      "[703/900] Sample processed in 0.56s, ETA: 3.62 min\n",
      "[704/900] Sample processed in 0.47s, ETA: 3.60 min\n",
      "[705/900] Sample processed in 0.50s, ETA: 3.58 min\n",
      "[706/900] Sample processed in 0.73s, ETA: 3.56 min\n",
      "[707/900] Sample processed in 0.50s, ETA: 3.54 min\n",
      "[708/900] Sample processed in 0.50s, ETA: 3.52 min\n",
      "[709/900] Sample processed in 0.41s, ETA: 3.50 min\n",
      "[710/900] Sample processed in 0.56s, ETA: 3.48 min\n",
      "[711/900] Sample processed in 0.41s, ETA: 3.46 min\n",
      "[712/900] Sample processed in 0.49s, ETA: 3.44 min\n",
      "[713/900] Sample processed in 0.48s, ETA: 3.41 min\n",
      "[714/900] Sample processed in 0.41s, ETA: 3.39 min\n",
      "[715/900] Sample processed in 0.39s, ETA: 3.37 min\n",
      "[716/900] Sample processed in 0.44s, ETA: 3.35 min\n",
      "[717/900] Sample processed in 0.30s, ETA: 3.33 min\n",
      "[718/900] Sample processed in 0.56s, ETA: 3.31 min\n",
      "[719/900] Sample processed in 0.33s, ETA: 3.29 min\n",
      "[720/900] Sample processed in 0.41s, ETA: 3.27 min\n",
      "[721/900] Sample processed in 0.38s, ETA: 3.25 min\n",
      "[722/900] Sample processed in 0.51s, ETA: 3.23 min\n",
      "[723/900] Sample processed in 0.43s, ETA: 3.20 min\n",
      "[724/900] Sample processed in 0.42s, ETA: 3.18 min\n",
      "[725/900] Sample processed in 0.54s, ETA: 3.16 min\n",
      "[726/900] Sample processed in 0.44s, ETA: 3.14 min\n",
      "[727/900] Sample processed in 0.44s, ETA: 3.12 min\n",
      "[728/900] Sample processed in 0.38s, ETA: 3.10 min\n",
      "[729/900] Sample processed in 0.36s, ETA: 3.08 min\n",
      "[730/900] Sample processed in 0.32s, ETA: 3.06 min\n",
      "[731/900] Sample processed in 0.50s, ETA: 3.04 min\n",
      "[732/900] Sample processed in 0.38s, ETA: 3.02 min\n",
      "[733/900] Sample processed in 0.33s, ETA: 3.00 min\n",
      "[734/900] Sample processed in 0.35s, ETA: 2.98 min\n",
      "[735/900] Sample processed in 0.44s, ETA: 2.96 min\n",
      "[736/900] Sample processed in 0.42s, ETA: 2.94 min\n",
      "[737/900] Sample processed in 0.50s, ETA: 2.92 min\n",
      "[738/900] Sample processed in 0.39s, ETA: 2.90 min\n",
      "[739/900] Sample processed in 0.72s, ETA: 2.88 min\n",
      "[740/900] Sample processed in 0.39s, ETA: 2.86 min\n",
      "[741/900] Sample processed in 0.47s, ETA: 2.84 min\n",
      "[742/900] Sample processed in 0.49s, ETA: 2.82 min\n",
      "[743/900] Sample processed in 0.47s, ETA: 2.80 min\n",
      "[744/900] Sample processed in 0.41s, ETA: 2.78 min\n",
      "[745/900] Sample processed in 0.29s, ETA: 2.76 min\n",
      "[746/900] Sample processed in 0.35s, ETA: 2.74 min\n",
      "[747/900] Sample processed in 0.50s, ETA: 2.72 min\n",
      "[748/900] Sample processed in 0.42s, ETA: 2.70 min\n",
      "[749/900] Sample processed in 0.39s, ETA: 2.68 min\n",
      "[750/900] Sample processed in 0.37s, ETA: 2.66 min\n",
      "[751/900] Sample processed in 0.32s, ETA: 2.64 min\n",
      "[752/900] Sample processed in 0.68s, ETA: 2.62 min\n",
      "[753/900] Sample processed in 0.42s, ETA: 2.60 min\n",
      "[754/900] Sample processed in 0.32s, ETA: 2.58 min\n",
      "[755/900] Sample processed in 0.35s, ETA: 2.56 min\n",
      "[756/900] Sample processed in 0.38s, ETA: 2.54 min\n",
      "[757/900] Sample processed in 0.72s, ETA: 2.52 min\n",
      "[758/900] Sample processed in 0.45s, ETA: 2.50 min\n",
      "[759/900] Sample processed in 0.40s, ETA: 2.48 min\n",
      "[760/900] Sample processed in 0.49s, ETA: 2.46 min\n",
      "[761/900] Sample processed in 0.44s, ETA: 2.44 min\n",
      "[762/900] Sample processed in 0.39s, ETA: 2.42 min\n",
      "[763/900] Sample processed in 0.32s, ETA: 2.40 min\n",
      "[764/900] Sample processed in 0.33s, ETA: 2.38 min\n",
      "[765/900] Sample processed in 0.47s, ETA: 2.36 min\n",
      "[766/900] Sample processed in 0.41s, ETA: 2.34 min\n",
      "[767/900] Sample processed in 0.47s, ETA: 2.32 min\n",
      "[768/900] Sample processed in 0.32s, ETA: 2.30 min\n",
      "[769/900] Sample processed in 0.41s, ETA: 2.29 min\n",
      "[770/900] Sample processed in 0.44s, ETA: 2.27 min\n",
      "[771/900] Sample processed in 0.48s, ETA: 2.25 min\n",
      "[772/900] Sample processed in 0.36s, ETA: 2.23 min\n",
      "[773/900] Sample processed in 0.48s, ETA: 2.21 min\n",
      "[774/900] Sample processed in 0.53s, ETA: 2.19 min\n",
      "[775/900] Sample processed in 0.71s, ETA: 2.17 min\n",
      "[776/900] Sample processed in 0.62s, ETA: 2.15 min\n",
      "[777/900] Sample processed in 0.35s, ETA: 2.13 min\n",
      "[778/900] Sample processed in 0.44s, ETA: 2.12 min\n",
      "[779/900] Sample processed in 0.38s, ETA: 2.10 min\n",
      "[780/900] Sample processed in 0.41s, ETA: 2.08 min\n",
      "[781/900] Sample processed in 0.56s, ETA: 2.06 min\n",
      "[782/900] Sample processed in 0.35s, ETA: 2.04 min\n",
      "[783/900] Sample processed in 0.41s, ETA: 2.02 min\n",
      "[784/900] Sample processed in 0.38s, ETA: 2.00 min\n",
      "[785/900] Sample processed in 0.47s, ETA: 1.98 min\n",
      "[786/900] Sample processed in 0.43s, ETA: 1.96 min\n",
      "[787/900] Sample processed in 0.33s, ETA: 1.95 min\n",
      "[788/900] Sample processed in 0.59s, ETA: 1.93 min\n",
      "[789/900] Sample processed in 0.56s, ETA: 1.91 min\n",
      "[790/900] Sample processed in 0.53s, ETA: 1.89 min\n",
      "[791/900] Sample processed in 0.44s, ETA: 1.87 min\n",
      "[792/900] Sample processed in 0.32s, ETA: 1.85 min\n",
      "[793/900] Sample processed in 0.73s, ETA: 1.84 min\n",
      "[794/900] Sample processed in 0.44s, ETA: 1.82 min\n",
      "[795/900] Sample processed in 0.44s, ETA: 1.80 min\n",
      "[796/900] Sample processed in 0.51s, ETA: 1.78 min\n",
      "[797/900] Sample processed in 0.47s, ETA: 1.76 min\n",
      "[798/900] Sample processed in 0.32s, ETA: 1.74 min\n",
      "[799/900] Sample processed in 0.50s, ETA: 1.73 min\n",
      "[800/900] Sample processed in 0.29s, ETA: 1.71 min\n",
      "[801/900] Sample processed in 15.50s, ETA: 1.72 min\n",
      "[802/900] Sample processed in 8.90s, ETA: 1.72 min\n",
      "[803/900] Sample processed in 10.05s, ETA: 1.72 min\n",
      "[804/900] Sample processed in 15.29s, ETA: 1.73 min\n",
      "[805/900] Sample processed in 4.57s, ETA: 1.72 min\n",
      "[806/900] Sample processed in 3.83s, ETA: 1.71 min\n",
      "[807/900] Sample processed in 7.25s, ETA: 1.70 min\n",
      "[808/900] Sample processed in 10.19s, ETA: 1.70 min\n",
      "[809/900] Sample processed in 3.22s, ETA: 1.68 min\n",
      "[810/900] Sample processed in 3.65s, ETA: 1.67 min\n",
      "[811/900] Sample processed in 11.01s, ETA: 1.67 min\n",
      "[812/900] Sample processed in 11.35s, ETA: 1.67 min\n",
      "[813/900] Sample processed in 9.80s, ETA: 1.67 min\n",
      "[814/900] Sample processed in 2.61s, ETA: 1.65 min\n",
      "[815/900] Sample processed in 7.82s, ETA: 1.64 min\n",
      "[816/900] Sample processed in 12.12s, ETA: 1.64 min\n",
      "[817/900] Sample processed in 8.60s, ETA: 1.63 min\n",
      "[818/900] Sample processed in 2.20s, ETA: 1.62 min\n",
      "[819/900] Sample processed in 15.22s, ETA: 1.62 min\n",
      "[820/900] Sample processed in 10.63s, ETA: 1.61 min\n",
      "[821/900] Sample processed in 4.69s, ETA: 1.60 min\n",
      "[822/900] Sample processed in 7.99s, ETA: 1.59 min\n",
      "[823/900] Sample processed in 10.50s, ETA: 1.58 min\n",
      "[824/900] Sample processed in 15.19s, ETA: 1.59 min\n",
      "[825/900] Sample processed in 6.83s, ETA: 1.57 min\n",
      "[826/900] Sample processed in 15.18s, ETA: 1.57 min\n",
      "[827/900] Sample processed in 9.62s, ETA: 1.56 min\n",
      "[828/900] Sample processed in 15.21s, ETA: 1.56 min\n",
      "[829/900] Sample processed in 2.33s, ETA: 1.54 min\n",
      "[830/900] Sample processed in 7.10s, ETA: 1.53 min\n",
      "[831/900] Sample processed in 2.99s, ETA: 1.51 min\n",
      "[832/900] Sample processed in 15.23s, ETA: 1.51 min\n",
      "[833/900] Sample processed in 9.72s, ETA: 1.50 min\n",
      "[834/900] Sample processed in 6.26s, ETA: 1.48 min\n",
      "[835/900] Sample processed in 4.16s, ETA: 1.46 min\n",
      "[836/900] Sample processed in 4.51s, ETA: 1.44 min\n",
      "[837/900] Sample processed in 15.20s, ETA: 1.44 min\n",
      "[838/900] Sample processed in 15.22s, ETA: 1.43 min\n",
      "[839/900] Sample processed in 5.82s, ETA: 1.41 min\n",
      "[840/900] Sample processed in 6.76s, ETA: 1.40 min\n",
      "[841/900] Sample processed in 15.24s, ETA: 1.39 min\n",
      "[842/900] Sample processed in 15.22s, ETA: 1.38 min\n",
      "[843/900] Sample processed in 7.29s, ETA: 1.37 min\n",
      "[844/900] Sample processed in 5.56s, ETA: 1.35 min\n",
      "[845/900] Sample processed in 15.19s, ETA: 1.34 min\n",
      "[846/900] Sample processed in 5.83s, ETA: 1.32 min\n",
      "[847/900] Sample processed in 15.22s, ETA: 1.31 min\n",
      "[848/900] Sample processed in 15.21s, ETA: 1.30 min\n",
      "[849/900] Sample processed in 8.89s, ETA: 1.28 min\n",
      "[850/900] Sample processed in 1.62s, ETA: 1.25 min\n",
      "[851/900] Sample processed in 15.22s, ETA: 1.24 min\n",
      "[852/900] Sample processed in 8.36s, ETA: 1.22 min\n",
      "[853/900] Sample processed in 2.47s, ETA: 1.20 min\n",
      "[854/900] Sample processed in 12.60s, ETA: 1.18 min\n",
      "[855/900] Sample processed in 7.32s, ETA: 1.16 min\n",
      "[856/900] Sample processed in 6.39s, ETA: 1.14 min\n",
      "[857/900] Sample processed in 10.09s, ETA: 1.12 min\n",
      "[858/900] Sample processed in 7.64s, ETA: 1.10 min\n",
      "[859/900] Sample processed in 9.26s, ETA: 1.08 min\n",
      "[860/900] Sample processed in 13.95s, ETA: 1.06 min\n",
      "[861/900] Sample processed in 6.12s, ETA: 1.04 min\n",
      "[862/900] Sample processed in 8.36s, ETA: 1.02 min\n",
      "[863/900] Sample processed in 6.91s, ETA: 1.00 min\n",
      "[864/900] Sample processed in 12.01s, ETA: 0.98 min\n",
      "[865/900] Sample processed in 4.95s, ETA: 0.95 min\n",
      "[866/900] Sample processed in 12.61s, ETA: 0.93 min\n",
      "[867/900] Sample processed in 5.20s, ETA: 0.91 min\n",
      "[868/900] Sample processed in 3.58s, ETA: 0.88 min\n",
      "[869/900] Sample processed in 6.88s, ETA: 0.86 min\n",
      "[870/900] Sample processed in 9.82s, ETA: 0.83 min\n",
      "[871/900] Sample processed in 8.02s, ETA: 0.81 min\n",
      "[872/900] Sample processed in 15.24s, ETA: 0.79 min\n",
      "[873/900] Sample processed in 11.51s, ETA: 0.76 min\n",
      "[874/900] Sample processed in 5.39s, ETA: 0.74 min\n",
      "[875/900] Sample processed in 3.01s, ETA: 0.71 min\n",
      "[876/900] Sample processed in 4.39s, ETA: 0.68 min\n",
      "[877/900] Sample processed in 1.36s, ETA: 0.65 min\n",
      "[878/900] Sample processed in 6.62s, ETA: 0.63 min\n",
      "[879/900] Sample processed in 15.22s, ETA: 0.60 min\n",
      "[880/900] Sample processed in 15.23s, ETA: 0.58 min\n",
      "[881/900] Sample processed in 9.82s, ETA: 0.56 min\n",
      "[882/900] Sample processed in 14.93s, ETA: 0.53 min\n",
      "[883/900] Sample processed in 4.25s, ETA: 0.50 min\n",
      "[884/900] Sample processed in 13.66s, ETA: 0.48 min\n",
      "[885/900] Sample processed in 6.66s, ETA: 0.45 min\n",
      "[886/900] Sample processed in 12.63s, ETA: 0.42 min\n",
      "[887/900] Sample processed in 5.77s, ETA: 0.39 min\n",
      "[888/900] Sample processed in 9.00s, ETA: 0.36 min\n",
      "[889/900] Sample processed in 5.14s, ETA: 0.33 min\n",
      "[890/900] Sample processed in 5.93s, ETA: 0.30 min\n",
      "[891/900] Sample processed in 15.20s, ETA: 0.28 min\n",
      "[892/900] Sample processed in 10.41s, ETA: 0.25 min\n",
      "[893/900] Sample processed in 13.33s, ETA: 0.22 min\n",
      "[894/900] Sample processed in 3.25s, ETA: 0.19 min\n",
      "[895/900] Sample processed in 7.72s, ETA: 0.16 min\n",
      "[896/900] Sample processed in 15.20s, ETA: 0.13 min\n",
      "[897/900] Sample processed in 11.44s, ETA: 0.09 min\n",
      "[898/900] Sample processed in 9.86s, ETA: 0.06 min\n",
      "[899/900] Sample processed in 9.07s, ETA: 0.03 min\n",
      "[900/900] Sample processed in 6.52s, ETA: 0.00 min\n",
      "\n",
      "All samples processed. Total time: 28.74 min\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "### ✅ Hugging Face에서 데이터 로드\n",
    "dataset_name = \"passionMan/test_dataset10\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")  # 'test' split 로드\n",
    "\n",
    "### ✅ JSONL 저장 함수 (평가 결과 저장용)\n",
    "def save_to_jsonl(file_path, data):\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f: \n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "### ✅ 모델 응답 생성 함수\n",
    "def generate_response(instruction_text, input_text, max_new_tokens=128):\n",
    "    try:\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        # ✅ 모델의 최대 입력 길이 가져오기 (보통 4096 또는 2048)\n",
    "        max_input_length = getattr(model.config, \"max_position_embeddings\", 4096)\n",
    "\n",
    "        # ✅ 입력 토큰 길이 확인\n",
    "        input_tokens = tokenizer(\n",
    "            alpaca_prompt.format(instruction_text, input_text, \"\"), \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        input_length = input_tokens['input_ids'].shape[1]\n",
    "\n",
    "        # 🔥 입력이 너무 길면 최대 입력 길이에 맞게 자름\n",
    "        if input_length > max_input_length:\n",
    "            print(f\"[WARNING] Truncating input from {input_length} to {max_input_length} tokens.\")\n",
    "            input_text = tokenizer.decode(input_tokens['input_ids'][0, :max_input_length], skip_special_tokens=True)\n",
    "\n",
    "        # ✅ 생성 수행 (max_new_tokens을 적용)\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(alpaca_prompt.format(instruction_text, input_text, \"\"), return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,  # ✅ 생성 길이 적용\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "        response_texts = [output.split(\"### Response:\\n\")[-1].strip() for output in decoded_outputs]\n",
    "        return response_texts[0].replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception in response generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ✅ 데이터 경로 설정 (결과 저장용)\n",
    "output_json_path = \"response/real_seed_IFD_rIFD14_10_percent.jsonl\"\n",
    "\n",
    "# ✅ Task별 데이터 그룹화 (각 태스크별 0~29번 샘플 선택)\n",
    "grouped_data = defaultdict(list)\n",
    "for item in dataset:\n",
    "    grouped_data[item[\"task\"]].append(item)\n",
    "\n",
    "# ✅ 성능 평가할 데이터 생성 (각 태스크별 30개만 추출)\n",
    "sampled_data = []\n",
    "for task, samples in grouped_data.items():\n",
    "    sampled_data.extend(samples[:100])  # 최대 30개 선택\n",
    "\n",
    "# ✅ 성능 평가 시작\n",
    "start_time = time.time()\n",
    "total_samples = len(sampled_data)\n",
    "\n",
    "for idx, item in enumerate(sampled_data):\n",
    "    sample_start_time = time.time()\n",
    "\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    instruction = item.get(\"instruction\", \"\")\n",
    "    task = item.get(\"task\", \"\").lower()\n",
    "\n",
    "    # ✅ 생성할 토큰 길이 설정 (생성 토큰 수 조절)\n",
    "    short_context_tasks = {\"qa1\", \"qa2\", \"qa3\", \"nli\", \"re\", \"re2\", \"ie\"}  # 생성 32\n",
    "    medium_context_tasks = {\"generation\"} # 생성 128\n",
    "    long_context_tasks = {\"summarization\", \"daily_diets\", \"alternative_diet\"}  # 생성 1024\n",
    "\n",
    "    if task in short_context_tasks:\n",
    "        max_new_tokens = 32  # ✅ 생성 길이 128\n",
    "    elif task in medium_context_tasks:\n",
    "        max_new_tokens = 512\n",
    "    elif task in long_context_tasks:\n",
    "        max_new_tokens = 512  # ✅ 생성 길이 1024\n",
    "    else:\n",
    "        max_new_tokens = 128  # 기본값\n",
    "\n",
    "    try:\n",
    "        model_output = generate_response(instruction, input_text, max_new_tokens)\n",
    "\n",
    "        if model_output is not None:\n",
    "            output_data = item.copy()\n",
    "            output_data.update({f\"model_output_{max_new_tokens}\": model_output})\n",
    "            save_to_jsonl(output_json_path, output_data)\n",
    "        else:\n",
    "            print(f\"[WARNING] Skipping sample {idx+1}/{total_samples} due to length limit or generation failure.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Skipping sample {idx+1}/{total_samples} due to unexpected error: {str(e)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_sample = elapsed_time / (idx + 1) \n",
    "    remaining_samples = total_samples - (idx + 1)\n",
    "    estimated_remaining_time = remaining_samples * avg_time_per_sample\n",
    "\n",
    "    print(f\"[{idx+1}/{total_samples}] Sample processed in {time.time() - sample_start_time:.2f}s, ETA: {estimated_remaining_time/60:.2f} min\")\n",
    "\n",
    "print(f\"\\nAll samples processed. Total time: {(time.time() - start_time)/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-26 17:53:33 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    model_name = \"/data/jaesung/llm_for_diabetes/src/trial7/main_table/llama_8b/outputs/last_seed/checkpoint-458\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Prepare a healthy daily meal plan for diebeic patien, including portion sizes in grams.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Breakfast: 1 cup of steel-cut oats with 1/2 cup of nonfat milk and 1 tablespoon of ground flaxseed. Lunch: 3 ounces of grilled chicken breast with 1 cup of steamed broccoli and 1/2 cup of brown rice. Snack: 1 small apple with 1 tablespoon of almond butter. Dinner: 4 ounces of grilled salmon with 1 cup of steamed asparagus and 1/2 cup of quinoa.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1000) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Can you recommend a daily diet recipe? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        # \"Prepare a healthy daily meal plan for a female aged 35 with 1400 kcal, including portion sizes in grams.\",\n",
    "        \"Prepare a healthy daily meal plan for diebeic patien, including portion sizes in grams.\",\n",
    "        \"\",\n",
    "        \"\", \n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes is a metabolic disorder that affects how the body uses food for energy. It occurs when the body cannot produce enough insulin or when the body cannot properly use the insulin it produces. Insulin is a hormone that helps the body use glucose, a type of sugar, for energy. People with diabetes must carefully manage their blood sugar levels to avoid complications such as heart disease, stroke, and nerve damage.\n",
      "\n",
      "A healthy diet for a diabetic patient should include plenty of vegetables, fruits, whole grains, lean proteins, and healthy fats. It should also limit processed foods, sugary drinks, and refined carbohydrates. A typical daily diet for a diabetic patient might look like this:\n",
      "\n",
      "Breakfast: oatmeal with fresh berries, a glass of milk, and a cup of green tea\n",
      "Lunch: grilled chicken breast with steamed vegetables, brown rice, and a glass of water\n",
      "Dinner: baked fish with roasted vegetables, quinoa, and a small glass of red wine\n",
      "Snacks: a handful of nuts, a piece of fruit, or a small serving of yogurt\n",
      "It's important to consult with a registered dietitian or healthcare provider to develop a personalized diet plan for a diabetic patient. They can provide guidance on portion sizes, macronutrient ratios, and specific foods to include or avoid based on individual needs and health goals.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(400) \n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Can you recommend a daily diet recipe for a diabetic patient? Please provide detailed information on breakfast, lunch, and dinner.\", \n",
    "        \"\",\n",
    "        \"\", \n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env_kernel",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
