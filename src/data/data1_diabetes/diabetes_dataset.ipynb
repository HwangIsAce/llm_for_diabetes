{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) raw dataset\n",
    "2) keywords based extraction\n",
    "3) train/test split\n",
    "4) sample a specific number of samples (high cardinality)\n",
    "5) paste the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8c35691f2347309e1cc7b08e9c4a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset load\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# qa datasets\n",
    "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "# medqa = load_dataset(\"bigbio/med_qa\")\n",
    "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "pubmedqa = load_dataset(\"bigbio/pubmed_qa\", trust_remote_code=True)\n",
    "# pubmedqa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
    "# bioasq = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")\n",
    "# mednli = load_dataset(\"bigbio/mednli\")\n",
    "\n",
    "# nli datasets\n",
    "# mednli = load_dataset(\"cnut1648/mnli_resampled_as_mednli\")\n",
    "bionli = load_dataset(\"clinicalnlplab/BioNLI_test\")\n",
    "\n",
    "# information extraction datasets\n",
    "# cie_mse = load_dataset(\"mitclinicalml/clinical-ie\", \"medication_status\", trust_remote_code=True) # medication status extraction\n",
    "# cie_ccr = load_dataset(\"mitclinicalml/clinical-ie\", \"coreference\", trust_remote_code=True) # clinical coreference resolution\n",
    "\n",
    "# medmentions = load_dataset(\"bigbio/medmentions\", trust_remote_code=True)\n",
    "# bc5cdr = load_dataset(\"bigbio/bc5cdr\", trust_remote_code=True)\n",
    "\n",
    "# chemdisgene = load_dataset(\"bigbio/chem_dis_gene\", trust_remote_code=True)\n",
    "# biored = load_dataset(\"bigbio/biored\", trust_remote_code=True)\n",
    "# biorelex = load_dataset(\"bigbio/biorelex\", trust_remote_code=True)\n",
    "biorel = load_dataset(\"DFKI-SLT/BioRel\")\n",
    "# biorex = load_dataset(\"bigbio/biorelex\", trust_remote_code=True)\n",
    "\n",
    "# generation task about clinical skills datasets\n",
    "# medichat = load_dataset(\"Mostafijur/medichat_conversation\", \"medichat_subset1\") # ~ medichat_subset15 # conv2note\n",
    "icliniq = load_dataset(\"lavita/ChatDoctor-iCliniq\")\n",
    "# mediqa = load_dataset(\"jonathankang/EN-MEDIQA\")\n",
    "pubmed = load_dataset(\"ccdv/pubmed-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "icliniq = icliniq['train'].train_test_split(test_size=0.3, seed=42)\n",
    "# pubmedqa = pubmedqa['train'].train_test_split(test_size=0.3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 10178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1273\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 182822\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 6150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 4183\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 11269\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 5544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 12806\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 6308\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "biorel\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'relation', 'h', 't'],\n",
      "        num_rows: 534277\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'relation', 'h', 't'],\n",
      "        num_rows: 114506\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'relation', 'h', 't'],\n",
      "        num_rows: 114565\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "icliniq\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'answer_icliniq', 'answer_chatgpt', 'answer_chatdoctor'],\n",
      "        num_rows: 5124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'answer_icliniq', 'answer_chatgpt', 'answer_chatdoctor'],\n",
      "        num_rows: 2197\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmed\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 119924\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6633\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6658\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"medqa\")\n",
    "print(medqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"medmcqa\")\n",
    "print(medmcqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"mednli\")\n",
    "# print(mednli)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"bionli\")\n",
    "print(bionli)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"cie_mse\")\n",
    "# print(cie_mse)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"cie_ccr\")\n",
    "# print(cie_ccr)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"medmentions\")\n",
    "# print(medmentions)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"bc5cdr\")\n",
    "# print(bc5cdr)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"chem-dis-gene\")\n",
    "# print(chemdisgene)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"biored\")\n",
    "# print(biored)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"biorelex\")\n",
    "# print(biorelex)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"biorel\")\n",
    "print(biorel)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"biorex\")\n",
    "# print(biorex)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"medichat\")\n",
    "# print(medichat)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"icliniq\")\n",
    "print(icliniq)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "# print(\"mediqa\")\n",
    "# print(mediqa)\n",
    "# print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmed\")\n",
    "print(pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa (train): 10178 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medqa (test): 1273 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (train): 182822 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (test): 6150 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (validation): 4183 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (train): 200000 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (validation): 11269 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (train): 11088 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (validation): 25612 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (test): 12616 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "biorel (train): 534277 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "biorel (validation): 114506 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "biorel (test): 114565 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "icliniq (train): 10248 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "icliniq (test): 4394 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (train): 119924 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (validation): 6633 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (test): 6658 rows processed and appended to all_data_filtered_by_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    " \n",
    "keywords = [\n",
    "    # 질환 관련\n",
    "    \"diabetes\", \"HbA1c\", \"blood sugar\", \n",
    "    \"glucose\", \"ketoacidosis\",\n",
    "    \n",
    "    # 약물 및 치료\n",
    "    \"insuline\",\n",
    "    \"metformin\", \"SGLT2 inhibitors\", \"GLP-1 receptor agonist\",\n",
    "\n",
    "    # 합병증\n",
    "    \"neuropathy\", \"retinopathy\", \"nephropathy\",\n",
    "]\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    if not text or not isinstance(text, str): \n",
    "        return False\n",
    "    return any(keyword in text.lower() for keyword in keywords)\n",
    "\n",
    "def process_dataset_split(dataset_name, split_name, dataset, text_columns, output_file, keywords=[\"diabetes\", \"insulin\"]):\n",
    "    rows = []\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset[i]\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                text = row[col]\n",
    "\n",
    "            elif col == \"abstract\" and \"passages\" in row:\n",
    "                if isinstance(row[\"passages\"], list):\n",
    "                    text = \" \".join([p[\"text\"] for p in row[\"passages\"] if p.get(\"type\") == \"abstract\"])\n",
    "                else:\n",
    "                    text = \"\"\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            is_related = 1 if contains_keywords(text, keywords) else 0\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"split_data\": split_name,\n",
    "                \"features\": row,  \n",
    "                \"input\": text,\n",
    "                \"output\": is_related\n",
    "            })\n",
    "\n",
    "    if rows:  \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "        print(f\"{dataset_name} ({split_name}): {len(rows)} rows processed and appended to {output_file}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} ({split_name}): No rows processed.\")\n",
    "\n",
    "datasets = {\n",
    "    \"medqa\": {\"splits\": medqa, \"columns\": [\"question\"]},\n",
    "    \"medmcqa\": {\"splits\": medmcqa, \"columns\": [\"question\"]},\n",
    "    \"pubmedqa\": {\"splits\": pubmedqa, \"columns\": [\"QUESTION\"]},\n",
    "    # \"mednli\": {\"splits\": mednli, \"columns\": [\"premise\", \"hypothesis\"]},\n",
    "    \"bionli\": {\"splits\": bionli, \"columns\": [\"query\", \"answer\"]},\n",
    "    # \"cie_mse\": {\"splits\": cie_mse, \"columns\": [\"snippet\"]},\n",
    "    # \"cie_ccr\": {\"splits\": cie_ccr, \"columns\": [\"snippet\"]},\n",
    "    # \"medmentions: {\"splits\": medmentions, \"columns\": [\"passages\"]},\n",
    "    # \"bc5cdr\": {\"splits\": bc5cdr, \"columns\": [\"passages\"]},\n",
    "    # \"chemdisgene\": {\"splits\": chemdisgene, \"columns\": [\"passages\"]},\n",
    "    # \"biored\": {\"splits\": biored, \"columns\": [\"passages\"]},\n",
    "    # \"biorelex\": {\"splits\": biorelex, \"columns\": [\"text\"]},\n",
    "    \"biorel\": {\"splits\": biorel, \"columns\": [\"text\"]},\n",
    "    # \"biorex\": {\"splits\": biorex, \"columns\": [\"text\"]},\n",
    "    # \"medichat\": {\"splits\": medichat, \"columns\": [\"conversation\"]},\n",
    "    \"icliniq\": {\"splits\": icliniq, \"columns\": [\"input\", \"answer_icliniq\"]},\n",
    "    \"pubmed\": {\"splits\": pubmed, \"columns\": [\"abstract\"]},\n",
    "}\n",
    "\n",
    "output_file = os.path.join(\"all_data_filtered_by_keywords.csv\")\n",
    "\n",
    "for dataset_name, details in datasets.items():\n",
    "    splits = details[\"splits\"]\n",
    "    text_columns = details[\"columns\"] \n",
    "\n",
    "    for split_name, split_data in splits.items():\n",
    "        process_dataset_split(dataset_name, split_name, split_data, text_columns, output_file, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/all_data_filtered_by_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filtered_by_keyword\n",
    "diabetes = all_df[all_df['output'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man presents to th...</td>\n",
       "      <td>A 68-year-old man presents to the emergency de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man comes to the p...</td>\n",
       "      <td>A 68-year-old man comes to the physician becau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset split_data                                           features  \\\n",
       "15   medqa      train  {'question': 'A 68-year-old man presents to th...   \n",
       "17   medqa      train  {'question': 'A 68-year-old man comes to the p...   \n",
       "\n",
       "                                                input  output  \n",
       "15  A 68-year-old man presents to the emergency de...       1  \n",
       "17  A 68-year-old man comes to the physician becau...       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset   split_data\n",
       "bionli    test           3906\n",
       "          train          3690\n",
       "          validation     8280\n",
       "biorel    test          15618\n",
       "          train         69864\n",
       "          validation    15396\n",
       "biorelex  train             1\n",
       "biorex    train             1\n",
       "icliniq   test            972\n",
       "          train          2238\n",
       "medmcqa   test            414\n",
       "          train         11952\n",
       "          validation      270\n",
       "medqa     test           1122\n",
       "          train          9564\n",
       "pubmed    test           3264\n",
       "          train         54714\n",
       "          validation     3222\n",
       "pubmedqa  train         21744\n",
       "          validation     1227\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def re_train_test_split_unique(dataframe, train_ratio=0.8):\n",
    "    new_rows = []\n",
    "\n",
    "    grouped = dataframe.groupby('dataset')\n",
    "\n",
    "    for dataset_name, group in grouped:\n",
    "        if dataset_name not in ['bionli', 'biorel', 'icliniq', 'medmcqa', 'medqa', 'pubmed', 'pubmedqa']:\n",
    "            continue\n",
    "\n",
    "        # 중복 제거\n",
    "        group = group.drop_duplicates(subset='features')\n",
    "\n",
    "        train_data, test_data = train_test_split(\n",
    "            group,\n",
    "            test_size=1-train_ratio,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_data = train_data.copy()\n",
    "        train_data['split_data'] = 'train'\n",
    "\n",
    "        test_data = test_data.copy()\n",
    "        test_data['split_data'] = 'test'\n",
    "\n",
    "        new_rows.append(train_data)\n",
    "        new_rows.append(test_data)\n",
    "\n",
    "    result_df = pd.concat(new_rows, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "diabetes2 = re_train_test_split_unique(diabetes, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset   split_data\n",
       "bionli    test            426\n",
       "          train          1701\n",
       "biorel    test           3359\n",
       "          train         13434\n",
       "icliniq   test             89\n",
       "          train           356\n",
       "medmcqa   test            422\n",
       "          train          1684\n",
       "medqa     test            357\n",
       "          train          1424\n",
       "pubmed    test           2039\n",
       "          train          8154\n",
       "pubmedqa  test           1532\n",
       "          train          6125\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes2.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def add_input_to_for_sampling(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # INPUT 값을 추출하여 새로운 컬럼에 추가\n",
    "    def extract_for_sampling(row):\n",
    "        if row['dataset'] == 'bionli':\n",
    "            features = ast.literal_eval(row['features']) \n",
    "            query = features.get('query', '') \n",
    "            input_text = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'biorel':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            text = features.get('text', None)\n",
    "            relation = features.get('relation', None)\n",
    "            return {'input': text, 'output': relation}\n",
    "        elif row['dataset'] == 'icliniq':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('input', None)\n",
    "            answer_iclinq = features.get('answer_iclinq', None)\n",
    "            return {'input': input_text, 'output': answer_iclinq}\n",
    "        elif row['dataset'] == 'medmcqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = \", \".join([\n",
    "                features.get('opa', ''),\n",
    "                features.get('opb', ''),\n",
    "                features.get('opc', ''),\n",
    "                features.get('opd', ''),\n",
    "            ]).strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'medqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmed':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('article', None)\n",
    "            abstract = features.get('abstract', None)\n",
    "            return {'input': input_text, 'output': abstract}\n",
    "        elif row['dataset'] == 'pubmedqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            question = features.get('QUESTION', None)\n",
    "            answer = features.get('LONG_ANSWER', None)\n",
    "            return {'input': question, 'output': answer}\n",
    "\n",
    "    dataframe['for_sampling'] = dataframe.apply(extract_for_sampling, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "diabetes3 = add_input_to_for_sampling(diabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI16787330', 'query': '\\nTASK: Ple...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] An increased oxidative stress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI22643846', 'query': '\\nTASK: Ple...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] Previously we demonstrated th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset split_data                                           features  \\\n",
       "0  bionli      train  {'id': 'BioNLI16787330', 'query': '\\nTASK: Ple...   \n",
       "1  bionli      train  {'id': 'BioNLI22643846', 'query': '\\nTASK: Ple...   \n",
       "\n",
       "                                               input  output  \\\n",
       "0  \\nTASK: Please classify the relationship betwe...       1   \n",
       "1  \\nTASK: Please classify the relationship betwe...       1   \n",
       "\n",
       "                                        for_sampling  \n",
       "0  {'input': '[PRE] An increased oxidative stress...  \n",
       "1  {'input': '[PRE] Previously we demonstrated th...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>biorel</td>\n",
       "      <td>train</td>\n",
       "      <td>{'text': 'hla antigens , complement allotypes ...</td>\n",
       "      <td>hla antigens , complement allotypes , insulin ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'hla antigens , complement allotypes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>biorel</td>\n",
       "      <td>train</td>\n",
       "      <td>{'text': \"glucosamine and n-acetylglucosamine ...</td>\n",
       "      <td>glucosamine and n-acetylglucosamine supported ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'glucosamine and n-acetylglucosamine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset split_data                                           features  \\\n",
       "2127  biorel      train  {'text': 'hla antigens , complement allotypes ...   \n",
       "2128  biorel      train  {'text': \"glucosamine and n-acetylglucosamine ...   \n",
       "\n",
       "                                                  input  output  \\\n",
       "2127  hla antigens , complement allotypes , insulin ...       1   \n",
       "2128  glucosamine and n-acetylglucosamine supported ...       1   \n",
       "\n",
       "                                           for_sampling  \n",
       "2127  {'input': 'hla antigens , complement allotypes...  \n",
       "2128  {'input': 'glucosamine and n-acetylglucosamine...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes3[diabetes3['dataset']=='biorel'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes3[diabetes3['dataset']=='biorel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41102"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "\n",
    "def filter_na_relations(dataframe):\n",
    "    def is_valid_relation(features_str):\n",
    "        try:\n",
    "            features_dict = ast.literal_eval(features_str)\n",
    "            return features_dict.get('relation') is not None and features_dict['relation'] != 'NA'\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    biorel_filtered = dataframe[dataframe['dataset'] == 'biorel']\n",
    "    biorel_filtered = biorel_filtered[biorel_filtered['features'].apply(is_valid_relation)]\n",
    "\n",
    "    others = dataframe[dataframe['dataset'] != 'biorel']\n",
    "\n",
    "    result = pd.concat([biorel_filtered, others], ignore_index=True)\n",
    "    return result\n",
    "\n",
    "diabetes3 = filter_na_relations(diabetes3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12054"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes3[diabetes3['dataset']=='biorel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36363"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'text': 'hla antigens , complement allotypes , insulin antibodies and thyrogastric autoantibodies were determined in 69 patients with type 1 ( insulin-dependent ) diabetes defined by a tendency to ketosis , non-obesity and insulin requirement within 2 years of diagnosis .', 'relation': 'may_be_treated_by', 'h': {'id': 'C0011854', 'name': 'insulin-dependent', 'pos': [134, 151]}, 't': {'id': 'C0021641', 'name': 'insulin', 'pos': [214, 221]}}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes3[diabetes3['dataset']=='biorel']['features'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36363"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jaesung/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: bionli (1701 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.86 seconds for 1701 rows\n",
      "Applying MMR for dataset: bionli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  14%|█▍        | 1/7 [00:04<00:24,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: biorel (9621 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/3 [00:01<?, ?it/s]\n",
      "Processing datasets:  14%|█▍        | 1/7 [00:05<00:33,  5.63s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 2.37 GiB is free. Process 2708991 has 23.23 GiB memory in use. Including non-PyTorch memory, this process has 13.72 GiB memory in use. Of the allocated memory 10.12 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 샘플링 실행\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m sampled_train \u001b[38;5;241m=\u001b[39m \u001b[43mmmr_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiabetes3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 결과 확인\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(sampled_train\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[18], line 64\u001b[0m, in \u001b[0;36mmmr_sampling\u001b[0;34m(dataframe, sampling_dict, embedding_model, batch_size, diversity, seed)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings for dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 64\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfor_sampling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding generation took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:393\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    391\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 393\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:540\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 2.37 GiB is free. Process 2708991 has 23.23 GiB memory in use. Including non-PyTorch memory, this process has 13.72 GiB memory in use. Of the allocated memory 10.12 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'train')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "# 샘플링 설정\n",
    "sampling_config = {\n",
    "    'bionli': 1000,\n",
    "    'biorel': 1000,\n",
    "    'icliniq': 300,\n",
    "    'medmcqa': 1000,\n",
    "    'medqa': 1000,\n",
    "    'pubmedqa': 1000,\n",
    "    'pubmed': 1000\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_train = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_train.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_train)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_train.to_csv(\"final_combined_train_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: bionli (426 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "Processing datasets:  14%|█▍        | 1/7 [00:00<00:03,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.44 seconds for 426 rows\n",
      "Applying MMR for dataset: bionli\n",
      "Generating embeddings for dataset: biorel (2433 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.52 seconds for 2433 rows\n",
      "Applying MMR for dataset: biorel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  29%|██▊       | 2/7 [00:02<00:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: icliniq (89 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.07 seconds for 89 rows\n",
      "Applying MMR for dataset: icliniq\n",
      "Generating embeddings for dataset: medmcqa (422 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Processing datasets:  57%|█████▋    | 4/7 [00:03<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.31 seconds for 422 rows\n",
      "Applying MMR for dataset: medmcqa\n",
      "Generating embeddings for dataset: medqa (357 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Processing datasets:  71%|███████▏  | 5/7 [00:03<00:01,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.27 seconds for 357 rows\n",
      "Applying MMR for dataset: medqa\n",
      "Generating embeddings for dataset: pubmedqa (1532 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.97 seconds for 1532 rows\n",
      "Applying MMR for dataset: pubmedqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  86%|████████▌ | 6/7 [00:04<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmed (2039 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 5.66 seconds for 2039 rows\n",
      "Applying MMR for dataset: pubmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 7/7 [00:10<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset split_data                                           features  \\\n",
      "0  bionli       test  {'id': 'BioNLI9284904', 'query': '\\nTASK: Plea...   \n",
      "1  bionli       test  {'id': 'BioNLI22820188', 'query': '\\nTASK: Ple...   \n",
      "2  bionli       test  {'id': 'BioNLI29104632', 'query': '\\nTASK: Ple...   \n",
      "3  bionli       test  {'id': 'BioNLI7793231', 'query': \"\\nTASK: Plea...   \n",
      "4  bionli       test  {'id': 'BioNLI12169771', 'query': '\\nTASK: Ple...   \n",
      "\n",
      "                                               input  output  \\\n",
      "0  \\nTASK: Please classify the relationship betwe...       1   \n",
      "1  \\nTASK: Please classify the relationship betwe...       1   \n",
      "2  \\nTASK: Please classify the relationship betwe...       1   \n",
      "3  \\nTASK: Please classify the relationship betwe...       1   \n",
      "4  \\nTASK: Please classify the relationship betwe...       1   \n",
      "\n",
      "                                        for_sampling  \n",
      "0  {'input': '[PRE] Because of a failure to detec...  \n",
      "1  {'input': '[PRE] Tumor necrosis factor-alpha (...  \n",
      "2  {'input': '[PRE] Uric acid nephropathy (UAN) i...  \n",
      "3  {'input': '[PRE] In an earlier report of our p...  \n",
      "4  {'input': '[PRE] Rat pituitary tumor cells (GC...  \n",
      "총 샘플링된 행 수: 1260\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'test')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "# 샘플링 설정\n",
    "sampling_config = {\n",
    "    'bionli': 200,\n",
    "    'biorel': 200,\n",
    "    'icliniq': 60,\n",
    "    'medmcqa': 200,\n",
    "    'medqa': 200,\n",
    "    'pubmedqa': 200,\n",
    "    'pubmed': 200\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_test = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_test.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_test)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_test.to_csv(\"final_combined_test_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_combined_train_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_train_sample.csv\")\n",
    "final_combined_test_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_test_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test 겹치는 feature 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거 후 겹치는 개수 확인\n",
    "train_unique = final_combined_train_sample.drop_duplicates(subset='features')\n",
    "test_unique = final_combined_test_sample.drop_duplicates(subset='features')\n",
    "\n",
    "# train/test 겹치는 개수 계산\n",
    "overlapping_count = len(pd.merge(train_unique, test_unique, on='features'))\n",
    "print(f\"Train/Test 겹치는 feature 개수: {overlapping_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_train_sample['task'] = None\n",
    "\n",
    "# bionli\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'bionli', 'task'] = 'nli' \n",
    "\n",
    "# biorel\n",
    "biorel_indices = final_combined_train_sample[final_combined_train_sample['dataset'] == 'biorel'].index\n",
    "biorel_splits = np.array_split(biorel_indices, 1)\n",
    "\n",
    "biorel_tasks = [\n",
    "    'ie_extract_relation',\n",
    "    # 'ie_classify_relation',\n",
    "    # 'ie_identify_relation_exist',\n",
    "    # 'ie_generate_relation_pair'\n",
    "]\n",
    "\n",
    "for split, task in zip(biorel_splits, biorel_tasks):\n",
    "    final_combined_train_sample.loc[split, 'task'] = task\n",
    "\n",
    "# icliniq\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'icliniq', 'task'] = 'generation'\n",
    "\n",
    "# medmcqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medmcqa', 'task'] = 'qa_subjective'\n",
    "\n",
    "# medqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medqa', 'task'] = 'qa_objective'\n",
    "\n",
    "# pubmedqa\n",
    "pubmedqa_indices = final_combined_train_sample[final_combined_train_sample['dataset'] == 'pubmedqa'].index\n",
    "pubmedqa_splits = np.array_split(pubmedqa_indices, 1)\n",
    "\n",
    "pubmedqa_tasks = [\n",
    "    # 'qa_mesh_tagging',\n",
    "    # 'qa_short_answer',\n",
    "    'qa_descriptive',\n",
    "    # 'qa_complex_answer'\n",
    "]\n",
    "\n",
    "for split, task in zip(pubmedqa_splits, pubmedqa_tasks):\n",
    "    final_combined_train_sample.loc[split, 'task'] = task\n",
    "\n",
    "# pubmed\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'pubmed', 'task'] = 'summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_test_sample['task'] = None\n",
    "\n",
    "# bionli\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'bionli', 'task'] = 'nli' \n",
    "\n",
    "# biorel\n",
    "biorel_indices = final_combined_test_sample[final_combined_test_sample['dataset'] == 'biorel'].index\n",
    "biorel_splits = np.array_split(biorel_indices, 1)\n",
    "\n",
    "biorel_tasks = [\n",
    "    'ie_extract_relation',\n",
    "    # 'ie_classify_relation',\n",
    "    # 'ie_identify_relation_exist',\n",
    "    # 'ie_generate_relation_pair'\n",
    "]\n",
    "\n",
    "for split, task in zip(biorel_splits, biorel_tasks):\n",
    "    final_combined_test_sample.loc[split, 'task'] = task\n",
    "\n",
    "# icliniq\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'icliniq', 'task'] = 'generation'\n",
    "\n",
    "# medmcqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medmcqa', 'task'] = 'qa_subjective'\n",
    "\n",
    "# medqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medqa', 'task'] = 'qa_objective'\n",
    "\n",
    "# pubmedqa\n",
    "pubmedqa_indices = final_combined_test_sample[final_combined_test_sample['dataset'] == 'pubmedqa'].index\n",
    "pubmedqa_splits = np.array_split(pubmedqa_indices, 1)\n",
    "\n",
    "pubmedqa_tasks = [\n",
    "    # 'qa_mesh_tagging',\n",
    "    # 'qa_short_answer',\n",
    "    'qa_descriptive',\n",
    "    # 'qa_complex_answer'\n",
    "]\n",
    "\n",
    "for split, task in zip(pubmedqa_splits, pubmedqa_tasks):\n",
    "    final_combined_test_sample.loc[split, 'task'] = task\n",
    "\n",
    "# pubmed\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'pubmed', 'task'] = 'summarization'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "instruction_mapping = {\n",
    "    'nli': [\n",
    "        \"Please classify the relationship between the given premise and hypothesis into one of the following labels: entailment, contradiction, or neutral. return only the label.\"\n",
    "        # \"Given a premise and a hypothesis, determine their relationship: entailment, contradiction, or neutral.\",\n",
    "        # \"Read the given premise and hypothesis. Decide if the hypothesis logically follows from the premise.\",\n",
    "        # \"Classify the relationship between the premise and the hypothesis into one of three categories: entailment, contradiction, or neutral.\",\n",
    "        # \"Analyze the relationship between the given premise and hypothesis. Categorize it as entailment, contradiction, or neutral.\",\n",
    "        # \"Does the premise logically support the hypothesis? Answer as entailment, contradiction, or neutral.\",\n",
    "        # \"Evaluate if the hypothesis can be inferred from the premise. Label it as entailment, contradiction, or neutral.\",\n",
    "        # \"Does the hypothesis contradict the premise or is it entailed by it? If neither, classify it as neutral.\",\n",
    "        # \"For the given premise and hypothesis, determine their logical relationship: entailment, contradiction, or neutral.\"\n",
    "    ],\n",
    "    'ie_extract_relation': [\n",
    "        \"Predict the relationship between the given entities in the given sentence. \"\n",
    "        # \"Extract the relationship described between the given entities in the text.\",\n",
    "        # \"Identify the type of relationship that connects the specified entities based on the provided text.\",\n",
    "        # \"Determine the specific relationship mentioned between the entities in the text.\",\n",
    "        # \"From the text, describe the relationship that exists between the provided entities.\"\n",
    "    ],\n",
    "    # 'ie_identify_relation_exist': [\n",
    "    #     \"Does the text describe any relationship between the specified entities? Answer Yes or No.\",\n",
    "    #     \"Based on the text, determine if there is a connection or relationship between the given entities. Answer Yes or No.\",\n",
    "    #     \"Identify whether a relationship exists between the specified entities in the text. Answer Yes or No.\",\n",
    "    #     \"From the text, confirm whether there is a described relationship connecting the given entities. Answer Yes or No.\"\n",
    "    # ],\n",
    "    # 'ie_classify_relation': [\n",
    "    #     \"Classify the type of relationship described between the specified entities in the text.\",\n",
    "    #     \"Identify the category that best describes the relationship between the entities based on the text.\",\n",
    "    #     \"Determine whether the relationship described in the text falls into predefined categories such as physical, causal, or regulatory.\",\n",
    "    #     \"Based on the text, categorize the relationship between the entities into its most relevant type.\"\n",
    "    # ],\n",
    "    # 'ie_generate_relation_pair': [\n",
    "    #     \"Generate all possible entity pairs from the text and identify their relationships.\",\n",
    "    #     \"Extract all entity pairs mentioned in the text along with the relationships that connect them.\",\n",
    "    #     \"Identify all entity pairs present in the text and specify their corresponding relationships if available.\",\n",
    "    #     \"From the text, list all pairs of entities and describe the relationships that exist between them.\"\n",
    "    # ],\n",
    "    'generation': [\n",
    "        # \"What are the next steps or follow-up actions suggested in the doctor’s response?\",\n",
    "        \"Generate a comprehensive medical response based on the patient's query.\",\n",
    "        # \"Provide a concise and actionable answer to the patient's question.\",\n",
    "        # \"What advice or recommendations does the doctor provide in response to the patient's concerns?\",\n",
    "        # \"What follow-up steps does the doctor suggest in their response?\"\n",
    "    ],\n",
    "    'qa_objective': [\n",
    "        \"Select the most appropriate answer for the given medical question from the provided options.\",\n",
    "        # \"Identify the correct answer to the medical question from the four given options.\",\n",
    "        # \"Pick the correct choice for the given medical question based on the provided options.\",\n",
    "        # \"Determine the correct option that best answers the given medical question.\",\n",
    "        # \"Choose the answer that accurately addresses the given medical query from the options provided.\",\n",
    "        # \"From the provided choices, select the option that correctly answers the medical question.\",\n",
    "        # \"Based on the medical question, select the most accurate answer from the given options.\",\n",
    "        # \"Identify the best answer for the given medical question from the options below.\",\n",
    "        # \"Select the correct answer that corresponds to the given medical question.\",\n",
    "        # \"Determine the appropriate choice for the medical question from the list of options.\"\n",
    "    ],\n",
    "    'qa_subjective': [\n",
    "        \"Select the most appropriate answer for the given medical question from the provided options.\"\n",
    "    ],\n",
    "    # 'qa_mesh_tagging': [\n",
    "    #     \"Identify all MeSH terms mentioned in the context and list them.\",\n",
    "    #     \"Extract relevant MeSH terms from the provided text that describe the main topics.\",\n",
    "    #     \"Tag the context with appropriate MeSH terms that represent its primary themes.\",\n",
    "    #     \"Using the context, identify the MeSH terms that best summarize its content.\",\n",
    "    #     \"Highlight the MeSH terms in the text that are crucial for understanding the main subject.\",\n",
    "    #     \"Using the given context, tag the text with relevant MeSH terms that describe its focus.\",\n",
    "    #     \"From the context, extract key MeSH terms that align with the primary topics discussed.\",\n",
    "    #     \"Analyze the text and identify all MeSH terms that pertain to the discussed topics.\"\n",
    "    # ],\n",
    "    # 'qa_complex_answer': [\n",
    "    #     \"Predict the correct answer (Yes, No, Maybe) for the given question and provide a detailed explanation based on the context.\",\n",
    "    #     \"Determine whether the answer is Yes, No, or Maybe and explain your choice using evidence from the context.\",\n",
    "    #     \"Using the provided context, predict the correct answer and write a detailed response to justify it.\",\n",
    "    #     \"Select the correct decision (Yes, No, Maybe) for the given question and elaborate on the reasoning.\",\n",
    "    #     \"Provide both the correct answer and a comprehensive explanation to the given question using the context.\",\n",
    "    #     \"Answer the question with Yes, No, or Maybe, and support your decision with details from the context.\",\n",
    "    #     \"Generate a Yes, No, or Maybe answer for the question and add a long-form response explaining why.\",\n",
    "    #     \"Decide the answer to the medical question (Yes, No, Maybe) and provide a detailed justification using the given context.\",\n",
    "    #     \"Using the context, predict the answer (Yes, No, Maybe) and generate a detailed explanation for the question.\",\n",
    "    #     \"Write the correct decision (Yes, No, Maybe) for the question and support it with relevant findings from the context.\"\n",
    "    # ],\n",
    "    # 'qa_short_answer': [\n",
    "    #     \"Choose the correct answer (Yes, No, or Maybe) for the given question based on the provided context.\",\n",
    "    #     \"Determine the correct decision (Yes, No, or Maybe) for the medical question using the given context.\",\n",
    "    #     \"From the provided information, select whether the answer to the question is Yes, No, or Maybe.\",\n",
    "    #     \"Analyze the context and identify whether the question should be answered with Yes, No, or Maybe.\",\n",
    "    #     \"Based on the given data, decide if the answer to the question is Yes, No, or Maybe.\"\n",
    "    # ],\n",
    "    'qa_descriptive': [\n",
    "        \"Generate a detailed answer to the question using the provided context.\",\n",
    "        # \"Create a comprehensive explanation for the question based on the given information.\",\n",
    "        # \"Write a long and detailed response to the question using the context provided.\",\n",
    "        # \"Using the context, provide an in-depth answer to the question with relevant details.\"\n",
    "    ],\n",
    "    'summarization': [\n",
    "        # \"Summarize the given article into a concise abstract that highlights the key findings and conclusions.\",\n",
    "        # \"Generate a brief and coherent abstract from the provided article text.\",\n",
    "        \"Write a summary of the article that captures the main ideas and significant details.\",\n",
    "        # \"Condense the given article into a clear and concise abstract that represents its core content.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_correct_instruction(row):\n",
    "    task = row['task']\n",
    "    if task in instruction_mapping:\n",
    "        return random.choice(instruction_mapping[task])\n",
    "    return \"Instruction not found\"\n",
    "\n",
    "final_combined_train_sample['instruction'] = final_combined_train_sample.apply(assign_correct_instruction, axis=1)\n",
    "final_combined_test_sample['instruction'] = final_combined_test_sample.apply(assign_correct_instruction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def add_input_to_for_sampling(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # INPUT 값을 추출하여 새로운 컬럼에 추가\n",
    "    def extract_for_sampling(row):\n",
    "        if row['dataset'] == 'bionli':\n",
    "            features = ast.literal_eval(row['features']) \n",
    "            query = features.get('query', '') \n",
    "            input_text = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'biorel':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            text = features.get('text', None)\n",
    "            relation = features.get('relation', None)\n",
    "            return {'input': text, 'output': relation}\n",
    "        elif row['dataset'] == 'icliniq':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('input', None)\n",
    "            answer_iclinq = features.get('answer_iclinq', None)\n",
    "            return {'input': input_text, 'output': answer_iclinq}\n",
    "        elif row['dataset'] == 'medmcqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = \", \".join([\n",
    "                features.get('opa', ''),\n",
    "                features.get('opb', ''),\n",
    "                features.get('opc', ''),\n",
    "                features.get('opd', ''),\n",
    "            ]).strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'medqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmed':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('article', None)\n",
    "            abstract = features.get('abstract', None)\n",
    "            return {'input': input_text, 'output': abstract}\n",
    "        elif row['dataset'] == 'pubmedqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            question = features.get('QUESTION', None)\n",
    "            answer = features.get('LONG_ANSWER', None)\n",
    "            return {'input': question, 'output': answer}\n",
    "\n",
    "    dataframe['for_sampling'] = dataframe.apply(extract_for_sampling, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "diabetes3 = add_input_to_for_sampling(diabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2002,\n",
       " 'icliniq',\n",
       " 'train',\n",
       " \"{'input': 'Hello doctor,I experienced difficulty in retracting the foreskin and pulling it down over the penis head (mild phimosis). With difficulty, I manage in 10 minutes. Do this fornightly to clean the penis. I wish to avoid circumcision and understand the application of Lignocaine gel will help. Should I apply it on penis gland or head or on top of the foreskin?', 'answer_icliniq': 'Hi. I can understand your concern about your symptoms. The application is a local anesthetic. Wherever you have pain when you retract foreskin (mostly skin will have pain). You can apply Lignocaine. You cannot apply Lignocaine inside of your foreskin since you have phimosis. So the external application is only possible. Considering your age, kindly rule our other causes of phimosis. Keep blood sugar under control and take antibiotics and apply the local moisturizing gel.  Take care.', 'answer_chatgpt': 'It is important to consult with a healthcare professional before attempting to treat phimosis on your own. Lignocaine gel can help with pain and discomfort during retraction, but it should only be used under the guidance of a healthcare professional. Applying the gel on the penis gland or head will provide more direct and concentrated pain relief, but again, it is important to seek medical advice before attempting any treatment. In some cases, if the phimosis is severe, circumcision may be necessary to prevent further complications.', 'answer_chatdoctor': 'Hi, Welcome to Chat Doctor forum. I am sorry that you have developed this problem. The lignocain is used to anesthetize the area before surgery. You can apply it locally on your penis head. It will not be effective if applied on the foreskin. It will cause burning sensation and you may feel uncomfortable. It has no side effects and does not cause any harm. Please try this and see the results. If after doing this for some time, you still find difficulty then you should consult a surgeon and get yourself operated. This is a simple procedure and usually done under local anesthesia. Wishing you good health. Thank you.'}\",\n",
       " 'Hi. I can understand your concern about your symptoms. The application is a local anesthetic. Wherever you have pain when you retract foreskin (mostly skin will have pain). You can apply Lignocaine. You cannot apply Lignocaine inside of your foreskin since you have phimosis. So the external application is only possible. Considering your age, kindly rule our other causes of phimosis. Keep blood sugar under control and take antibiotics and apply the local moisturizing gel.  Take care.',\n",
       " 1,\n",
       " \"{'input': 'Hello doctor,I experienced difficulty in retracting the foreskin and pulling it down over the penis head (mild phimosis). With difficulty, I manage in 10 minutes. Do this fornightly to clean the penis. I wish to avoid circumcision and understand the application of Lignocaine gel will help. Should I apply it on penis gland or head or on top of the foreskin?', 'output': None}\",\n",
       " 'generation',\n",
       " \"Generate a comprehensive medical response based on the patient's query.\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample[final_combined_train_sample['task']=='generation'].iloc[2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_features(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}\n",
    "        \n",
    "def generate_input_output(row):\n",
    "    input_value = \"\"; output_value = \"\"\n",
    "\n",
    "    task = row['task']\n",
    "    features = parse_features(row['features'])\n",
    "\n",
    "    if row['dataset']=='bionli' and task=='nli':\n",
    "        query = features.get('query', '') \n",
    "        input_value = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "        output_value = features.get('answer', None)\n",
    "    elif row['dataset']=='biorel' and task=='ie_extract_relation':\n",
    "        input_value = features.get('text', None)\n",
    "        output_value = features.get('relation', None)\n",
    "    # elif row['dataset']=='biorel' and task=='ie_identify_relation_exist':\n",
    "    #     input_value = features.get('text', '')\n",
    "    #     output_value = 'YES' if features.get('relation') else 'NO'\n",
    "    # elif row['dataset']=='biorel' and task=='ie_classify_relation':\n",
    "    #     input_value = features.get('text', '')\n",
    "    #     output_value = features.get('relation', 'unknown')\n",
    "    # elif row['dataset']=='biorel' and task=='ie_generate_relation_pair':\n",
    "    #     input_value = features.get('text', '')\n",
    "    #     h_name = features.get('h', {}).get('name', 'unknown')\n",
    "    #     t_name = features.get('t', {}).get('name', 'unknown')\n",
    "    #     relation = features.get('relation', 'unknown')\n",
    "    #     output_value = f\"Entity Pair: ('{h_name}', '{t_name}'), Relationship: '{relation}'\"\n",
    "    elif row['dataset']=='icliniq' and task=='generation':\n",
    "        input_value = features.get('input', None)\n",
    "        output_value = features.get('answer_icliniq', None)\n",
    "    elif row['dataset']=='medqa' and task=='qa_objective':\n",
    "        question = features.get('question', None)\n",
    "        options = features.get('options', None)\n",
    "        answer = features.get('answer', None)\n",
    "        answer_idx = features.get('answer_idx', None)\n",
    "\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}) {answer}\"\n",
    "        )\n",
    "    elif row['dataset']=='medmcqa' and task=='qa_subjective': \n",
    "        question = features.get('question', None)\n",
    "        options = {\n",
    "            'A': features.get('opa', '').strip(),\n",
    "            'B': features.get('opb', '').strip(),\n",
    "            'C': features.get('opc', '').strip(),\n",
    "            'D': features.get('opd', '').strip(),\n",
    "        }\n",
    "        answer_num = features.get('cop', None)\n",
    "        answer_idx = 'A' if answer_num == 0 else 'B' if answer_num == 1 else 'C' if answer_num == 2 else 'D'\n",
    "        answer = features.get('opa', '').strip() if answer_num == 0 else features.get('opb', '').strip() if answer_num == 1 else features.get('opc', '').strip() if answer_num == 2 else features.get('opd', '').strip()\n",
    "\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}) {answer}\"\n",
    "        )\n",
    "\n",
    "    # elif row['dataset']=='pubmedqa' and task=='qa_mesh_tagging':\n",
    "    #     features = ast.literal_eval(row['features'])  \n",
    "    #     question = features.get('question', '').strip()\n",
    "    #     context = \" \".join(features.get('context', {}).get('contexts', [])).strip()\n",
    "    #     meshes = features.get('context', {}).get('meshes', [])\n",
    "        \n",
    "    #     input_value = (\n",
    "    #         f\"Question: {question} \"\n",
    "    #         f\"Context: {context}\"\n",
    "    #     )\n",
    "    #     output_value = f\"MeSH Tags: {', '.join(meshes)}\"\n",
    "    # elif row['dataset']=='pubmedqa' and task=='qa_complex_answer':\n",
    "    #     features = ast.literal_eval(row['features'])  \n",
    "    #     question = features.get('question', '').strip()\n",
    "    #     context = \" \".join(features.get('context', {}).get('contexts', [])).strip()\n",
    "    #     short_answer = features.get('final_decision', '').strip()  \n",
    "    #     long_answer = features.get('long_answer', '').strip()  \n",
    "\n",
    "    #     input_value = (\n",
    "    #         f\"Question: {question} \"\n",
    "    #         f\"Context: {context}\"\n",
    "    #     )\n",
    "    #     \n",
    "    #     output_value = (\n",
    "    #         f\"Short Answer: {short_answer} \"\n",
    "    #         f\"Long Answer: {long_answer}\"\n",
    "    #     )\n",
    "    # elif row['dataset']=='pubmedqa' and task=='qa_short_answer':\n",
    "    #     features = ast.literal_eval(row['features'])  \n",
    "    #     question = features.get('question', '').strip()\n",
    "    #     context = \" \".join(features.get('context', {}).get('contexts', [])).strip()\n",
    "    #     short_answer = features.get('long_answer', '').strip()  \n",
    "\n",
    "    #     input_value = (\n",
    "    #         f\"Question: {question} \"\n",
    "    #         f\"Context: {context}\"\n",
    "    #     )\n",
    "    #     output_value = f\"Answer: {short_answer}\"\n",
    "    elif row['dataset']=='pubmedqa' and task=='qa_descriptive':\n",
    "        question = features.get('QUESTION', '').strip()\n",
    "        context = \" \".join(features.get('CONTEXTS'))\n",
    "        long_answer = features.get('LONG_ANSWER', '').strip() \n",
    "\n",
    "        input_value = (\n",
    "            f\"Question: {question} \"\n",
    "            f\"Context: {context}\"\n",
    "        )\n",
    "        output_value = f\"{long_answer}\"\n",
    "\n",
    "    elif row['dataset']=='pubmed' and task=='summarization':\n",
    "        article = features.get('article', '').strip()  \n",
    "        abstract = features.get('abstract', '').strip() \n",
    "        \n",
    "        input_value = f\"{article}\"\n",
    "        output_value = abstract\n",
    "    return input_value, output_value\n",
    "\n",
    "final_combined_train_sample[['input', 'output']] = final_combined_train_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")\n",
    "\n",
    "final_combined_test_sample[['input', 'output']] = final_combined_test_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bionli' 'biorel' 'icliniq' 'medmcqa' 'medqa' 'pubmedqa' 'pubmed']\n",
      "\n",
      "['nli' 'ie_extract_relation' 'generation' 'qa_subjective' 'qa_objective'\n",
      " 'qa_descriptive' 'summarization']\n"
     ]
    }
   ],
   "source": [
    "print(final_combined_train_sample['dataset'].unique())\n",
    "print('')\n",
    "print(final_combined_train_sample['task'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_df = pd.concat([final_combined_train_sample, final_combined_test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final_combined_train_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/train_instruction_dataset.json\", orient=\"columns\", indent=4)\n",
    "final_combined_test_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/test_instruction_dataset.json\", orient=\"columns\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meal_kernel",
   "language": "python",
   "name": "meal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
