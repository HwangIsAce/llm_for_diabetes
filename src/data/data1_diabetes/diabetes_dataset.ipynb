{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) raw dataset\n",
    "2) keywords based extraction\n",
    "3) train/test split\n",
    "4) sample a specific number of samples (high cardinality)\n",
    "5) paste the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafc42741fbf4035a3ac21f4deb66e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "## dataset load\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# qa datasets\n",
    "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "pubmedqa = load_dataset(\"bigbio/pubmed_qa\", trust_remote_code=True)\n",
    "# mfc = load_dataset(\"veggiebird/medical-flashcards\")\n",
    "\n",
    "# nli datasets\n",
    "bionli = load_dataset(\"clinicalnlplab/BioNLI_test\")\n",
    "\n",
    "# information extraction datasets\n",
    "ddi = load_dataset(\"YufeiHFUT/DDI2013\")\n",
    "chemdner =load_dataset(\"kjappelbaum/chemnlp-chemdner\")\n",
    "# medmentions = load_dataset(\"bigbio/medmentions\", trust_remote_code=True) train-108, test-37\n",
    "# biorel = load_dataset(\"DFKI-SLT/BioRel\")\n",
    "# phee = load_dataset(\"sarus-tech/phee\")\n",
    "\n",
    "# generation datasets\n",
    "healthcaremagic = load_dataset(\"lighteval/med_dialog\", \"healthcaremagic\")\n",
    "icliniq = load_dataset(\"lighteval/med_dialog\", \"icliniq\")\n",
    "pubmed = load_dataset(\"akemiH/NoteChat\")\n",
    "# pubmed = load_dataset(\"Gaborandi/diabetes_mellitus_type2_pubmed_abstracts\")\n",
    "# pubmed = load_dataset(\"ccdv/pubmed-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['data', 'conversation'],\n",
       "        num_rows: 165600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['data', 'conversation'],\n",
       "        num_rows: 20701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['data', 'conversation'],\n",
       "        num_rows: 20700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 train과 test로 80:20 비율로 분할\n",
    "split_dataset = pubmed[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# test 데이터를 다시 50:50으로 나눠 test와 validation 생성\n",
    "test_valid = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# 최종 DatasetDict 구성\n",
    "pubmed = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "    \"validation\": test_valid[\"train\"]\n",
    "})\n",
    "\n",
    "\n",
    "pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 15552\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 1944\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 1944\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tmp = chemdner['train'].to_pandas()\n",
    "tmp = tmp.drop(columns=[\"split\"])\n",
    "\n",
    "train_df, temp_df = train_test_split(tmp, test_size=0.2, shuffle=True, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "chemdner = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\" : test_dataset,\n",
    "})\n",
    "\n",
    "chemdner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 205973\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 25746\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 25750\n",
       " })}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "# meddialog = {\n",
    "#     \"train\": concatenate_datasets([icliniq['train']]),\n",
    "#     \"validation\": concatenate_datasets([icliniq['validation']]),\n",
    "#     \"test\": concatenate_datasets([icliniq['test']]),\n",
    "# }\n",
    "\n",
    "meddialog = {\n",
    "    \"train\": concatenate_datasets([healthcaremagic['train'], icliniq['train']]),\n",
    "    \"validation\": concatenate_datasets([healthcaremagic['validation'], icliniq['validation']]),\n",
    "    \"test\": concatenate_datasets([healthcaremagic['test'], icliniq['test']]),\n",
    "}\n",
    "\n",
    "meddialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 10178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1273\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 182822\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 6150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 4183\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 11269\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 5544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 12806\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 6308\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DDI\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 18779\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 5761\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 7244\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 15552\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 1944\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 1944\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "meddialog\n",
      "{'train': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 205973\n",
      "}), 'validation': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 25746\n",
      "}), 'test': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 25750\n",
      "})}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmed\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['data', 'conversation'],\n",
      "        num_rows: 165600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['data', 'conversation'],\n",
      "        num_rows: 20701\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['data', 'conversation'],\n",
      "        num_rows: 20700\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"medqa\")\n",
    "print(medqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"medmcqa\")\n",
    "print(medmcqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"bionli\")\n",
    "print(bionli)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"DDI\")\n",
    "print(ddi)\n",
    "print('------------------------------------------------------------------------------'* 2)\n",
    "print(\"chemdner\")\n",
    "print(chemdner)\n",
    "print('------------------------------------------------------------------------------'* 2)\n",
    "print(\"meddialog\")\n",
    "print(meddialog)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmed\")\n",
    "print(pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa (train): 20356 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medqa (test): 2546 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (train): 182822 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (test): 6150 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (validation): 4183 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (train): 600000 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (validation): 33807 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (train): 11088 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (validation): 25612 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (test): 12616 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (train): 18779 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (test): 5761 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (valid): 7244 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (train): 15552 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (validation): 1944 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (test): 1944 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (train): 411943 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (validation): 51492 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (test): 51500 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (train): 331200 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (test): 41402 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (validation): 41400 rows processed and appended to all_data_filtered_by_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "\n",
    "keywords = [keyword.lower() for keyword in [\n",
    "    \"Diabetes\", \"Diabetic\", \n",
    "    \"Diabetes Mellitus\", \"Diabetes Mellitus, Experimental\", \"Diabetes Mellitus, Type 1\",\n",
    "    \"Wolfram Syndrome\", \"Diabetes Mellitus, Type 2\", \"Diabetes Mellitus, Lipoatrophic\",\n",
    "    \"Diabetes, Gestational\", \"Donohue Syndrome\", \"Latent Autoimmune Diabetes in Adults\",\n",
    "    \"Prediabetic State\", \"Diabetes Complications\", \"Diabesity\", \"Diabetic Angiopathies\",\n",
    "    \"Diabetic Cardiomyopathies\", \"Diabetic Coma\", \"Diabetic Ketoacidosis\",\n",
    "    \"Diabetic Nephropathies\", \"Diabetic Neuropathies\", \"Fetal Macrosomia\"\n",
    "]]\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    if not text or not isinstance(text, str): \n",
    "        return False\n",
    "    text_lower = text.lower() \n",
    "    return any(keyword.lower() in text_lower for keyword in keywords)\n",
    "\n",
    "def process_dataset_split(dataset_name, split_name, dataset, text_columns, output_file, keywords=[\"diabetes\", \"insulin\"]):\n",
    "    rows = []\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset[i]\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                text = row[col]\n",
    "\n",
    "            elif col == \"abstract\" and \"passages\" in row:\n",
    "                if isinstance(row[\"passages\"], list):\n",
    "                    text = \" \".join([p[\"text\"] for p in row[\"passages\"] if p.get(\"type\") == \"abstract\"])\n",
    "                else:\n",
    "                    text = \"\"\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            is_related = 1 if contains_keywords(text, keywords) else 0\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"split_data\": split_name,\n",
    "                \"features\": row,  \n",
    "                \"input\": text,\n",
    "                \"output\": is_related\n",
    "            })\n",
    "\n",
    "    if rows:  \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "        print(f\"{dataset_name} ({split_name}): {len(rows)} rows processed and appended to {output_file}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} ({split_name}): No rows processed.\")\n",
    "\n",
    "datasets = {\n",
    "    \"medqa\": {\"splits\": medqa, \"columns\": [\"question\", \"answer\"]},\n",
    "    \"medmcqa\": {\"splits\": medmcqa, \"columns\": [\"question\"]},\n",
    "    \"pubmedqa\": {\"splits\": pubmedqa, \"columns\": [\"QUESTION\", \"CONTEXTS\", \"LONG_ANSWER\"]},\n",
    "    \"bionli\": {\"splits\": bionli, \"columns\": [\"query\", \"answer\"]},\n",
    "    \"ddi\": {\"splits\": ddi, \"columns\": [\"text\"]},\n",
    "    \"chemdner\": {\"splits\": chemdner, \"columns\": ['text']},\n",
    "    \"meddialog\": {\"splits\": meddialog, \"columns\": [\"tgt\", \"src\"]},\n",
    "    \"pubmed\": {\"splits\": pubmed, \"columns\": [\"conversation\", \"data\"]},\n",
    "}\n",
    "\n",
    "\n",
    "output_file = os.path.join(\"all_data_filtered_by_keywords.csv\")\n",
    "\n",
    "for dataset_name, details in datasets.items():\n",
    "    splits = details[\"splits\"]\n",
    "    text_columns = details[\"columns\"] \n",
    "\n",
    "    for split_name, split_data in splits.items():\n",
    "        process_dataset_split(dataset_name, split_name, split_data, text_columns, output_file, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/all_data_filtered_by_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filtered_by_keyword\n",
    "diabetes = all_df[all_df['output'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man presents to th...</td>\n",
       "      <td>A 68-year-old man presents to the emergency de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man comes to the p...</td>\n",
       "      <td>A 68-year-old man comes to the physician becau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset split_data                                           features  \\\n",
       "30   medqa      train  {'question': 'A 68-year-old man presents to th...   \n",
       "34   medqa      train  {'question': 'A 68-year-old man comes to the p...   \n",
       "\n",
       "                                                input  output  \n",
       "30  A 68-year-old man presents to the emergency de...       1  \n",
       "34  A 68-year-old man comes to the physician becau...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset    split_data\n",
       "bionli     test            224\n",
       "           train           221\n",
       "           validation      537\n",
       "chemdner   test             57\n",
       "           train           522\n",
       "           validation       71\n",
       "ddi        test             61\n",
       "           train           289\n",
       "           valid            45\n",
       "meddialog  test           1152\n",
       "           train          9229\n",
       "           validation     1064\n",
       "medmcqa    test             33\n",
       "           train          1375\n",
       "           validation       34\n",
       "medqa      test            124\n",
       "           train          1069\n",
       "pubmed     test           4074\n",
       "           train         30832\n",
       "           validation     3868\n",
       "pubmedqa   train         12152\n",
       "           validation      680\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def re_train_test_split_unique(dataframe, train_ratio=0.8):\n",
    "    new_rows = []\n",
    "\n",
    "    grouped = dataframe.groupby('dataset')\n",
    "\n",
    "    for dataset_name, group in grouped:\n",
    "        if dataset_name not in ['medqa', 'medmcqa', 'pubmedqa', 'bionli', 'chemdner', 'ddi', 'meddialog', 'pubmed']:\n",
    "            continue\n",
    "\n",
    "        # 중복 제거\n",
    "        group = group.drop_duplicates(subset='features')\n",
    "\n",
    "        train_data, test_data = train_test_split(\n",
    "            group,\n",
    "            test_size=1-train_ratio,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_data = train_data.copy()\n",
    "        train_data['split_data'] = 'train'\n",
    "\n",
    "        test_data = test_data.copy()\n",
    "        test_data['split_data'] = 'test'\n",
    "\n",
    "        new_rows.append(train_data)\n",
    "        new_rows.append(test_data)\n",
    "\n",
    "    result_df = pd.concat(new_rows, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "diabetes2 = re_train_test_split_unique(diabetes, train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset    split_data\n",
       "bionli     test             76\n",
       "           train           683\n",
       "chemdner   test             65\n",
       "           train           585\n",
       "ddi        test             40\n",
       "           train           355\n",
       "meddialog  test            999\n",
       "           train          8990\n",
       "medmcqa    test            145\n",
       "           train          1297\n",
       "medqa      test            119\n",
       "           train          1065\n",
       "pubmed     test           1703\n",
       "           train         15325\n",
       "pubmedqa   test            811\n",
       "           train          7297\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes2.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def add_input_to_for_sampling(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # INPUT 값을 추출하여 새로운 컬럼에 추가\n",
    "    def extract_for_sampling(row):\n",
    "        if row['dataset'] == 'medqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'medmcqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = \", \".join([\n",
    "                features.get('opa', ''),\n",
    "                features.get('opb', ''),\n",
    "                features.get('opc', ''),\n",
    "                features.get('opd', ''),\n",
    "            ]).strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmedqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            question = features.get('QUESTION', None)\n",
    "            answer = features.get('LONG_ANSWER', None)\n",
    "            return {'input': question, 'output': answer}\n",
    "        elif row['dataset'] == 'bionli':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            query = features.get('query', '') \n",
    "            input_text = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'chemdner':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('text', None)\n",
    "            answer = features.get('entities', None)\n",
    "            if isinstance(answer, list) and len(answer) > 0:\n",
    "                answer = answer[0]\n",
    "            else:\n",
    "                answer = ''\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'ddi':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('conversations', [])[0].get('value', None).split(\"INPUT:\")[1].split(\"OUTPUT:\")[0].strip()\n",
    "            answer = features.get('conversations', [])[1].get('value', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'meddialog':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('src', None).split(\"Doctor:\")[0].replace(\"Patient:\", \"\").strip()\n",
    "            answer = features.get('src', None).split(\"Doctor:\")[1].strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmed':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('conversation', None)\n",
    "            abstract = features.get('data', None)\n",
    "            return {'input': input_text, 'output': abstract}\n",
    "\n",
    "    dataframe['for_sampling'] = dataframe.apply(extract_for_sampling, axis=1)\n",
    "\n",
    "    # chemdner 의 \"\" 비율 줄이기\n",
    "    chemdner_df = dataframe[dataframe['dataset'] == 'chemdner'].copy()\n",
    "    empty_outputs = chemdner_df[chemdner_df['for_sampling'].apply(lambda x: x['output'] == '')]\n",
    "    non_empty_outputs = chemdner_df[chemdner_df['for_sampling'].apply(lambda x: x['output'] != '')]\n",
    "    target_empty_size = int(len(non_empty_outputs) * 0.05 / 0.95) \n",
    "    sampled_empty_outputs = empty_outputs.sample(n=min(target_empty_size, len(empty_outputs)), random_state=42)\n",
    "    balanced_chemdner_df = pd.concat([non_empty_outputs, sampled_empty_outputs])\n",
    "    other_datasets = dataframe[dataframe['dataset'] != 'chemdner']\n",
    "    final_dataframe = pd.concat([other_datasets, balanced_chemdner_df])\n",
    "    \n",
    "    return final_dataframe\n",
    "\n",
    "diabetes3 = add_input_to_for_sampling(diabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI28408138', 'query': \"\\nTASK: Ple...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] Since 1875, controversy has e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI19419299', 'query': '\\nTASK: Ple...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] TNF-alpha is a major etiologi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset split_data                                           features  \\\n",
       "0  bionli      train  {'id': 'BioNLI28408138', 'query': \"\\nTASK: Ple...   \n",
       "1  bionli      train  {'id': 'BioNLI19419299', 'query': '\\nTASK: Ple...   \n",
       "\n",
       "                                               input  output  \\\n",
       "0  \\nTASK: Please classify the relationship betwe...       1   \n",
       "1  \\nTASK: Please classify the relationship betwe...       1   \n",
       "\n",
       "                                        for_sampling  \n",
       "0  {'input': '[PRE] Since 1875, controversy has e...  \n",
       "1  {'input': '[PRE] TNF-alpha is a major etiologi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jaesung/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medqa (1065 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.53 seconds for 1065 rows\n",
      "Applying MMR for dataset: medqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  12%|█▎        | 1/8 [00:02<00:15,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medmcqa (1297 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.87 seconds for 1297 rows\n",
      "Applying MMR for dataset: medmcqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  25%|██▌       | 2/8 [00:03<00:11,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmedqa (7297 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 4.23 seconds for 7297 rows\n",
      "Applying MMR for dataset: pubmedqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  38%|███▊      | 3/8 [00:12<00:25,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: bionli (683 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.64 seconds for 683 rows\n",
      "Applying MMR for dataset: bionli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  50%|█████     | 4/8 [00:14<00:14,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: chemdner (416 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.33 seconds for 416 rows\n",
      "Applying MMR for dataset: chemdner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  62%|██████▎   | 5/8 [00:14<00:07,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: ddi (355 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.19 seconds for 355 rows\n",
      "Applying MMR for dataset: ddi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing datasets:  75%|███████▌  | 6/8 [00:15<00:03,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmed (15325 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:13<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 16.98 seconds for 15325 rows\n",
      "Applying MMR for dataset: pubmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  88%|████████▊ | 7/8 [00:42<00:10, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: meddialog (8990 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 6.84 seconds for 8990 rows\n",
      "Applying MMR for dataset: meddialog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 8/8 [00:55<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset split_data                                           features  \\\n",
      "0   medqa      train  {'question': 'A 52-year-old man presents to th...   \n",
      "1   medqa      train  {'question': 'A 72-year-old woman with type 2 ...   \n",
      "2   medqa      train  {'question': \"A pediatrician is investigating ...   \n",
      "3   medqa      train  {'question': 'You are conducting a systematic ...   \n",
      "4   medqa      train  {'question': 'A 50-year-old woman with a histo...   \n",
      "\n",
      "                                               input  output  \\\n",
      "0  A 52-year-old man presents to the emergency de...       1   \n",
      "1  A 72-year-old woman with type 2 diabetes melli...       1   \n",
      "2  A pediatrician is investigating determinants o...       1   \n",
      "3  You are conducting a systematic review on the ...       1   \n",
      "4  A 50-year-old woman with a history of schizoph...       1   \n",
      "\n",
      "                                        for_sampling  \n",
      "0  {'input': 'A 52-year-old man presents to the e...  \n",
      "1  {'input': 'A 72-year-old woman with type 2 dia...  \n",
      "2  {'input': 'A pediatrician is investigating det...  \n",
      "3  {'input': 'You are conducting a systematic rev...  \n",
      "4  {'input': 'A 50-year-old woman with a history ...  \n",
      "총 샘플링된 행 수: 3716\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'train')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "# 샘플링 설정\n",
    "sampling_config = {\n",
    "    'medqa': 500,\n",
    "    'medmcqa': 500,\n",
    "    'pubmedqa': 500, # 1000\n",
    "    'bionli': 500, # 1500\n",
    "    'chemdner': 500,\n",
    "    'ddi': 300,\n",
    "    'pubmed': 500,\n",
    "    'meddialog': 500, \n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_train = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_train.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_train)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_train.to_csv(\"final_combined_train_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medqa (119 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Processing datasets:  12%|█▎        | 1/8 [00:00<00:01,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.14 seconds for 119 rows\n",
      "Applying MMR for dataset: medqa\n",
      "Generating embeddings for dataset: medmcqa (145 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.15 seconds for 145 rows\n",
      "Applying MMR for dataset: medmcqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  25%|██▌       | 2/8 [00:00<00:01,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmedqa (811 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "Processing datasets:  38%|███▊      | 3/8 [00:01<00:02,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.65 seconds for 811 rows\n",
      "Applying MMR for dataset: pubmedqa\n",
      "Generating embeddings for dataset: bionli (76 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.09 seconds for 76 rows\n",
      "Applying MMR for dataset: bionli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  50%|█████     | 4/8 [00:01<00:01,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: chemdner (45 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.06 seconds for 45 rows\n",
      "Applying MMR for dataset: chemdner\n",
      "Generating embeddings for dataset: ddi (40 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.01it/s]\n",
      "Processing datasets:  75%|███████▌  | 6/8 [00:01<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.03 seconds for 40 rows\n",
      "Applying MMR for dataset: ddi\n",
      "Generating embeddings for dataset: meddialog (999 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processing datasets:  88%|████████▊ | 7/8 [00:02<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.80 seconds for 999 rows\n",
      "Applying MMR for dataset: meddialog\n",
      "Generating embeddings for dataset: pubmed (1703 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.94 seconds for 1703 rows\n",
      "Applying MMR for dataset: pubmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 8/8 [00:04<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset split_data                                           features  \\\n",
      "0   medqa       test  {'question': 'A 60-year-old woman presents to ...   \n",
      "1   medqa       test  {'question': 'A 72-year-old male presents to a...   \n",
      "2   medqa       test  {'question': 'A 65-year-old female patient wit...   \n",
      "3   medqa       test  {'question': 'A randomized controlled trial wa...   \n",
      "4   medqa       test  {'question': 'A 62-year-old male presents to h...   \n",
      "\n",
      "                                               input  output  \\\n",
      "0  A 60-year-old woman presents to the emergency ...       1   \n",
      "1  A 72-year-old male presents to a cardiac surge...       1   \n",
      "2  A 65-year-old female patient with a past medic...       1   \n",
      "3  A randomized controlled trial was initiated to...       1   \n",
      "4  A 62-year-old male presents to his primary car...       1   \n",
      "\n",
      "                                        for_sampling  \n",
      "0  {'input': 'A 60-year-old woman presents to the...  \n",
      "1  {'input': 'A 72-year-old male presents to a ca...  \n",
      "2  {'input': 'A 65-year-old female patient with a...  \n",
      "3  {'input': 'A randomized controlled trial was i...  \n",
      "4  {'input': 'A 62-year-old male presents to his ...  \n",
      "총 샘플링된 행 수: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'test')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "sampling_config = {\n",
    "    'medqa': 100,\n",
    "    'medmcqa': 100,\n",
    "    'pubmedqa': 100, # 150 \n",
    "    'bionli': 76, # 225\n",
    "    'chemdner': 65,\n",
    "    'ddi': 40,\n",
    "    'meddialog': 100, \n",
    "    'pubmed': 100,\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_test = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_test.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_test)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_test.to_csv(\"final_combined_test_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_combined_train_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_train_sample.csv\")\n",
    "final_combined_test_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_test_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "medqa        500\n",
       "medmcqa      500\n",
       "pubmedqa     500\n",
       "bionli       500\n",
       "pubmed       500\n",
       "meddialog    500\n",
       "chemdner     416\n",
       "ddi          300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test 겹치는 feature 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거 후 겹치는 개수 확인\n",
    "train_unique = final_combined_train_sample.drop_duplicates(subset='features')\n",
    "test_unique = final_combined_test_sample.drop_duplicates(subset='features')\n",
    "\n",
    "# train/test 겹치는 개수 계산\n",
    "overlapping_count = len(pd.merge(train_unique, test_unique, on='features'))\n",
    "print(f\"Train/Test 겹치는 feature 개수: {overlapping_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_train_sample['task'] = None\n",
    "# medqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medqa', 'task'] = 'qa1'\n",
    "# medmcqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medmcqa', 'task'] = 'qa2'\n",
    "# pubmedqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'pubmedqa', 'task'] = 'qa3'\n",
    "# bionli\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'bionli', 'task'] = 'nli'\n",
    "# chemdner\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'chemdner', 'task'] = 'ie'\n",
    "# ddi\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'ddi', 'task'] = 're'\n",
    "# meddialog\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'meddialog', 'task'] = 'generation'\n",
    "# pubmed\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'pubmed', 'task'] = 'summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_test_sample['task'] = None\n",
    "# medqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medqa', 'task'] = 'qa1'\n",
    "# medmcqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medmcqa', 'task'] = 'qa2'\n",
    "# pubmedqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'pubmedqa', 'task'] = 'qa3'\n",
    "# bionli\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'bionli', 'task'] = 'nli'\n",
    "# chemdner\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'chemdner', 'task'] = 'ie'\n",
    "# ddi\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'ddi', 'task'] = 're'\n",
    "# meddialog\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'meddialog', 'task'] = 'generation'\n",
    "# pubmed\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'pubmed', 'task'] = 'summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "def parse_features(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}\n",
    "\n",
    "    \n",
    "def generate_input_output(row):\n",
    "    instruction_value = \"\"; input_value = \"\"; output_value = \"\"\n",
    "\n",
    "    task = row['task']\n",
    "    features = parse_features(row['features'])\n",
    "\n",
    "\n",
    "    if row['dataset']=='medqa' and task=='qa1':\n",
    "        question = features.get('question', None)\n",
    "        options = features.get('options', None)\n",
    "        answer = features.get('answer', None)\n",
    "        answer_idx = features.get('answer_idx', None)\n",
    "        instruction_value =  \"You are given a multiple-choice medical question with four possible answers. Your task is to identify and select the single most appropriate answer. based on your medical knowledge. Provide your response as a single letter: A, B, C, or D.\"\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}\"\n",
    "        )\n",
    "    elif row['dataset']=='medmcqa' and task=='qa2': \n",
    "        question = features.get('question', None)\n",
    "        options = {\n",
    "            'A': features.get('opa', '').strip(),\n",
    "            'B': features.get('opb', '').strip(),\n",
    "            'C': features.get('opc', '').strip(),\n",
    "            'D': features.get('opd', '').strip(),\n",
    "        }\n",
    "        answer_num = features.get('cop', None)\n",
    "        answer_idx = 'A' if answer_num == 0 else 'B' if answer_num == 1 else 'C' if answer_num == 2 else 'D'\n",
    "        answer = features.get('opa', '').strip() if answer_num == 0 else features.get('opb', '').strip() if answer_num == 1 else features.get('opc', '').strip() if answer_num == 2 else features.get('opd', '').strip()\n",
    "        instruction_value =  \"You are given a multiple-choice medical question with four possible answers. Your task is to identify and select the single most appropriate answer. based on your medical knowledge. Provide your response as a single letter: A, B, C, or D.\"\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}\"\n",
    "        )\n",
    "    elif row['dataset']=='pubmedqa' and task=='qa3':\n",
    "        question = features.get('QUESTION', '').strip()\n",
    "        context = \" \".join(features.get('CONTEXTS'))\n",
    "        # answer = features.get('LONG_ANSWER', '').strip() \n",
    "        answer = features.get('final_decision', '').strip()\n",
    "        instruction_value = \"Choose the correct anser (Yes, No, or Maybe) for the given question based on the proviced context.\"\n",
    "        input_value = (\n",
    "            f\"Question: {question} \"\n",
    "            f\"Context: {context}\"\n",
    "        )\n",
    "        output_value = f\"{answer}\"\n",
    "    elif row['dataset']=='bionli' and task=='nli':\n",
    "        query = features.get('query', '') \n",
    "        instruction_value = \"Analyze the logical relationship between the given premise and hypothesis. Your task is to classify this relationship into one of the following categories: Entailment: Select this when the hypothesis logically follows from the premise. In other words, if the premise is true, the hypothesis must also be true Contradiction: Select this when the hypothesis directly contradicts the premise. This means that if the premise is true, the hypothesis must be false. Please choose only one option: 'entailment' or 'contradiction'.\"\n",
    "\n",
    "        input_value = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "        output_value = features.get('answer', None)\n",
    "    \n",
    "    elif row['dataset']=='chemdner' and task=='ie':\n",
    "        instruction_value = \"Extract chemical entities from the given text, ensuring they belong to one of these categories: Chemical Compounds (e.g., glucose, ATP, cholesterol), Drugs & Pharmaceuticals (e.g., metformin, aspirin, paclitaxel), Metabolites & Bioactive Molecules (e.g., serotonin, nitric oxide, kynurenine) ,Vitamins & Minerals (e.g., vitamin D, zinc, calcium), Toxic & Environmental Chemicals (e.g., arsenic, BPA, dioxin). Guidelines: Extract only named entities from these categories. Exclude general chemical properties (e.g., acidity, solubility) and chemical processes (e.g., oxidation, hydrolysis). Keep entity names as they appear in the text. Return only the extracted names.\"\n",
    "\n",
    "        input_value = features.get('text', None)\n",
    "        answer = features.get('entities', None)\n",
    "        if isinstance(answer, list) and len(answer) > 0:\n",
    "            output_value = ', '.join(answer)\n",
    "        else:\n",
    "            output_value = ''\n",
    "    elif row['dataset']== 'ddi' and task=='re':\n",
    "        instruction_value = \"Analyze the sentence with two drugs labeled as @DRUG_A$ and @DRUG_B$. Extract the interaction between @DRUG_A$ and @DRUG_B$ from the input sentence by selecting only one of the following options: 'DDI-effect', 'DDI-mechanism', 'DDI-advise', 'DDI-false', and 'DDI-int'. 'DDI-effect': Choose this when the interaction describes an effect or a pharmacodynamic mechanism. 'DDI-mechanism': Choose this for interactions explained by pharmacokinetic mechanisms. 'DDI-advise': Choose this when the sentence provides a recommendation or advice about the drug interaction. 'DDI-false': Choose this if there is no actual drug-drug interaction in the sentence. 'DDI-int': Choose this when a drug-drug interaction is mentioned without additional detail.\"\n",
    "        input_value = features.get('conversations', [])[0].get('value', None).split(\"INPUT:\")[1].split(\"OUTPUT:\")[0].strip()\n",
    "        output_value = features.get('conversations', [])[1].get('value', None)\n",
    "    elif row['dataset']=='meddialog' and task=='generation':\n",
    "        # instruction_value = features.get('tgt', None)\n",
    "        instruction_value = \"As a medical AI assistant, generate a professional and medically accurate response to the given patient query. Ensure that your response aligns with medical guidelines and provides clear, practical advice. Avoid assumptions and base your answer solely on the provided information.\"\n",
    "        input_value = features.get('src', None).split(\"Doctor:\")[0].replace(\"Patient:\", \"\").strip()\n",
    "        output_value = features.get('src', None).split(\"Doctor:\")[1].strip()\n",
    "    elif row['dataset']=='pubmed' and task=='summarization':\n",
    "        instruction_value = \"Summarize the following doctor-patient conversation into a structured clinical note.\"\n",
    "        article = (features.get('conversation') or '').strip()  \n",
    "        abstract = (features.get('data')or '').strip() \n",
    "        input_value = f\"{article}\"\n",
    "        output_value = abstract\n",
    "    return instruction_value, input_value, output_value\n",
    "\n",
    "final_combined_train_sample[['instruction', 'input', 'output']] = final_combined_train_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")\n",
    "\n",
    "final_combined_test_sample[['instruction', 'input', 'output']] = final_combined_test_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "yes    461\n",
       "no      39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample[final_combined_train_sample['task']=='qa3']['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task           dataset  \n",
       "generation     meddialog    500\n",
       "nli            bionli       500\n",
       "qa1            medqa        500\n",
       "qa2            medmcqa      500\n",
       "qa3            pubmedqa     500\n",
       "summarization  pubmed       500\n",
       "ie             chemdner     416\n",
       "re             ddi          300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample[['task', 'dataset']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 균형 조정을 위한 함수 정의\n",
    "# def balance_classes(df, task_name, target_ratio=3):\n",
    "#     \"\"\"\n",
    "#     특정 task에서 'yes'와 'no'의 비율을 target_ratio:1로 맞춤\n",
    "#     \"\"\"\n",
    "#     # 해당 task만 필터링\n",
    "#     task_df = df[df['task'] == task_name]\n",
    "#     other_tasks_df = df[df['task'] != task_name]  # 다른 task 데이터는 그대로 유지\n",
    "\n",
    "#     # 'yes'와 'no' 분리\n",
    "#     yes_df = task_df[task_df['output'] == 'yes']\n",
    "#     no_df = task_df[task_df['output'] == 'no']\n",
    "\n",
    "#     # 목표 yes 샘플 개수 설정 (no의 target_ratio 배)\n",
    "#     target_yes_count = len(no_df) * target_ratio\n",
    "\n",
    "#     # 'yes' 데이터 downsampling\n",
    "#     yes_sampled = yes_df.sample(n=target_yes_count, random_state=42)\n",
    "\n",
    "#     # 균형을 맞춘 task 데이터셋 생성\n",
    "#     balanced_task_df = pd.concat([yes_sampled, no_df])\n",
    "\n",
    "#     # 다른 task 데이터와 다시 합쳐서 반환\n",
    "#     balanced_df = pd.concat([balanced_task_df, other_tasks_df])\n",
    "\n",
    "#     return balanced_df\n",
    "\n",
    "# # Train 데이터 균형 조정 (qa_objective_3만 수정)\n",
    "# final_combined_train_sample = balance_classes(final_combined_train_sample, 'qa_objective_3')\n",
    "\n",
    "# # Test 데이터 균형 조정 (qa_objective_3만 수정)\n",
    "# final_combined_test_sample = balance_classes(final_combined_test_sample, 'qa_objective_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_combined_train_sample[final_combined_train_sample['task']=='qa_objective_3']['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_combined_train_sample[['task','dataset']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # IE 데이터 필터링 (Train & Test)\n",
    "# ie_train_df = final_combined_train_sample[final_combined_train_sample['task'] == 'ie'].copy()\n",
    "# ie_test_df = final_combined_test_sample[final_combined_test_sample['task'] == 'ie'].copy()\n",
    "\n",
    "# # Instruction 증강 함수\n",
    "# def augment_instruction(instruction, task):\n",
    "#     \"\"\"\n",
    "#     IE 태스크에 대해 다양한 형태의 Instruction을 생성하여 증강하는 함수.\n",
    "#     \"\"\"\n",
    "#     instruction_variations = {\n",
    "#         \"ie\": [\n",
    "#             \"Extract all chemical entities mentioned in the given text. The extracted entities must fall into one of the following categories: Chemical Compounds: Substances with a defined molecular structure, including organic and inorganic compounds. Examples: glucose, cholesterol, ATP, quercetin. Drugs & Pharmaceuticals: Medications and bioactive compounds used for treatment, diagnosis, or prevention of diseases. Examples: metformin, aspirin, atorvastatin, warfarin, paclitaxel. Metabolites & Bioactive Molecules: Naturally occurring molecules that play a role in biological processes. Examples: serotonin, cortisol, nitric oxide (NO), superoxide, kynurenine. Vitamins & Minerals: Essential nutrients required for biological functions. Examples: vitamin D, vitamin C, zinc, calcium, magnesium. Toxic & Environmental Chemicals: Pollutants and hazardous chemicals found in the environment. Examples: arsenic, polychlorinated biphenyls (PCB), dioxin (TCDD), bisphenol A (BPA). Exclude general chemical properties or reaction mechanisms. Maintain the original entity names without abbreviation or alteration.\",\n",
    "\n",
    "#             \"Identify all chemical substances present in the given text, ensuring they belong to one of the predefined categories: Chemical Compounds: Defined molecular substances like glucose, cholesterol, and ATP. Drugs & Pharmaceuticals: Medicinal substances such as aspirin, atorvastatin, and paclitaxel. Metabolites & Bioactive Molecules: Naturally occurring biological compounds, including serotonin and nitric oxide. Vitamins & Minerals: Essential nutrients like vitamin D, zinc, and calcium. Toxic & Environmental Chemicals: Pollutants such as arsenic, dioxin, and bisphenol A. Exclude general chemical descriptions and processes. Extract only explicitly named entities.\",\n",
    "\n",
    "#             \"Detect and list all named chemical entities in the text, categorizing them into one of the following: Chemical Compounds: Molecularly defined substances (e.g., ATP, quercetin). Drugs & Pharmaceuticals: Medications and active compounds (e.g., metformin, warfarin). Metabolites & Bioactive Molecules: Biological molecules affecting physiological functions (e.g., cortisol, superoxide). Vitamins & Minerals: Essential nutrients required for health (e.g., vitamin C, magnesium). Toxic & Environmental Chemicals: Hazardous environmental substances (e.g., PCB, BPA, dioxin). Ensure that extracted entities match these categories without modification. Do not extract chemical properties or reaction descriptions.\",\n",
    "\n",
    "#             \"Extract chemical entities explicitly named in the text, making sure they fit into one of the specified categories: Chemical Compounds: Organic and inorganic molecules like ATP and glucose. Drugs & Pharmaceuticals: Medicinal and bioactive compounds such as aspirin and atorvastatin. Metabolites & Bioactive Molecules: Naturally occurring biochemical substances like nitric oxide and serotonin. Vitamins & Minerals: Essential micronutrients, including vitamin D, calcium, and zinc. Toxic & Environmental Chemicals: Harmful environmental compounds such as dioxin and bisphenol A. Exclude general chemical properties or reaction mechanisms. Maintain the original entity names without abbreviation or alteration.\"\n",
    "#         ]\n",
    "\n",
    "#     }\n",
    "#     return random.choice(instruction_variations.get(task, [instruction]))  # 기본적으로 원래 instruction 포함\n",
    "\n",
    "# # 데이터 증강 함수 (IE는 5배 증강, 나머지 컬럼 유지)\n",
    "# def augment_ie_data_preserve(df):\n",
    "#     augmented_data = []\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         task = row['task']\n",
    "\n",
    "#         # IE만 5배 증강\n",
    "#         augmentation_factor = 4 if task == 'ie' else 1\n",
    "\n",
    "#         for _ in range(augmentation_factor):\n",
    "#             new_row = row.copy()  # 원본 행 복사하여 모든 컬럼 유지\n",
    "#             new_row['instruction'] = augment_instruction(row['instruction'], task)  # Instruction만 변경\n",
    "#             augmented_data.append(new_row)\n",
    "\n",
    "#     return pd.DataFrame(augmented_data)\n",
    "\n",
    "# # IE 데이터 증강 수행 (Train & Test)\n",
    "# ie_train_augmented_df = augment_ie_data_preserve(ie_train_df)\n",
    "# ie_test_augmented_df = augment_ie_data_preserve(ie_test_df)\n",
    "\n",
    "# # 원본 데이터와 결합하여 최종 증강된 데이터 생성 (Train & Test)\n",
    "# final_combined_train_sample = pd.concat([final_combined_train_sample, ie_train_augmented_df], ignore_index=True)\n",
    "# final_combined_test_sample = pd.concat([final_combined_test_sample, ie_test_augmented_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task           dataset  \n",
       "generation     meddialog    500\n",
       "nli            bionli       500\n",
       "qa1            medqa        500\n",
       "qa2            medmcqa      500\n",
       "qa3            pubmedqa     500\n",
       "summarization  pubmed       500\n",
       "ie             chemdner     416\n",
       "re             ddi          300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample[['task','dataset']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_combined_train_sample, final_combined_test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_train_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/train_instruction_dataset.json\", orient=\"columns\", indent=4)\n",
    "final_combined_test_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/test_instruction_dataset.json\", orient=\"columns\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 52-year-old man presents to th...</td>\n",
       "      <td>A 52-year-old man presents to the emergency de...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'input': 'A 52-year-old man presents to the e...</td>\n",
       "      <td>qa1</td>\n",
       "      <td>You are given a multiple-choice medical questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 72-year-old woman with type 2 ...</td>\n",
       "      <td>A 72-year-old woman with type 2 diabetes melli...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'input': 'A 72-year-old woman with type 2 dia...</td>\n",
       "      <td>qa1</td>\n",
       "      <td>You are given a multiple-choice medical questi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 dataset split_data  \\\n",
       "0           0   medqa      train   \n",
       "1           1   medqa      train   \n",
       "\n",
       "                                            features  \\\n",
       "0  {'question': 'A 52-year-old man presents to th...   \n",
       "1  {'question': 'A 72-year-old woman with type 2 ...   \n",
       "\n",
       "                                               input output  \\\n",
       "0  A 52-year-old man presents to the emergency de...      A   \n",
       "1  A 72-year-old woman with type 2 diabetes melli...      C   \n",
       "\n",
       "                                        for_sampling task  \\\n",
       "0  {'input': 'A 52-year-old man presents to the e...  qa1   \n",
       "1  {'input': 'A 72-year-old woman with type 2 dia...  qa1   \n",
       "\n",
       "                                         instruction  \n",
       "0  You are given a multiple-choice medical questi...  \n",
       "1  You are given a multiple-choice medical questi...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_gpu_kernel",
   "language": "python",
   "name": "faiss_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
