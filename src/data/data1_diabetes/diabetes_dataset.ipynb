{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) raw dataset\n",
    "2) keywords based extraction\n",
    "3) train/test split\n",
    "4) sample a specific number of samples (high cardinality)\n",
    "5) paste the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jaesung/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/huggingface_hub/_login.py:340\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwidgets\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logout, notebook_login\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# logout()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     36\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/huggingface_hub/_login.py:343\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `notebook_login` function can only be used in a notebook (Jupyter or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_session \u001b[38;5;129;01mand\u001b[39;00m get_token() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser is already logged in.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import logout, notebook_login\n",
    "# logout()\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset load\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# qa datasets\n",
    "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
    "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "pubmedqa = load_dataset(\"bigbio/pubmed_qa\", trust_remote_code=True)\n",
    "# mfc = load_dataset(\"veggiebird/medical-flashcards\")\n",
    "\n",
    "# nli datasets\n",
    "bionli = load_dataset(\"clinicalnlplab/BioNLI_test\")\n",
    "\n",
    "# information extraction datasets\n",
    "ddi = load_dataset(\"YufeiHFUT/DDI2013\")\n",
    "chemdner =load_dataset(\"kjappelbaum/chemnlp-chemdner\")\n",
    "# medmentions = load_dataset(\"bigbio/medmentions\", trust_remote_code=True) train-108, test-37\n",
    "# biorel = load_dataset(\"DFKI-SLT/BioRel\")\n",
    "# phee = load_dataset(\"sarus-tech/phee\")\n",
    "\n",
    "# generation datasets\n",
    "healthcaremagic = load_dataset(\"lighteval/med_dialog\", \"healthcaremagic\")\n",
    "icliniq = load_dataset(\"lighteval/med_dialog\", \"icliniq\")\n",
    "pubmed = load_dataset(\"ccdv/pubmed-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 15552\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 1944\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['entities', 'text', '__index_level_0__'],\n",
       "        num_rows: 1944\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tmp = chemdner['train'].to_pandas()\n",
    "tmp = tmp.drop(columns=[\"split\"])\n",
    "\n",
    "train_df, temp_df = train_test_split(tmp, test_size=0.2, shuffle=True, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "chemdner = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\" : test_dataset,\n",
    "})\n",
    "\n",
    "chemdner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 205973\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 25746\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['tgt', 'src', 'id'],\n",
       "     num_rows: 25750\n",
       " })}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "meddialog = {\n",
    "    \"train\": concatenate_datasets([healthcaremagic['train'], icliniq['train']]),\n",
    "    \"validation\": concatenate_datasets([healthcaremagic['validation'], icliniq['validation']]),\n",
    "    \"test\": concatenate_datasets([healthcaremagic['test'], icliniq['test']]),\n",
    "}\n",
    "\n",
    "meddialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 10178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
      "        num_rows: 1273\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "medmcqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 182822\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 6150\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
      "        num_rows: 4183\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmedqa\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
      "        num_rows: 11269\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "bionli\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 5544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 12806\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'query', 'answer', 'choices', 'gold'],\n",
      "        num_rows: 6308\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DDI\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 18779\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 5761\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'conversations', 'text'],\n",
      "        num_rows: 7244\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "chemdner\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 15552\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 1944\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['entities', 'text', '__index_level_0__'],\n",
      "        num_rows: 1944\n",
      "    })\n",
      "})\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "meddialog\n",
      "{'train': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 205973\n",
      "}), 'validation': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 25746\n",
      "}), 'test': Dataset({\n",
      "    features: ['tgt', 'src', 'id'],\n",
      "    num_rows: 25750\n",
      "})}\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "pubmed\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 119924\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6633\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract'],\n",
      "        num_rows: 6658\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"medqa\")\n",
    "print(medqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"medmcqa\")\n",
    "print(medmcqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmedqa\")\n",
    "print(pubmedqa)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"bionli\")\n",
    "print(bionli)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"DDI\")\n",
    "print(ddi)\n",
    "print('------------------------------------------------------------------------------'* 2)\n",
    "print(\"chemdner\")\n",
    "print(chemdner)\n",
    "print('------------------------------------------------------------------------------'* 2)\n",
    "print(\"meddialog\")\n",
    "print(meddialog)\n",
    "print('------------------------------------------------------------------------------'* 2) \n",
    "print(\"pubmed\")\n",
    "print(pubmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa (train): 20356 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medqa (test): 2546 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (train): 182822 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (test): 6150 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "medmcqa (validation): 4183 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (train): 600000 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmedqa (validation): 33807 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (train): 11088 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (validation): 25612 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "bionli (test): 12616 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (train): 18779 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (test): 5761 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "ddi (valid): 7244 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (train): 15552 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (validation): 1944 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "chemdner (test): 1944 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (train): 411943 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (validation): 51492 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "meddialog (test): 51500 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (train): 119924 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (validation): 6633 rows processed and appended to all_data_filtered_by_keywords.csv\n",
      "pubmed (test): 6658 rows processed and appended to all_data_filtered_by_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "\n",
    "keywords = [keyword.lower() for keyword in [\n",
    "    \"Diabetes\",\n",
    "    \"Diabetes Mellitus\", \"Diabetes Mellitus, Experimental\", \"Diabetes Mellitus, Type 1\",\n",
    "    \"Wolfram Syndrome\", \"Diabetes Mellitus, Type 2\", \"Diabetes Mellitus, Lipoatrophic\",\n",
    "    \"Diabetes, Gestational\", \"Donohue Syndrome\", \"Latent Autoimmune Diabetes in Adults\",\n",
    "    \"Prediabetic State\", \"Diabetes Complications\", \"Diabesity\", \"Diabetic Angiopathies\",\n",
    "    \"Diabetic Cardiomyopathies\", \"Diabetic Coma\", \"Diabetic Ketoacidosis\",\n",
    "    \"Diabetic Nephropathies\", \"Diabetic Neuropathies\", \"Fetal Macrosomia\"\n",
    "]]\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    if not text or not isinstance(text, str): \n",
    "        return False\n",
    "    text_lower = text.lower() \n",
    "    return any(keyword.lower() in text_lower for keyword in keywords)\n",
    "\n",
    "def process_dataset_split(dataset_name, split_name, dataset, text_columns, output_file, keywords=[\"diabetes\", \"insulin\"]):\n",
    "    rows = []\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset[i]\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col in row:\n",
    "                text = row[col]\n",
    "\n",
    "            elif col == \"abstract\" and \"passages\" in row:\n",
    "                if isinstance(row[\"passages\"], list):\n",
    "                    text = \" \".join([p[\"text\"] for p in row[\"passages\"] if p.get(\"type\") == \"abstract\"])\n",
    "                else:\n",
    "                    text = \"\"\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            is_related = 1 if contains_keywords(text, keywords) else 0\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"split_data\": split_name,\n",
    "                \"features\": row,  \n",
    "                \"input\": text,\n",
    "                \"output\": is_related\n",
    "            })\n",
    "\n",
    "    if rows:  \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "        print(f\"{dataset_name} ({split_name}): {len(rows)} rows processed and appended to {output_file}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} ({split_name}): No rows processed.\")\n",
    "\n",
    "datasets = {\n",
    "    \"medqa\": {\"splits\": medqa, \"columns\": [\"question\", \"answer\"]},\n",
    "    \"medmcqa\": {\"splits\": medmcqa, \"columns\": [\"question\"]},\n",
    "    \"pubmedqa\": {\"splits\": pubmedqa, \"columns\": [\"QUESTION\", \"CONTEXTS\", \"LONG_ANSWER\"]},\n",
    "    \"bionli\": {\"splits\": bionli, \"columns\": [\"query\", \"answer\"]},\n",
    "    \"ddi\": {\"splits\": ddi, \"columns\": [\"text\"]},\n",
    "    \"chemdner\": {\"splits\": chemdner, \"columns\": ['text']},\n",
    "    \"meddialog\": {\"splits\": meddialog, \"columns\": [\"tgt\", \"src\"]},\n",
    "    \"pubmed\": {\"splits\": pubmed, \"columns\": [\"abstract\"]},\n",
    "}\n",
    "\n",
    "\n",
    "output_file = os.path.join(\"all_data_filtered_by_keywords.csv\")\n",
    "\n",
    "for dataset_name, details in datasets.items():\n",
    "    splits = details[\"splits\"]\n",
    "    text_columns = details[\"columns\"] \n",
    "\n",
    "    for split_name, split_data in splits.items():\n",
    "        process_dataset_split(dataset_name, split_name, split_data, text_columns, output_file, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/all_data_filtered_by_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filtered_by_keyword\n",
    "diabetes = all_df[all_df['output'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man presents to th...</td>\n",
       "      <td>A 68-year-old man presents to the emergency de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 68-year-old man comes to the p...</td>\n",
       "      <td>A 68-year-old man comes to the physician becau...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset split_data                                           features  \\\n",
       "30   medqa      train  {'question': 'A 68-year-old man presents to th...   \n",
       "34   medqa      train  {'question': 'A 68-year-old man comes to the p...   \n",
       "\n",
       "                                                input  output  \n",
       "30  A 68-year-old man presents to the emergency de...       1  \n",
       "34  A 68-year-old man comes to the physician becau...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset    split_data\n",
       "bionli     test            783\n",
       "           train           594\n",
       "           validation     1419\n",
       "chemdner   test             87\n",
       "           train           706\n",
       "           validation      104\n",
       "ddi        test             66\n",
       "           train           529\n",
       "           valid           588\n",
       "meddialog  test           1249\n",
       "           train          9962\n",
       "           validation     1145\n",
       "medmcqa    test             54\n",
       "           train          1810\n",
       "           validation       42\n",
       "medqa      test            132\n",
       "           train          1129\n",
       "pubmed     test            468\n",
       "           train          8061\n",
       "           validation      479\n",
       "pubmedqa   train         17558\n",
       "           validation      981\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def re_train_test_split_unique(dataframe, train_ratio=0.8):\n",
    "    new_rows = []\n",
    "\n",
    "    grouped = dataframe.groupby('dataset')\n",
    "\n",
    "    for dataset_name, group in grouped:\n",
    "        if dataset_name not in ['medqa', 'medmcqa', 'pubmedqa', 'bionli', 'chemdner', 'ddi', 'meddialog', 'pubmed']:\n",
    "            continue\n",
    "\n",
    "        # 중복 제거\n",
    "        group = group.drop_duplicates(subset='features')\n",
    "\n",
    "        train_data, test_data = train_test_split(\n",
    "            group,\n",
    "            test_size=1-train_ratio,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_data = train_data.copy()\n",
    "        train_data['split_data'] = 'train'\n",
    "\n",
    "        test_data = test_data.copy()\n",
    "        test_data['split_data'] = 'test'\n",
    "\n",
    "        new_rows.append(train_data)\n",
    "        new_rows.append(test_data)\n",
    "\n",
    "    result_df = pd.concat(new_rows, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "diabetes2 = re_train_test_split_unique(diabetes, train_ratio=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset    split_data\n",
       "bionli     test           338\n",
       "           train         1914\n",
       "chemdner   test           135\n",
       "           train          762\n",
       "ddi        test           178\n",
       "           train         1005\n",
       "meddialog  test          1615\n",
       "           train         9151\n",
       "medmcqa    test           286\n",
       "           train         1620\n",
       "medqa      test           185\n",
       "           train         1048\n",
       "pubmed     test          1351\n",
       "           train         7651\n",
       "pubmedqa   test          1720\n",
       "           train         9740\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes2.groupby(['dataset', 'split_data'])['output'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def add_input_to_for_sampling(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # INPUT 값을 추출하여 새로운 컬럼에 추가\n",
    "    def extract_for_sampling(row):\n",
    "        if row['dataset'] == 'medqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'medmcqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('question', None)\n",
    "            answer = \", \".join([\n",
    "                features.get('opa', ''),\n",
    "                features.get('opb', ''),\n",
    "                features.get('opc', ''),\n",
    "                features.get('opd', ''),\n",
    "            ]).strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmedqa':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            question = features.get('QUESTION', None)\n",
    "            answer = features.get('LONG_ANSWER', None)\n",
    "            return {'input': question, 'output': answer}\n",
    "        elif row['dataset'] == 'bionli':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            query = features.get('query', '') \n",
    "            input_text = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "            answer = features.get('answer', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'chemdner':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('text', None)\n",
    "            answer = features.get('entities', None)\n",
    "            if isinstance(answer, list) and len(answer) > 0:\n",
    "                answer = answer[0]\n",
    "            else:\n",
    "                answer = ''\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'ddi':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('conversations', [])[0].get('value', None).split(\"INPUT:\")[1].split(\"OUTPUT:\")[0].strip()\n",
    "            answer = features.get('conversations', [])[1].get('value', None)\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'meddialog':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('src', None).split(\"Doctor:\")[0].replace(\"Patient:\", \"\").strip()\n",
    "            answer = features.get('src', None).split(\"Doctor:\")[1].strip()\n",
    "            return {'input': input_text, 'output': answer}\n",
    "        elif row['dataset'] == 'pubmed':\n",
    "            features = ast.literal_eval(row['features'])\n",
    "            input_text = features.get('article', None)\n",
    "            abstract = features.get('abstract', None)\n",
    "            return {'input': input_text, 'output': abstract}\n",
    "\n",
    "    dataframe['for_sampling'] = dataframe.apply(extract_for_sampling, axis=1)\n",
    "\n",
    "    # chemdner 의 \"\" 비율 줄이기\n",
    "    chemdner_df = dataframe[dataframe['dataset'] == 'chemdner'].copy()\n",
    "    empty_outputs = chemdner_df[chemdner_df['for_sampling'].apply(lambda x: x['output'] == '')]\n",
    "    non_empty_outputs = chemdner_df[chemdner_df['for_sampling'].apply(lambda x: x['output'] != '')]\n",
    "    target_empty_size = int(len(non_empty_outputs) * 0.05 / 0.95) \n",
    "    sampled_empty_outputs = empty_outputs.sample(n=min(target_empty_size, len(empty_outputs)), random_state=42)\n",
    "    balanced_chemdner_df = pd.concat([non_empty_outputs, sampled_empty_outputs])\n",
    "    other_datasets = dataframe[dataframe['dataset'] != 'chemdner']\n",
    "    final_dataframe = pd.concat([other_datasets, balanced_chemdner_df])\n",
    "    \n",
    "    return final_dataframe\n",
    "\n",
    "diabetes3 = add_input_to_for_sampling(diabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI17726157', 'query': '\\nTASK: Ple...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] Dipeptidyl peptidase IV inhib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bionli</td>\n",
       "      <td>train</td>\n",
       "      <td>{'id': 'BioNLI2176649', 'query': '\\nTASK: Plea...</td>\n",
       "      <td>\\nTASK: Please classify the relationship betwe...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': '[PRE] Cold exposure reverses the di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset split_data                                           features  \\\n",
       "0  bionli      train  {'id': 'BioNLI17726157', 'query': '\\nTASK: Ple...   \n",
       "1  bionli      train  {'id': 'BioNLI2176649', 'query': '\\nTASK: Plea...   \n",
       "\n",
       "                                               input  output  \\\n",
       "0  \\nTASK: Please classify the relationship betwe...       1   \n",
       "1  \\nTASK: Please classify the relationship betwe...       1   \n",
       "\n",
       "                                        for_sampling  \n",
       "0  {'input': '[PRE] Dipeptidyl peptidase IV inhib...  \n",
       "1  {'input': '[PRE] Cold exposure reverses the di...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jaesung/anaconda3/envs/faiss_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medqa (1048 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.46 seconds for 1048 rows\n",
      "Applying MMR for dataset: medqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  12%|█▎        | 1/8 [00:02<00:18,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medmcqa (1620 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.02 seconds for 1620 rows\n",
      "Applying MMR for dataset: medmcqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  25%|██▌       | 2/8 [00:05<00:17,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmedqa (9740 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 4.84 seconds for 9740 rows\n",
      "Applying MMR for dataset: pubmedqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  38%|███▊      | 3/8 [05:38<12:48, 153.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: bionli (1914 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.50 seconds for 1914 rows\n",
      "Applying MMR for dataset: bionli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  50%|█████     | 4/8 [05:44<06:21, 95.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: chemdner (548 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.48 seconds for 548 rows\n",
      "Applying MMR for dataset: chemdner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  62%|██████▎   | 5/8 [05:45<03:03, 61.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: ddi (1005 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.71 seconds for 1005 rows\n",
      "Applying MMR for dataset: ddi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  75%|███████▌  | 6/8 [05:47<01:22, 41.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: meddialog (9151 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.55 seconds for 9151 rows\n",
      "Applying MMR for dataset: meddialog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  88%|████████▊ | 7/8 [07:55<01:09, 69.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmed (7651 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:13<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 21.73 seconds for 7651 rows\n",
      "Applying MMR for dataset: pubmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 8/8 [09:59<00:00, 74.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset split_data                                           features  \\\n",
      "0   medqa      train  {'question': 'A 42-year-old woman presents to ...   \n",
      "1   medqa      train  {'question': \"A 69-year-old man presents to hi...   \n",
      "2   medqa      train  {'question': 'A 64-year-old female with a long...   \n",
      "3   medqa      train  {'question': 'An endocrinologist is working wi...   \n",
      "4   medqa      train  {'question': \"A pediatrician is investigating ...   \n",
      "\n",
      "                                               input  output  \\\n",
      "0  A 42-year-old woman presents to the emergency ...       1   \n",
      "1  A 69-year-old man presents to his dermatologis...       1   \n",
      "2  A 64-year-old female with a long-standing hist...       1   \n",
      "3  An endocrinologist is working with a pharmaceu...       1   \n",
      "4  A pediatrician is investigating determinants o...       1   \n",
      "\n",
      "                                        for_sampling  \n",
      "0  {'input': 'A 42-year-old woman presents to the...  \n",
      "1  {'input': 'A 69-year-old man presents to his d...  \n",
      "2  {'input': 'A 64-year-old female with a long-st...  \n",
      "3  {'input': 'An endocrinologist is working with ...  \n",
      "4  {'input': 'A pediatrician is investigating det...  \n",
      "총 샘플링된 행 수: 21248\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'train')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "# 샘플링 설정\n",
    "sampling_config = {\n",
    "    'medqa': 1000,\n",
    "    'medmcqa': 1000,\n",
    "    'pubmedqa': 8000, # 1000\n",
    "    'bionli': 1800, # 1500\n",
    "    'chemdner': 600,\n",
    "    'ddi': 900,\n",
    "    'meddialog': 4000,\n",
    "    'pubmed': 4000,\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_train = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_train.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_train)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_train.to_csv(\"final_combined_train_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medqa (185 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.19 seconds for 185 rows\n",
      "Applying MMR for dataset: medqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing datasets:  12%|█▎        | 1/8 [00:00<00:01,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: medmcqa (286 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "Processing datasets:  25%|██▌       | 2/8 [00:00<00:01,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.21 seconds for 286 rows\n",
      "Applying MMR for dataset: medmcqa\n",
      "Generating embeddings for dataset: pubmedqa (1720 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 1.09 seconds for 1720 rows\n",
      "Applying MMR for dataset: pubmedqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  38%|███▊      | 3/8 [00:04<00:09,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: bionli (338 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "Processing datasets:  50%|█████     | 4/8 [00:05<00:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.37 seconds for 338 rows\n",
      "Applying MMR for dataset: bionli\n",
      "Generating embeddings for dataset: chemdner (97 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Processing datasets:  62%|██████▎   | 5/8 [00:05<00:02,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.10 seconds for 97 rows\n",
      "Applying MMR for dataset: chemdner\n",
      "Generating embeddings for dataset: ddi (178 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.16 seconds for 178 rows\n",
      "Applying MMR for dataset: ddi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  75%|███████▌  | 6/8 [00:05<00:01,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: meddialog (1615 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 0.06 seconds for 1615 rows\n",
      "Applying MMR for dataset: meddialog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:  88%|████████▊ | 7/8 [00:06<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for dataset: pubmed (1351 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 4.12 seconds for 1351 rows\n",
      "Applying MMR for dataset: pubmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 8/8 [00:12<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset split_data                                           features  \\\n",
      "0   medqa       test  {'question': 'A 67-year-old man presents to hi...   \n",
      "1   medqa       test  {'question': \"A 37-year-old woman presents to ...   \n",
      "2   medqa       test  {'question': \"Shortly after delivery, a female...   \n",
      "3   medqa       test  {'question': 'A 64-year-old man with a history...   \n",
      "4   medqa       test  {'question': 'A researcher is tracing the fate...   \n",
      "\n",
      "                                               input  output  \\\n",
      "0  A 67-year-old man presents to his primary care...       1   \n",
      "1  A 37-year-old woman presents to the emergency ...       1   \n",
      "2  Shortly after delivery, a female newborn devel...       1   \n",
      "3  A 64-year-old man with a history of type 2 dia...       1   \n",
      "4  A researcher is tracing the fate of C-peptide,...       1   \n",
      "\n",
      "                                        for_sampling  \n",
      "0  {'input': 'A 67-year-old man presents to his p...  \n",
      "1  {'input': 'A 37-year-old woman presents to the...  \n",
      "2  {'input': 'Shortly after delivery, a female ne...  \n",
      "3  {'input': 'A 64-year-old man with a history of...  \n",
      "4  {'input': 'A researcher is tracing the fate of...  \n",
      "총 샘플링된 행 수: 3195\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def mmr(query_embedding, doc_embeddings, diversity, top_n):\n",
    "    faiss.normalize_L2(doc_embeddings)\n",
    "    faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "\n",
    "    selected_indices = []\n",
    "    candidate_indices = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_n):\n",
    "        if not candidate_indices:\n",
    "            break\n",
    "\n",
    "        if len(selected_indices) == 0:\n",
    "            selected_idx = candidate_indices[np.argmax(np.dot(doc_embeddings[candidate_indices], query_embedding.T))]\n",
    "        else:\n",
    "            selected_embeddings = doc_embeddings[selected_indices]\n",
    "            similarity_to_selected = np.dot(doc_embeddings[candidate_indices], selected_embeddings.T)\n",
    "            diversity_scores = np.max(similarity_to_selected, axis=1)\n",
    "            relevance_scores = np.dot(doc_embeddings[candidate_indices], query_embedding.T).flatten()\n",
    "            mmr_scores = (1 - diversity) * relevance_scores - diversity * diversity_scores\n",
    "            selected_idx = candidate_indices[np.argmax(mmr_scores)]\n",
    "\n",
    "        selected_indices.append(selected_idx)\n",
    "        candidate_indices.remove(selected_idx)\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "def mmr_sampling(dataframe, sampling_dict, embedding_model='all-MiniLM-L6-v2', batch_size=64, diversity=0.7, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    print(\"Loading SentenceTransformer model on GPU...\")\n",
    "    model = SentenceTransformer(embedding_model, device=\"cuda:0\")\n",
    "\n",
    "    sampled_rows = []\n",
    "\n",
    "    for dataset_name, sample_count in tqdm(sampling_dict.items(), desc=\"Processing datasets\"):\n",
    "        subset = dataframe[(dataframe['dataset'] == dataset_name) & (dataframe['split_data'] == 'test')].copy()\n",
    "        num_rows = len(subset)\n",
    "        if num_rows == 0:\n",
    "            print(f\"No data found for dataset: {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating embeddings for dataset: {dataset_name} ({num_rows} rows)\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        embeddings = model.encode(\n",
    "            subset['for_sampling'].astype(str).tolist(),\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Embedding generation took {elapsed_time:.2f} seconds for {num_rows} rows\")\n",
    "\n",
    "        query_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        print(f\"Applying MMR for dataset: {dataset_name}\")\n",
    "        selected_indices = mmr(query_embedding, embeddings, diversity, sample_count)\n",
    "        sampled_subset = subset.iloc[selected_indices]\n",
    "        sampled_rows.append(sampled_subset)\n",
    "\n",
    "    return pd.concat(sampled_rows, ignore_index=True)\n",
    "\n",
    "sampling_config = {\n",
    "    'medqa': 150,\n",
    "    'medmcqa': 150,\n",
    "    'pubmedqa': 1200, # 150 \n",
    "    'bionli': 270, # 225\n",
    "    'chemdner': 90,\n",
    "    'ddi': 135,\n",
    "    'meddialog': 600,\n",
    "    'pubmed': 600,\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 샘플링 실행\n",
    "sampled_test = mmr_sampling(diabetes3, sampling_config, batch_size=4096, diversity=0.7, seed=42)\n",
    "\n",
    "# 결과 확인\n",
    "print(sampled_test.head())\n",
    "print(f\"총 샘플링된 행 수: {len(sampled_test)}\")\n",
    "\n",
    "# 결과 저장\n",
    "sampled_test.to_csv(\"final_combined_test_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_combined_train_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_train_sample.csv\")\n",
    "final_combined_test_sample = pd.read_csv(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/final_combined_test_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test 겹치는 feature 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거 후 겹치는 개수 확인\n",
    "train_unique = final_combined_train_sample.drop_duplicates(subset='features')\n",
    "test_unique = final_combined_test_sample.drop_duplicates(subset='features')\n",
    "\n",
    "# train/test 겹치는 개수 계산\n",
    "overlapping_count = len(pd.merge(train_unique, test_unique, on='features'))\n",
    "print(f\"Train/Test 겹치는 feature 개수: {overlapping_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_train_sample['task'] = None\n",
    "# medqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medqa', 'task'] = 'qa_objective_1'\n",
    "# medmcqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'medmcqa', 'task'] = 'qa_objective_2'\n",
    "# pubmedqa\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'pubmedqa', 'task'] = 'qa_objective_3'\n",
    "# bionli\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'bionli', 'task'] = 'nli'\n",
    "# chemdner\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'chemdner', 'task'] = 'ie'\n",
    "# ddi\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'ddi', 'task'] = 're'\n",
    "# meddialog\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'meddialog', 'task'] = 'generation'\n",
    "# pubmed\n",
    "final_combined_train_sample.loc[final_combined_train_sample['dataset'] == 'pubmed', 'task'] = 'summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_combined_test_sample['task'] = None\n",
    "# medqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medqa', 'task'] = 'qa_objective_1'\n",
    "# medmcqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'medmcqa', 'task'] = 'qa_objective_2'\n",
    "# pubmedqa\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'pubmedqa', 'task'] = 'qa_objective_3'\n",
    "# bionli\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'bionli', 'task'] = 'nli'\n",
    "# chemdner\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'chemdner', 'task'] = 'ie'\n",
    "# ddi\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'ddi', 'task'] = 're'\n",
    "# meddialog\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'meddialog', 'task'] = 'generation'\n",
    "# pubmed\n",
    "final_combined_test_sample.loc[final_combined_test_sample['dataset'] == 'pubmed', 'task'] = 'summarization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 42-year-old woman presents to ...</td>\n",
       "      <td>A 42-year-old woman presents to the emergency ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'A 42-year-old woman presents to the...</td>\n",
       "      <td>qa_objective_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': \"A 69-year-old man presents to hi...</td>\n",
       "      <td>A 69-year-old man presents to his dermatologis...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': \"A 69-year-old man presents to his d...</td>\n",
       "      <td>qa_objective_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 dataset split_data  \\\n",
       "0           0   medqa      train   \n",
       "1           1   medqa      train   \n",
       "\n",
       "                                            features  \\\n",
       "0  {'question': 'A 42-year-old woman presents to ...   \n",
       "1  {'question': \"A 69-year-old man presents to hi...   \n",
       "\n",
       "                                               input  output  \\\n",
       "0  A 42-year-old woman presents to the emergency ...       1   \n",
       "1  A 69-year-old man presents to his dermatologis...       1   \n",
       "\n",
       "                                        for_sampling            task  \n",
       "0  {'input': 'A 42-year-old woman presents to the...  qa_objective_1  \n",
       "1  {'input': \"A 69-year-old man presents to his d...  qa_objective_1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_features(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}\n",
    "    \n",
    "def generate_input_output(row):\n",
    "    instruction_value = \"\"; input_value = \"\"; output_value = \"\"\n",
    "\n",
    "    task = row['task']\n",
    "    features = parse_features(row['features'])\n",
    "    if row['dataset']=='medqa' and task=='qa_objective_1':\n",
    "        question = features.get('question', None)\n",
    "        options = features.get('options', None)\n",
    "        answer = features.get('answer', None)\n",
    "        answer_idx = features.get('answer_idx', None)\n",
    "        instruction_value = \"Select the most appropriate answer for the given medical question from the provided options.\"\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}) {answer}\"\n",
    "        )\n",
    "    elif row['dataset']=='medmcqa' and task=='qa_objective_2': \n",
    "        question = features.get('question', None)\n",
    "        options = {\n",
    "            'A': features.get('opa', '').strip(),\n",
    "            'B': features.get('opb', '').strip(),\n",
    "            'C': features.get('opc', '').strip(),\n",
    "            'D': features.get('opd', '').strip(),\n",
    "        }\n",
    "        answer_num = features.get('cop', None)\n",
    "        answer_idx = 'A' if answer_num == 0 else 'B' if answer_num == 1 else 'C' if answer_num == 2 else 'D'\n",
    "        answer = features.get('opa', '').strip() if answer_num == 0 else features.get('opb', '').strip() if answer_num == 1 else features.get('opc', '').strip() if answer_num == 2 else features.get('opd', '').strip()\n",
    "        instruction_value = \"Select the most appropriate answer for the given medical question from the provided options.\"\n",
    "        input_value = (\n",
    "            f\"{question} Please select one of the following: A) {options['A']}, B) {options['B']}, C) {options['C']}, D) {options['D']}.\"\n",
    "        )\n",
    "        output_value = (\n",
    "            f\"{answer_idx}) {answer}\"\n",
    "        )\n",
    "    elif row['dataset']=='pubmedqa' and task=='qa_objective_3':\n",
    "        question = features.get('QUESTION', '').strip()\n",
    "        context = \" \".join(features.get('CONTEXTS'))\n",
    "        # answer = features.get('LONG_ANSWER', '').strip() \n",
    "        answer = features.get('final_decision', '').strip()\n",
    "        instruction_value = \"Choose the correct anser (Yes, No, or Maybe) for the given question based on the proviced context.\"\n",
    "        input_value = (\n",
    "            f\"Question: {question} \"\n",
    "            f\"Context: {context}\"\n",
    "        )\n",
    "        output_value = f\"{answer}\"\n",
    "    elif row['dataset']=='bionli' and task=='nli':\n",
    "        query = features.get('query', '') \n",
    "        instruction_value =  \"Please classify the relationship between the given premise and hypothesis into one of the following labels: entailment, contradiction, or neutral. return only the label.\"\n",
    "        input_value = query.split(\"INPUT: \")[1].split(\"[HYP]\")[0].strip() if \"INPUT: \" in query else None\n",
    "        output_value = features.get('answer', None)\n",
    "    elif row['dataset']=='chemdner' and task=='ie':\n",
    "        instruction_value = \"Extract all chemical entities (chemical compounds, drugs, and molecules) from the given text.\"\n",
    "        input_value = features.get('text', None)\n",
    "        answer = features.get('entities', None)\n",
    "        if isinstance(answer, list) and len(answer) > 0:\n",
    "            output_value = answer[0]\n",
    "        else:\n",
    "            output_value = ''\n",
    "    elif row['dataset']== 'ddi' and task=='re':\n",
    "        instruction_value = \"Analyze the sentence with two drugs labeled as @DRUG_A$ and @DRUG_B$. Extract the interaction between @DRUG_A$ and @DRUG_B$ from the input sentence by selecting only one of the following options: 'DDI-effect', 'DDI-mechanism', 'DDI-advise', 'DDI-false', and 'DDI-int'. 'DDI-effect': Choose this when the interaction describes an effect or a pharmacodynamic mechanism. 'DDI-mechanism': Choose this for interactions explained by pharmacokinetic mechanisms. 'DDI-advise': Choose this when the sentence provides a recommendation or advice about the drug interaction. 'DDI-false': Choose this if there is no actual drug-drug interaction in the sentence. 'DDI-int': Choose this when a drug-drug interaction is mentioned without additional detail.\"\n",
    "        input_value = features.get('conversations', [])[0].get('value', None).split(\"INPUT:\")[1].split(\"OUTPUT:\")[0].strip()\n",
    "        output_value = features.get('conversations', [])[1].get('value', None)\n",
    "    elif row['dataset']=='meddialog' and task=='generation':\n",
    "        instruction_value = features.get('tgt', None)\n",
    "        input_value = features.get('src', None).split(\"Doctor:\")[0].replace(\"Patient:\", \"\").strip()\n",
    "        output_value = features.get('src', None).split(\"Doctor:\")[1].strip()\n",
    "    elif row['dataset']=='pubmed' and task=='summarization':\n",
    "        instruction_value = \"Summarize the key findings of the given PubMed abstract into structured fields: Objective, Methods, Results, and Conclusion.\"\n",
    "        article = features.get('article', '').strip()  \n",
    "        abstract = features.get('abstract', '').strip() \n",
    "        input_value = f\"{article}\"\n",
    "        output_value = abstract\n",
    "    return instruction_value, input_value, output_value\n",
    "\n",
    "final_combined_train_sample[['instruction', 'input', 'output']] = final_combined_train_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")\n",
    "\n",
    "final_combined_test_sample[['instruction', 'input', 'output']] = final_combined_test_sample.apply(\n",
    "    lambda row: pd.Series(generate_input_output(row)), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "# pubmedqa\n",
    "\n",
    "# trainset\n",
    "df_train = final_combined_train_sample[final_combined_train_sample['dataset']=='pubmedqa']\n",
    "df_yes = df_train[df_train['output']=='yes']\n",
    "df_no = df_train[df_train['output']=='no']\n",
    "\n",
    "df_downsampled = resample(df_yes, replace=False, n_samples=len(df_no), random_state=42)\n",
    "balanced_df = pd.concat([df_downsampled, df_no])\n",
    "\n",
    "final_df_balanced = final_combined_train_sample[final_combined_train_sample['dataset'] != 'pubmedqa']\n",
    "final_combined_train_sample = pd.concat([final_df_balanced, balanced_df])\n",
    "\n",
    "# testset\n",
    "df_test = final_combined_test_sample[final_combined_test_sample['dataset']=='pubmedqa']\n",
    "df_yes = df_test[df_test['output']=='yes']\n",
    "df_no = df_test[df_test['output']=='no']\n",
    "\n",
    "df_downsampled = resample(df_yes, replace=False, n_samples=len(df_no), random_state=42)\n",
    "balanced_df = pd.concat([df_downsampled, df_no])\n",
    "\n",
    "final_df_balanced = final_combined_test_sample[final_combined_test_sample['dataset'] != 'pubmedqa']\n",
    "final_combined_test_sample = pd.concat([final_df_balanced, balanced_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "# bionli\n",
    "\n",
    "# trainset\n",
    "df_train = final_combined_train_sample[final_combined_train_sample['dataset']=='bionli']\n",
    "df_yes = df_train[df_train['output']=='contradiction']\n",
    "df_no = df_train[df_train['output']=='entailment']\n",
    "\n",
    "df_downsampled = resample(df_yes, replace=False, n_samples=len(df_no), random_state=42)\n",
    "balanced_df = pd.concat([df_downsampled, df_no])\n",
    "\n",
    "final_df_balanced = final_combined_train_sample[final_combined_train_sample['dataset'] != 'bionli']\n",
    "final_combined_train_sample = pd.concat([final_df_balanced, balanced_df])\n",
    "\n",
    "# testset\n",
    "df_test = final_combined_test_sample[final_combined_test_sample['dataset']=='bionli']\n",
    "df_yes = df_test[df_test['output']=='contradiction']\n",
    "df_no = df_test[df_test['output']=='entailment']\n",
    "\n",
    "df_downsampled = resample(df_yes, replace=False, n_samples=len(df_no), random_state=42)\n",
    "balanced_df = pd.concat([df_downsampled, df_no])\n",
    "\n",
    "final_df_balanced = final_combined_test_sample[final_combined_test_sample['dataset'] != 'bionli']\n",
    "final_combined_test_sample = pd.concat([final_df_balanced, balanced_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_combined_train_sample, final_combined_test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_train_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/train_instruction_dataset.json\", orient=\"columns\", indent=4)\n",
    "final_combined_test_sample.to_json(\"/data/jaesung/llm_for_diabetes/src/data/data1_diabetes/test_instruction_dataset.json\", orient=\"columns\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>split_data</th>\n",
       "      <th>features</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>for_sampling</th>\n",
       "      <th>task</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': 'A 42-year-old woman presents to ...</td>\n",
       "      <td>A 42-year-old woman presents to the emergency ...</td>\n",
       "      <td>D) Ursodeoxycholic acid</td>\n",
       "      <td>{'input': 'A 42-year-old woman presents to the...</td>\n",
       "      <td>qa_objective_1</td>\n",
       "      <td>Select the most appropriate answer for the giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>medqa</td>\n",
       "      <td>train</td>\n",
       "      <td>{'question': \"A 69-year-old man presents to hi...</td>\n",
       "      <td>A 69-year-old man presents to his dermatologis...</td>\n",
       "      <td>B) Immunosuppression</td>\n",
       "      <td>{'input': \"A 69-year-old man presents to his d...</td>\n",
       "      <td>qa_objective_1</td>\n",
       "      <td>Select the most appropriate answer for the giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 dataset split_data  \\\n",
       "0           0   medqa      train   \n",
       "1           1   medqa      train   \n",
       "\n",
       "                                            features  \\\n",
       "0  {'question': 'A 42-year-old woman presents to ...   \n",
       "1  {'question': \"A 69-year-old man presents to hi...   \n",
       "\n",
       "                                               input                   output  \\\n",
       "0  A 42-year-old woman presents to the emergency ...  D) Ursodeoxycholic acid   \n",
       "1  A 69-year-old man presents to his dermatologis...     B) Immunosuppression   \n",
       "\n",
       "                                        for_sampling            task  \\\n",
       "0  {'input': 'A 42-year-old woman presents to the...  qa_objective_1   \n",
       "1  {'input': \"A 69-year-old man presents to his d...  qa_objective_1   \n",
       "\n",
       "                                         instruction  \n",
       "0  Select the most appropriate answer for the giv...  \n",
       "1  Select the most appropriate answer for the giv...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_combined_train_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_gpu_kernel",
   "language": "python",
   "name": "faiss_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
